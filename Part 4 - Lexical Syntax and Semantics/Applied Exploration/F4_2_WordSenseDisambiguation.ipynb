{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/blob/main/F4_2_WordSenseDisambiguation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C192SOmJS6lw"
      },
      "source": [
        "# CS 195: Natural Language Processing\n",
        "## WordSense Disambiguation\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ericmanley/f23-CS195NLP/blob/main/F4_2_WordSenseDisambiguation.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVKGyzFGlCff"
      },
      "source": [
        "## References\n",
        "\n",
        "Word Senses and WordNet, Chapter 23 of *Speech and Language Processing* by Daniel Jurafsky & James H. Martin: https://web.stanford.edu/~jurafsky/slp3/23.pdf\n",
        "\n",
        "WordNet documentation: https://www.nltk.org/api/nltk.corpus.reader.wordnet.html\n",
        "\n",
        "SemCor Corpus Module documentation: https://www.nltk.org/api/nltk.corpus.reader.semcor.html\n",
        "\n",
        "NLTK Stopwords: https://pythonspot.com/nltk-stop-words/\n",
        "\n",
        "Lemmatization with NLTK: https://www.geeksforgeeks.org/python-lemmatization-with-nltk/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPe2X-9PlCff",
        "outputId": "8d2a919e-a0d4-4d2e-a6e0-7fed847cb277"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bdWnsm7LlCfg"
      },
      "outputs": [],
      "source": [
        "#you shouldn't need to do this in Colab, but I had to do it on my own machine\n",
        "#in order to connect to the nltk service\n",
        "import nltk\n",
        "import ssl\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9Hez79AlCfh"
      },
      "source": [
        "## Word Sense Disambiguation\n",
        "\n",
        "As we explored last time, one word can have many *senses*.\n",
        "\n",
        "The **WordNet** database can be used to look up different word senses of a particular word.\n",
        "\n",
        "The task of figuring out which sense is being usede in a given context is called **word sense disambiguation**\n",
        "\n",
        "Important for\n",
        "* extracting proper meaning from text\n",
        "* translation - e.g., different senses of one word in English might have different translations\n",
        "* question answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfO21Ox_lCfh"
      },
      "source": [
        "## Typical approach for WSD\n",
        "\n",
        "Look at the *context* of a word - what other words are around it\n",
        "\n",
        "For example, consider the word **bank** in\n",
        "\n",
        "\"I need to go to the bank and deposit my paycheck.\"\n",
        "\n",
        "We can determine from *deposit*, *paycheck*, and maybe even *go to* that we're talking about a financial institution and not a river bank.\n",
        "\n",
        "Which definition does the context share the most words with?\n",
        "\n",
        "*Definition 1:* 'sloping land (especially the slope beside a body of water)'\n",
        "\n",
        "*Definition 2:* 'a financial institution that accepts deposits and channels the money into lending activities'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-F6BP4ylCfh",
        "outputId": "76f4edff-9cf2-4ae7-8e2d-992cb385f494"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "def compute_overlap(set1, set2):\n",
        "    count_overlap = 0\n",
        "    for item in set1:\n",
        "        if item in set2:\n",
        "            count_overlap += 1\n",
        "    return count_overlap\n",
        "\n",
        "\n",
        "sentence = [\"i\", \"need\", \"to\", \"go\", \"to\", \"the\", \"bank\", \"and\", \"deposit\", \"my\", \"paycheck\"]\n",
        "definition1 = [\"sloping\", \"land\", \"especially\", \"the\", \"slope\", \"beside\", \"a\", \"body\", \"of\", \"water\"]\n",
        "definition2 = [\"a\", \"financial\", \"institution\", \"that\", \"accepts\", \"deposits\", \"and\", \"channels\", \"the\", \"money\", \"into\", \"lending\", \"activities\"]\n",
        "\n",
        "print(compute_overlap(sentence,definition1))\n",
        "print(compute_overlap(sentence,definition2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGHCGTxdlCfi"
      },
      "source": [
        "### Discuss: What problems do you see with this approach?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0muC8v96lCfi"
      },
      "source": [
        "## The Simplified Lesk Algorithm\n",
        "\n",
        "The **Simplified Lesk Algorithm** loops over all possible word senses to find the one whose definition/examples share the most words in common with the sentence context.\n",
        "\n",
        "Given a `word` and `sentence`\n",
        "1. Make a *set* of all the words in the sentence (my need to tokenize)\n",
        "2. Look up all the `synsets` for `word` in **WordNet**\n",
        "3. Loop through the list of `synsets`\n",
        "    * create a signature - the set of all the words that appear the definition and list of examples for this `word` from **WordNet** (may need to tokenize)\n",
        "    * compute the overlap between the signature and the word context\n",
        "    * if this is better than the previous best overlap, save the new sense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8yKQvN-subzh"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "#nltk.download('wordnet') #only need to do this once\n",
        "\n",
        "def simplified_lesk(word,sentence):\n",
        "    best_sense = 0\n",
        "\n",
        "    #fill this in\n",
        "\n",
        "    return best_sense"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHPuoDnTlCfi"
      },
      "source": [
        "### Discuss: How should we tokenize our text data for this problem?\n",
        "\n",
        "I think that we should:\n",
        "\n",
        "- 1. Tokenize the input data\n",
        "- 2. Sentence Tokenization\n",
        "- 3. Word tokenization\n",
        "- 4. Synset Retrieval\n",
        "- 5. Signature creation\n",
        "- 6. Word context tokenization\n",
        "- 7. Overlap calculation\n",
        "- 8. Sense disambiguation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZQyeSbtlCfj"
      },
      "source": [
        "### Group Exercise: Finish implementing this algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiM7omMirK3b",
        "outputId": "bb91c8f7-3596-456d-8504-ce6d555f2a1a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here is the tokenized sentence:  {'my', 'the', 'I', 'deposit', 'bank', 'money', 'went', '.', 'to'}\n",
            "-----------------------------------------------\n",
            "Word Synsets:  [Synset('bank.n.01'), Synset('depository_financial_institution.n.01'), Synset('bank.n.03'), Synset('bank.n.04'), Synset('bank.n.05'), Synset('bank.n.06'), Synset('bank.n.07'), Synset('savings_bank.n.02'), Synset('bank.n.09'), Synset('bank.n.10'), Synset('bank.v.01'), Synset('bank.v.02'), Synset('bank.v.03'), Synset('bank.v.04'), Synset('bank.v.05'), Synset('deposit.v.02'), Synset('bank.v.07'), Synset('trust.v.01')]\n",
            "-----------------------------------------------\n",
            "Signature:  {')', 'sloping', 'on', 'the', 'water', 'canoe', 'beside', 'a', 'up', 'bank', 'land', 'pulled', '(', 'body', 'they', 'especially', 'of', 'slope'}\n",
            "Signature:  {'bank', 'land', 'body', 'especially', 'of', 'sloping', 'currents', 'beside', 'river', 'watched', 'canoe', 'and', 'he', '(', 'they', ')', 'on', 'the', 'water', 'a', 'up', 'sat', 'pulled', 'slope'}\n",
            "Signature:  {'activities', 'check', 'the', 'financial', 'that', 'a', 'institution', 'and', 'money', 'into', 'he', 'cashed', 'channels', 'lending', 'bank', 'at', 'accepts', 'deposits'}\n",
            "Signature:  {'cashed', 'institution', 'bank', 'into', 'home', 'lending', 'activities', 'financial', 'that', 'mortgage', 'holds', 'deposits', 'my', 'and', 'money', 'he', 'check', 'on', 'the', 'a', 'channels', 'at', 'accepts'}\n",
            "Signature:  {'or', 'earth', 'a', 'bank', 'huge', 'long', 'pile', 'of', 'ridge'}\n",
            "Signature:  {'similar', 'or', 'switches', 'row', 'tiers', 'a', 'an', 'objects', 'arrangement', 'he', 'in', 'bank', 'operated', 'of'}\n",
            "Signature:  {'tried', 'games', 'Monte', 'bank', 'dealer', 'break', 'Carlo', 'held', 'some', 'gambling', 'house', 'by', 'he', 'funds', 'to', 'or', 'the', 'a', 'in', 'at'}\n",
            "Signature:  {'slot', 'bank', 'with', 'home', 'empty', 'keeping', 'for', 'usually', 'money', 'coin', '(', 'top', ')', 'the', 'a', 'was', 'in', 'container', 'at'}\n",
            "Signature:  {'on', 'Witherspoon', 'the', 'transacted', 'business', 'and', 'a', 'bank', 'in', 'banking', 'building', 'is', 'Nassau', 'corner', 'of', 'which'}\n",
            "Signature:  {'bank', 'into', 'especially', 'about', 'aircraft', 'turning', 'longitudinal', 'laterally', ';', 'steep', 'flight', 'axis', 'went', '(', 'its', 'plane', ')', 'the', 'a', 'maneuver', 'tips', 'in'}\n",
            "Signature:  {'aircraft', 'the', 'laterally', 'pilot', 'bank', 'tip', 'to', 'had'}\n",
            "Signature:  {'a', 'enclose', 'bank', 'with', 'roads'}\n",
            "Signature:  {'or', 'this', 'business', 'account', 'do', 'a', 'bank', 'an', 'you', 'in', 'with', 'Where', '?', 'keep', 'at', 'town'}\n",
            "Signature:  {'her', 'put', 'account', 'every', 'bank', 'a', 'into', 'month', 'She', 'paycheck', 'deposits'}\n",
            "Signature:  {'the', 'burning', 'ashes', 'bank', 'a', 'fire', 'with', 'cover', 'to', 'rate', 'so', 'of', 'control'}\n",
            "Signature:  {'or', 'confidence', 'God', 'in', 'can', 'trust', 'We', 'have', 'faith'}\n",
            "Signature:  {'on', 'or', 'confidence', 'your', 'friends', 'God', 'in', 'can', 'trust', 'Rely', 'We', 'have', 'faith'}\n",
            "Signature:  {'on', 'or', 'confidence', 'education', 'your', 'friends', 'bank', 'God', 'in', 'good', 'can', 'trust', 'Rely', 'We', 'have', 'faith'}\n",
            "Signature:  {'education', 'bank', 'recipes', \"'s\", 'can', 'God', 'good', 'We', 'by', 'my', 'your', 'trust', 'Rely', 'faith', 'on', 'or', 'confidence', 'I', 'swear', 'friends', 'in', 'grandmother', 'have'}\n",
            "Best Sense: trust.v.01\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "import nltk\n",
        "nltk.download('punkt') # Only need to do this once\n",
        "nltk.download('wordnet') #only need to do this once\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def simplified_lesk(word, sentence):\n",
        "    best_sense = None\n",
        "    max_overlap = 0\n",
        "\n",
        "    # Tokenize the sentence\n",
        "    sentence_words = set(word_tokenize(sentence))\n",
        "    print('Here is the tokenized sentence: ',sentence_words)\n",
        "    print('-----------------------------------------------')\n",
        "\n",
        "    # Get the synsets for the target word\n",
        "    word_synsets = wn.synsets(word)\n",
        "    print('Word Synsets: ',word_synsets)\n",
        "    print('-----------------------------------------------')\n",
        "\n",
        "    for synset in word_synsets:\n",
        "        # Create a signature for the current synset\n",
        "        signature = set(word_tokenize(synset.definition()))\n",
        "\n",
        "        for example in synset.examples():\n",
        "            signature.update(word_tokenize(example))\n",
        "            print('Signature: ',signature)\n",
        "\n",
        "        # Calculate the overlap between the signature and the word context\n",
        "        overlap = len(signature.intersection(sentence_words))\n",
        "\n",
        "        # If the current overlap is better than the previous best, update best_sense\n",
        "        if overlap > max_overlap:\n",
        "            max_overlap = overlap\n",
        "        best_sense = synset\n",
        "\n",
        "    return best_sense\n",
        "\n",
        "# Example usage:\n",
        "word = \"bank\"\n",
        "sentence = \"I went to the bank to deposit my money.\"\n",
        "\n",
        "best_sense = simplified_lesk(word, sentence)\n",
        "if best_sense:\n",
        "    print(\"Best Sense:\", best_sense.name())\n",
        "else:\n",
        "    print(\"No sense found for the word in the given context.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2RwJipTlCfj"
      },
      "source": [
        "## Improving the algorithm\n",
        "\n",
        "Two things we could do to try to improve the Lesk algorithm\n",
        "\n",
        "1. Remove tokens that don't carry meaning like punctuation and *stopwords* (words like \"the\", \"is\", \"to\", etc.)\n",
        "\n",
        "2. Lemmatize the words - convert them into their base form\n",
        "\n",
        "Try to catch the word \"deposit(s)\" in\n",
        "* \"a financial institution that accepts **deposits** and channels the money into lending activities'\n",
        "* \"I need to go to the bank and **deposit** my paycheck.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDCGSz4klCfj"
      },
      "source": [
        "## Stopwords Corpus\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIdjeWn5lCfj",
        "outputId": "26719d67-90ce-441f-fdbd-f0dd6fcb1fa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'their', 'few', 'with', 'any', 'o', 'ourselves', 'myself', \"doesn't\", 'there', 'she', \"hadn't\", 'if', 'some', 'nor', 'very', \"you'll\", 'during', \"wasn't\", 'again', 'my', 'does', 'further', 'haven', \"shouldn't\", 'we', 'an', 'because', 'same', 'is', 'they', 'under', 'no', 'were', 'not', 'am', 'on', 'why', 'having', 'the', \"mustn't\", \"shan't\", 'own', 'when', 'was', 'you', 'hers', 'up', 'but', \"should've\", 'yours', \"it's\", 'so', 'had', 'until', \"needn't\", 'these', 'both', 'our', \"didn't\", 'theirs', 'be', 'only', 'ain', 'being', 'couldn', 'then', 'most', 'about', 'it', 'after', 'shouldn', 'that', 'should', \"won't\", 'yourself', \"you're\", 'above', 'where', 'will', 'he', 'll', 'other', 'its', 'now', \"weren't\", 'here', 'too', 'which', 'mightn', 'a', \"wouldn't\", 'in', 'whom', 'from', 'isn', 'hasn', 'y', 'off', 'itself', 'at', 'over', 'out', 'her', 'who', 'themselves', 'aren', 'before', 'through', 'of', 'against', \"that'll\", 'once', 'needn', 'mustn', 'do', 'doesn', 'such', 'won', 'me', 're', 'your', \"couldn't\", 'himself', 'each', \"you'd\", 'him', 'did', 'to', \"isn't\", 'or', 't', 'more', 'down', \"hasn't\", 'don', 've', 'just', \"haven't\", 'this', 'into', 'ma', 'been', \"she's\", 'them', 'can', 'while', 'd', 'yourselves', 'are', 'ours', \"aren't\", 'm', 's', 'has', 'than', 'those', 'for', 'doing', 'by', 'what', 'and', 'how', 'didn', \"mightn't\", 'his', 'below', 'as', 'herself', 'i', \"don't\", 'shan', 'wouldn', \"you've\", 'wasn', 'weren', 'all', 'between', 'hadn', 'have'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords') #only need to do this once\n",
        "stops = set(stopwords.words('english'))\n",
        "print(stops)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeI0Jm3SlCfj"
      },
      "source": [
        "## WordNet Lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-122H6POlCfj",
        "outputId": "d0821695-f796-460e-b186-59d4b0857a65",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "deposit : deposit\n",
            "deposits: deposit\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "#nltk.download('wordnet') #do it once\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"deposit :\", lemmatizer.lemmatize(\"deposit\"))\n",
        "print(\"deposits:\", lemmatizer.lemmatize(\"deposits\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVSiZURElCfj"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "Add stopword removal and lemmatization to your Lesk Algorithm implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gehPP0b7zQGh",
        "outputId": "2e2cde01-7645-4297-a350-1864557edac4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------\n",
            "Here is the tokenized and lemmatized sentence:  ['went', 'bank', 'deposit', 'money', '.']\n",
            "-----------------------------------------------\n",
            "Word Synsets:  [Synset('bank.n.01'), Synset('depository_financial_institution.n.01'), Synset('bank.n.03'), Synset('bank.n.04'), Synset('bank.n.05'), Synset('bank.n.06'), Synset('bank.n.07'), Synset('savings_bank.n.02'), Synset('bank.n.09'), Synset('bank.n.10'), Synset('bank.v.01'), Synset('bank.v.02'), Synset('bank.v.03'), Synset('bank.v.04'), Synset('bank.v.05'), Synset('deposit.v.02'), Synset('bank.v.07'), Synset('trust.v.01')]\n",
            "-----------------------------------------------\n",
            "Signature:  {')', 'sloping', 'on', 'the', 'water', 'canoe', 'beside', 'a', 'up', 'bank', 'land', 'pulled', '(', 'body', 'they', 'especially', 'of', 'slope'}\n",
            "Signature:  {'bank', 'land', 'body', 'especially', 'of', 'sloping', 'beside', 'river', 'watched', 'canoe', 'current', 'and', 'he', '(', 'they', ')', 'on', 'the', 'water', 'a', 'up', 'sat', 'pulled', 'slope'}\n",
            "Signature:  {'check', 'the', 'cashed', 'deposit', 'financial', 'that', 'a', 'institution', 'and', 'channel', 'money', 'into', 'activity', 'he', 'lending', 'bank', 'at', 'accepts'}\n",
            "Signature:  {'hold', 'deposit', 'cashed', 'institution', 'bank', 'channel', 'into', 'activity', 'home', 'lending', 'financial', 'that', 'mortgage', 'my', 'and', 'money', 'he', 'check', 'on', 'the', 'a', 'at', 'accepts'}\n",
            "Signature:  {'or', 'earth', 'a', 'bank', 'huge', 'long', 'pile', 'of', 'ridge'}\n",
            "Signature:  {'similar', 'or', 'object', 'row', 'a', 'an', 'bank', 'arrangement', 'he', 'in', 'tier', 'operated', 'of', 'switch'}\n",
            "Signature:  {'tried', 'Monte', 'bank', 'dealer', 'break', 'Carlo', 'held', 'some', 'gambling', 'house', 'by', 'he', 'fund', 'to', 'or', 'the', 'a', 'in', 'game', 'at'}\n",
            "Signature:  {'slot', 'bank', 'with', 'home', 'empty', 'keeping', 'for', 'wa', 'usually', 'money', 'coin', '(', 'top', ')', 'the', 'a', 'in', 'container', 'at'}\n",
            "Signature:  {'on', 'Witherspoon', 'the', 'transacted', 'business', 'and', 'a', 'bank', 'in', 'banking', 'building', 'is', 'Nassau', 'corner', 'of', 'which'}\n",
            "Signature:  {'bank', 'into', 'especially', 'about', 'aircraft', 'it', 'turning', 'longitudinal', 'laterally', ';', 'steep', 'flight', 'axis', 'went', '(', 'tip', 'plane', ')', 'the', 'a', 'maneuver', 'in'}\n",
            "Signature:  {'aircraft', 'the', 'laterally', 'pilot', 'bank', 'tip', 'to', 'had'}\n",
            "Signature:  {'a', 'enclose', 'road', 'bank', 'with'}\n",
            "Signature:  {'or', 'this', 'business', 'account', 'do', 'a', 'bank', 'an', 'you', 'in', 'with', 'Where', '?', 'keep', 'at', 'town'}\n",
            "Signature:  {'her', 'deposit', 'put', 'account', 'every', 'bank', 'a', 'into', 'month', 'She', 'paycheck'}\n",
            "Signature:  {'ash', 'the', 'burning', 'bank', 'a', 'fire', 'with', 'cover', 'to', 'rate', 'so', 'of', 'control'}\n",
            "Signature:  {'or', 'confidence', 'God', 'in', 'can', 'trust', 'We', 'have', 'faith'}\n",
            "Signature:  {'on', 'or', 'confidence', 'your', 'God', 'in', 'can', 'trust', 'friend', 'Rely', 'We', 'have', 'faith'}\n",
            "Signature:  {'on', 'or', 'confidence', 'education', 'your', 'bank', 'God', 'in', 'good', 'can', 'trust', 'friend', 'Rely', 'We', 'have', 'faith'}\n",
            "Signature:  {'education', 'bank', \"'s\", 'can', 'God', 'good', 'friend', 'We', 'by', 'my', 'your', 'trust', 'Rely', 'faith', 'on', 'or', 'confidence', 'I', 'swear', 'in', 'grandmother', 'recipe', 'have'}\n",
            "-----------------------------------------------\n",
            "Best Sense: depository_financial_institution.n.01\n",
            "-----------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "#nltk.download('punkt')      # Only need to do this once\n",
        "#nltk.download('wordnet')    # Only need to do this once\n",
        "#nltk.download('stopwords')  # Only need to do this once\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def simplified_lesk(word, sentence):\n",
        "    best_sense = None\n",
        "    max_overlap = 0\n",
        "\n",
        "    # Tokenize and lemmatize the sentence\n",
        "    sentence_words = [lemmatizer.lemmatize(word) for word in word_tokenize(sentence) if word.lower() not in stop_words]\n",
        "    print('-----------------------------------------------')\n",
        "    print('Here is the tokenized and lemmatized sentence: ', sentence_words)\n",
        "    print('-----------------------------------------------')\n",
        "\n",
        "    # Get the synsets for the target word\n",
        "    word_synsets = wn.synsets(word)\n",
        "    print('Word Synsets: ', word_synsets)\n",
        "    print('-----------------------------------------------')\n",
        "\n",
        "    for synset in word_synsets:\n",
        "        # Create a signature for the current synset\n",
        "        definition = [lemmatizer.lemmatize(word) for word in word_tokenize(synset.definition())]\n",
        "        signature = set(definition)\n",
        "\n",
        "        for example in synset.examples():\n",
        "            example_words = [lemmatizer.lemmatize(word) for word in word_tokenize(example)]\n",
        "            signature.update(example_words)\n",
        "            print('Signature: ', signature)\n",
        "\n",
        "        # Calculate the overlap between the signature and the word context\n",
        "        overlap = len(signature.intersection(sentence_words))\n",
        "\n",
        "        # If the current overlap is better than the previous best, update best_sense\n",
        "        if overlap > max_overlap:\n",
        "            max_overlap = overlap\n",
        "            best_sense = synset\n",
        "\n",
        "    return best_sense\n",
        "\n",
        "# Example usage:\n",
        "word = \"bank\"\n",
        "sentence = \"I went to the bank to deposit my money.\"\n",
        "\n",
        "best_sense = simplified_lesk(word, sentence)\n",
        "if best_sense:\n",
        "    print('-----------------------------------------------')\n",
        "    print(\"Best Sense:\", best_sense.name())\n",
        "    print('-----------------------------------------------')\n",
        "else:\n",
        "    print('-----------------------------------------------')\n",
        "    print(\"No sense found for the word in the given context.\")\n",
        "    print('-----------------------------------------------')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipDsrAmilCfj"
      },
      "source": [
        "## Dataset for evaluation WSD\n",
        "\n",
        "The SemCor NLTK corpus contains text that has been tagged with WordNet sense (mostly Lemmas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAT8n9KalCfj",
        "outputId": "c0b66dd4-0620-4d49-e728-c62107469224"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package semcor to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('semcor') #do this once\n",
        "from nltk.corpus import semcor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TakjtIAAlCfj",
        "outputId": "4127530e-e46c-4996-a69b-1aac87dcdc67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['brown1/tagfiles/br-a01.xml', 'brown1/tagfiles/br-a02.xml', 'brown1/tagfiles/br-a11.xml', 'brown1/tagfiles/br-a12.xml', 'brown1/tagfiles/br-a13.xml', 'brown1/tagfiles/br-a14.xml', 'brown1/tagfiles/br-a15.xml', 'brown1/tagfiles/br-b13.xml', 'brown1/tagfiles/br-b20.xml', 'brown1/tagfiles/br-c01.xml', 'brown1/tagfiles/br-c02.xml', 'brown1/tagfiles/br-c04.xml', 'brown1/tagfiles/br-d01.xml', 'brown1/tagfiles/br-d02.xml', 'brown1/tagfiles/br-d03.xml', 'brown1/tagfiles/br-d04.xml', 'brown1/tagfiles/br-e01.xml', 'brown1/tagfiles/br-e02.xml', 'brown1/tagfiles/br-e04.xml', 'brown1/tagfiles/br-e21.xml', 'brown1/tagfiles/br-e24.xml', 'brown1/tagfiles/br-e29.xml', 'brown1/tagfiles/br-f03.xml', 'brown1/tagfiles/br-f10.xml', 'brown1/tagfiles/br-f19.xml', 'brown1/tagfiles/br-f43.xml', 'brown1/tagfiles/br-g01.xml', 'brown1/tagfiles/br-g11.xml', 'brown1/tagfiles/br-g15.xml', 'brown1/tagfiles/br-h01.xml', 'brown1/tagfiles/br-j01.xml', 'brown1/tagfiles/br-j02.xml', 'brown1/tagfiles/br-j03.xml', 'brown1/tagfiles/br-j04.xml', 'brown1/tagfiles/br-j05.xml', 'brown1/tagfiles/br-j06.xml', 'brown1/tagfiles/br-j07.xml', 'brown1/tagfiles/br-j08.xml', 'brown1/tagfiles/br-j09.xml', 'brown1/tagfiles/br-j10.xml', 'brown1/tagfiles/br-j11.xml', 'brown1/tagfiles/br-j12.xml', 'brown1/tagfiles/br-j13.xml', 'brown1/tagfiles/br-j14.xml', 'brown1/tagfiles/br-j15.xml', 'brown1/tagfiles/br-j16.xml', 'brown1/tagfiles/br-j17.xml', 'brown1/tagfiles/br-j18.xml', 'brown1/tagfiles/br-j19.xml', 'brown1/tagfiles/br-j20.xml', 'brown1/tagfiles/br-j22.xml', 'brown1/tagfiles/br-j23.xml', 'brown1/tagfiles/br-j37.xml', 'brown1/tagfiles/br-j52.xml', 'brown1/tagfiles/br-j53.xml', 'brown1/tagfiles/br-j54.xml', 'brown1/tagfiles/br-j55.xml', 'brown1/tagfiles/br-j56.xml', 'brown1/tagfiles/br-j57.xml', 'brown1/tagfiles/br-j58.xml', 'brown1/tagfiles/br-j59.xml', 'brown1/tagfiles/br-j60.xml', 'brown1/tagfiles/br-j70.xml', 'brown1/tagfiles/br-k01.xml', 'brown1/tagfiles/br-k02.xml', 'brown1/tagfiles/br-k03.xml', 'brown1/tagfiles/br-k04.xml', 'brown1/tagfiles/br-k05.xml', 'brown1/tagfiles/br-k06.xml', 'brown1/tagfiles/br-k07.xml', 'brown1/tagfiles/br-k08.xml', 'brown1/tagfiles/br-k09.xml', 'brown1/tagfiles/br-k10.xml', 'brown1/tagfiles/br-k11.xml', 'brown1/tagfiles/br-k12.xml', 'brown1/tagfiles/br-k13.xml', 'brown1/tagfiles/br-k14.xml', 'brown1/tagfiles/br-k15.xml', 'brown1/tagfiles/br-k16.xml', 'brown1/tagfiles/br-k17.xml', 'brown1/tagfiles/br-k18.xml', 'brown1/tagfiles/br-k19.xml', 'brown1/tagfiles/br-k20.xml', 'brown1/tagfiles/br-k21.xml', 'brown1/tagfiles/br-k22.xml', 'brown1/tagfiles/br-k23.xml', 'brown1/tagfiles/br-k24.xml', 'brown1/tagfiles/br-k25.xml', 'brown1/tagfiles/br-k26.xml', 'brown1/tagfiles/br-k27.xml', 'brown1/tagfiles/br-k28.xml', 'brown1/tagfiles/br-k29.xml', 'brown1/tagfiles/br-l11.xml', 'brown1/tagfiles/br-l12.xml', 'brown1/tagfiles/br-m01.xml', 'brown1/tagfiles/br-m02.xml', 'brown1/tagfiles/br-n05.xml', 'brown1/tagfiles/br-p01.xml', 'brown1/tagfiles/br-r05.xml', 'brown1/tagfiles/br-r06.xml', 'brown1/tagfiles/br-r07.xml', 'brown1/tagfiles/br-r08.xml', 'brown1/tagfiles/br-r09.xml', 'brown2/tagfiles/br-e22.xml', 'brown2/tagfiles/br-e23.xml', 'brown2/tagfiles/br-e25.xml', 'brown2/tagfiles/br-e26.xml', 'brown2/tagfiles/br-e27.xml', 'brown2/tagfiles/br-e28.xml', 'brown2/tagfiles/br-e30.xml', 'brown2/tagfiles/br-e31.xml', 'brown2/tagfiles/br-f08.xml', 'brown2/tagfiles/br-f13.xml', 'brown2/tagfiles/br-f14.xml', 'brown2/tagfiles/br-f15.xml', 'brown2/tagfiles/br-f16.xml', 'brown2/tagfiles/br-f17.xml', 'brown2/tagfiles/br-f18.xml', 'brown2/tagfiles/br-f20.xml', 'brown2/tagfiles/br-f21.xml', 'brown2/tagfiles/br-f22.xml', 'brown2/tagfiles/br-f23.xml', 'brown2/tagfiles/br-f24.xml', 'brown2/tagfiles/br-f25.xml', 'brown2/tagfiles/br-f33.xml', 'brown2/tagfiles/br-f44.xml', 'brown2/tagfiles/br-g12.xml', 'brown2/tagfiles/br-g14.xml', 'brown2/tagfiles/br-g16.xml', 'brown2/tagfiles/br-g17.xml', 'brown2/tagfiles/br-g18.xml', 'brown2/tagfiles/br-g19.xml', 'brown2/tagfiles/br-g20.xml', 'brown2/tagfiles/br-g21.xml', 'brown2/tagfiles/br-g22.xml', 'brown2/tagfiles/br-g23.xml', 'brown2/tagfiles/br-g28.xml', 'brown2/tagfiles/br-g31.xml', 'brown2/tagfiles/br-g39.xml', 'brown2/tagfiles/br-g43.xml', 'brown2/tagfiles/br-g44.xml', 'brown2/tagfiles/br-h09.xml', 'brown2/tagfiles/br-h11.xml', 'brown2/tagfiles/br-h12.xml', 'brown2/tagfiles/br-h13.xml', 'brown2/tagfiles/br-h14.xml', 'brown2/tagfiles/br-h15.xml', 'brown2/tagfiles/br-h16.xml', 'brown2/tagfiles/br-h17.xml', 'brown2/tagfiles/br-h18.xml', 'brown2/tagfiles/br-h21.xml', 'brown2/tagfiles/br-h24.xml', 'brown2/tagfiles/br-j29.xml', 'brown2/tagfiles/br-j30.xml', 'brown2/tagfiles/br-j31.xml', 'brown2/tagfiles/br-j32.xml', 'brown2/tagfiles/br-j33.xml', 'brown2/tagfiles/br-j34.xml', 'brown2/tagfiles/br-j35.xml', 'brown2/tagfiles/br-j38.xml', 'brown2/tagfiles/br-j41.xml', 'brown2/tagfiles/br-j42.xml', 'brown2/tagfiles/br-l08.xml', 'brown2/tagfiles/br-l09.xml', 'brown2/tagfiles/br-l10.xml', 'brown2/tagfiles/br-l13.xml', 'brown2/tagfiles/br-l14.xml', 'brown2/tagfiles/br-l15.xml', 'brown2/tagfiles/br-l16.xml', 'brown2/tagfiles/br-l17.xml', 'brown2/tagfiles/br-l18.xml', 'brown2/tagfiles/br-n09.xml', 'brown2/tagfiles/br-n10.xml', 'brown2/tagfiles/br-n11.xml', 'brown2/tagfiles/br-n12.xml', 'brown2/tagfiles/br-n14.xml', 'brown2/tagfiles/br-n15.xml', 'brown2/tagfiles/br-n16.xml', 'brown2/tagfiles/br-n17.xml', 'brown2/tagfiles/br-n20.xml', 'brown2/tagfiles/br-p07.xml', 'brown2/tagfiles/br-p09.xml', 'brown2/tagfiles/br-p10.xml', 'brown2/tagfiles/br-p12.xml', 'brown2/tagfiles/br-p24.xml', 'brown2/tagfiles/br-r04.xml', 'brownv/tagfiles/br-a03.xml', 'brownv/tagfiles/br-a04.xml', 'brownv/tagfiles/br-a05.xml', 'brownv/tagfiles/br-a06.xml', 'brownv/tagfiles/br-a07.xml', 'brownv/tagfiles/br-a08.xml', 'brownv/tagfiles/br-a09.xml', 'brownv/tagfiles/br-a10.xml', 'brownv/tagfiles/br-a16.xml', 'brownv/tagfiles/br-a17.xml', 'brownv/tagfiles/br-a18.xml', 'brownv/tagfiles/br-a19.xml', 'brownv/tagfiles/br-a20.xml', 'brownv/tagfiles/br-a21.xml', 'brownv/tagfiles/br-a22.xml', 'brownv/tagfiles/br-a23.xml', 'brownv/tagfiles/br-a24.xml', 'brownv/tagfiles/br-a25.xml', 'brownv/tagfiles/br-a26.xml', 'brownv/tagfiles/br-a27.xml', 'brownv/tagfiles/br-a28.xml', 'brownv/tagfiles/br-a29.xml', 'brownv/tagfiles/br-a30.xml', 'brownv/tagfiles/br-a31.xml', 'brownv/tagfiles/br-a32.xml', 'brownv/tagfiles/br-a33.xml', 'brownv/tagfiles/br-a34.xml', 'brownv/tagfiles/br-a35.xml', 'brownv/tagfiles/br-a36.xml', 'brownv/tagfiles/br-a37.xml', 'brownv/tagfiles/br-a38.xml', 'brownv/tagfiles/br-a39.xml', 'brownv/tagfiles/br-a40.xml', 'brownv/tagfiles/br-a41.xml', 'brownv/tagfiles/br-a42.xml', 'brownv/tagfiles/br-a43.xml', 'brownv/tagfiles/br-a44.xml', 'brownv/tagfiles/br-b01.xml', 'brownv/tagfiles/br-b02.xml', 'brownv/tagfiles/br-b03.xml', 'brownv/tagfiles/br-b04.xml', 'brownv/tagfiles/br-b05.xml', 'brownv/tagfiles/br-b06.xml', 'brownv/tagfiles/br-b07.xml', 'brownv/tagfiles/br-b08.xml', 'brownv/tagfiles/br-b09.xml', 'brownv/tagfiles/br-b10.xml', 'brownv/tagfiles/br-b11.xml', 'brownv/tagfiles/br-b12.xml', 'brownv/tagfiles/br-b14.xml', 'brownv/tagfiles/br-b15.xml', 'brownv/tagfiles/br-b16.xml', 'brownv/tagfiles/br-b17.xml', 'brownv/tagfiles/br-b18.xml', 'brownv/tagfiles/br-b19.xml', 'brownv/tagfiles/br-b21.xml', 'brownv/tagfiles/br-b22.xml', 'brownv/tagfiles/br-b23.xml', 'brownv/tagfiles/br-b24.xml', 'brownv/tagfiles/br-b25.xml', 'brownv/tagfiles/br-b26.xml', 'brownv/tagfiles/br-b27.xml', 'brownv/tagfiles/br-c03.xml', 'brownv/tagfiles/br-c05.xml', 'brownv/tagfiles/br-c06.xml', 'brownv/tagfiles/br-c07.xml', 'brownv/tagfiles/br-c08.xml', 'brownv/tagfiles/br-c09.xml', 'brownv/tagfiles/br-c10.xml', 'brownv/tagfiles/br-c11.xml', 'brownv/tagfiles/br-c12.xml', 'brownv/tagfiles/br-c13.xml', 'brownv/tagfiles/br-c14.xml', 'brownv/tagfiles/br-c15.xml', 'brownv/tagfiles/br-c16.xml', 'brownv/tagfiles/br-c17.xml', 'brownv/tagfiles/br-d05.xml', 'brownv/tagfiles/br-d06.xml', 'brownv/tagfiles/br-d07.xml', 'brownv/tagfiles/br-d08.xml', 'brownv/tagfiles/br-d09.xml', 'brownv/tagfiles/br-d10.xml', 'brownv/tagfiles/br-d11.xml', 'brownv/tagfiles/br-d12.xml', 'brownv/tagfiles/br-d13.xml', 'brownv/tagfiles/br-d14.xml', 'brownv/tagfiles/br-d15.xml', 'brownv/tagfiles/br-d16.xml', 'brownv/tagfiles/br-d17.xml', 'brownv/tagfiles/br-e03.xml', 'brownv/tagfiles/br-e05.xml', 'brownv/tagfiles/br-e06.xml', 'brownv/tagfiles/br-e07.xml', 'brownv/tagfiles/br-e08.xml', 'brownv/tagfiles/br-e09.xml', 'brownv/tagfiles/br-e10.xml', 'brownv/tagfiles/br-e11.xml', 'brownv/tagfiles/br-e12.xml', 'brownv/tagfiles/br-e13.xml', 'brownv/tagfiles/br-e14.xml', 'brownv/tagfiles/br-e15.xml', 'brownv/tagfiles/br-e16.xml', 'brownv/tagfiles/br-e17.xml', 'brownv/tagfiles/br-e18.xml', 'brownv/tagfiles/br-e19.xml', 'brownv/tagfiles/br-e20.xml', 'brownv/tagfiles/br-f01.xml', 'brownv/tagfiles/br-f02.xml', 'brownv/tagfiles/br-f04.xml', 'brownv/tagfiles/br-f05.xml', 'brownv/tagfiles/br-f06.xml', 'brownv/tagfiles/br-f07.xml', 'brownv/tagfiles/br-f09.xml', 'brownv/tagfiles/br-f11.xml', 'brownv/tagfiles/br-f12.xml', 'brownv/tagfiles/br-g02.xml', 'brownv/tagfiles/br-g03.xml', 'brownv/tagfiles/br-g04.xml', 'brownv/tagfiles/br-g05.xml', 'brownv/tagfiles/br-g06.xml', 'brownv/tagfiles/br-g07.xml', 'brownv/tagfiles/br-g08.xml', 'brownv/tagfiles/br-g09.xml', 'brownv/tagfiles/br-g10.xml', 'brownv/tagfiles/br-g13.xml', 'brownv/tagfiles/br-h02.xml', 'brownv/tagfiles/br-h03.xml', 'brownv/tagfiles/br-h04.xml', 'brownv/tagfiles/br-h05.xml', 'brownv/tagfiles/br-h06.xml', 'brownv/tagfiles/br-h07.xml', 'brownv/tagfiles/br-h08.xml', 'brownv/tagfiles/br-h10.xml', 'brownv/tagfiles/br-j21.xml', 'brownv/tagfiles/br-j24.xml', 'brownv/tagfiles/br-j25.xml', 'brownv/tagfiles/br-j26.xml', 'brownv/tagfiles/br-j27.xml', 'brownv/tagfiles/br-j28.xml', 'brownv/tagfiles/br-l01.xml', 'brownv/tagfiles/br-l02.xml', 'brownv/tagfiles/br-l03.xml', 'brownv/tagfiles/br-l04.xml', 'brownv/tagfiles/br-l05.xml', 'brownv/tagfiles/br-l06.xml', 'brownv/tagfiles/br-l07.xml', 'brownv/tagfiles/br-m03.xml', 'brownv/tagfiles/br-m04.xml', 'brownv/tagfiles/br-m05.xml', 'brownv/tagfiles/br-m06.xml', 'brownv/tagfiles/br-n01.xml', 'brownv/tagfiles/br-n02.xml', 'brownv/tagfiles/br-n03.xml', 'brownv/tagfiles/br-n04.xml', 'brownv/tagfiles/br-n06.xml', 'brownv/tagfiles/br-n07.xml', 'brownv/tagfiles/br-n08.xml', 'brownv/tagfiles/br-p02.xml', 'brownv/tagfiles/br-p03.xml', 'brownv/tagfiles/br-p04.xml', 'brownv/tagfiles/br-p05.xml', 'brownv/tagfiles/br-p06.xml', 'brownv/tagfiles/br-p08.xml', 'brownv/tagfiles/br-r01.xml', 'brownv/tagfiles/br-r02.xml', 'brownv/tagfiles/br-r03.xml']\n"
          ]
        }
      ],
      "source": [
        "# Get a list of file identifiers in SemCor\n",
        "file_ids = semcor.fileids()\n",
        "print(file_ids) #looks like they're from the brown dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJdoBB8YlCfk",
        "outputId": "6ac17bfc-391d-4030-a26d-0919cdd29adc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', 'Atlanta', \"'s\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term', 'end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]\n"
          ]
        }
      ],
      "source": [
        "# Access the sense-tagged sentences from a file\n",
        "sentences = semcor.sents(file_ids[0])\n",
        "print(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p81zE7nnlCfk",
        "outputId": "03364711-11d1-4ef2-ec8c-7d5c71698eba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[['The'], Tree(Lemma('group.n.01.group'), [Tree('NE', ['Fulton', 'County', 'Grand', 'Jury'])]), Tree(Lemma('state.v.01.say'), ['said']), Tree(Lemma('friday.n.01.Friday'), ['Friday']), ['an'], Tree(Lemma('probe.n.01.investigation'), ['investigation']), ['of'], Tree(Lemma('atlanta.n.01.Atlanta'), ['Atlanta']), [\"'s\"], Tree(Lemma('late.s.03.recent'), ['recent']), Tree(Lemma('primary.n.01.primary_election'), ['primary', 'election']), Tree(Lemma('produce.v.04.produce'), ['produced']), ['``'], ['no'], Tree(Lemma('evidence.n.01.evidence'), ['evidence']), [\"''\"], ['that'], ['any'], Tree(Lemma('abnormality.n.04.irregularity'), ['irregularities']), Tree(Lemma('happen.v.01.take_place'), ['took', 'place']), ['.']], [['The'], Tree(Lemma('jury.n.01.jury'), ['jury']), Tree(Lemma('far.r.02.far'), ['further']), Tree(Lemma('state.v.01.say'), ['said']), ['in'], Tree(Lemma('term.n.02.term'), ['term']), Tree(Lemma('end.n.02.end'), ['end']), Tree(Lemma('presentment.n.01.presentment'), ['presentments']), ['that'], ['the'], Tree(Lemma('group.n.01.group'), [Tree('NE', ['City', 'Executive', 'Committee'])]), [','], ['which'], Tree(Lemma('own.v.01.have'), ['had']), Tree(Lemma('overall.s.02.overall'), ['over-all']), Tree(Lemma('mission.n.03.charge'), ['charge']), ['of'], ['the'], Tree(Lemma('election.n.01.election'), ['election']), [','], ['``'], Tree(Lemma('deserve.v.01.deserve'), ['deserves']), ['the'], Tree(Lemma('praise.n.01.praise'), ['praise']), ['and'], Tree(Lemma('thanks.n.01.thanks'), ['thanks']), ['of'], ['the'], Tree(Lemma('location.n.01.location'), [Tree('NE', ['City', 'of', 'Atlanta'])]), [\"''\"], ['for'], ['the'], Tree(Lemma('manner.n.01.manner'), ['manner']), ['in'], ['which'], ['the'], Tree(Lemma('election.n.01.election'), ['election']), ['was'], Tree(Lemma('conduct.v.01.conduct'), ['conducted']), ['.']], ...]\n"
          ]
        }
      ],
      "source": [
        "# Access the sense tags for those sentences\n",
        "tags = semcor.tagged_sents(file_ids[0],tag=\"sem\")\n",
        "print(tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KAzFTEflCfk"
      },
      "source": [
        "This is a complex format - notice that some (but not all!) of the words are grouped together in a tree structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cN5mlxSflCfk",
        "outputId": "b18b671a-8ac7-480b-c10b-f9c8c93b09fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['The']\n",
            "(Lemma('group.n.01.group') (NE Fulton County Grand Jury))\n",
            "(Lemma('state.v.01.say') said)\n",
            "(Lemma('friday.n.01.Friday') Friday)\n",
            "['an']\n",
            "(Lemma('probe.n.01.investigation') investigation)\n",
            "['of']\n",
            "(Lemma('atlanta.n.01.Atlanta') Atlanta)\n",
            "[\"'s\"]\n",
            "(Lemma('late.s.03.recent') recent)\n",
            "(Lemma('primary.n.01.primary_election') primary election)\n",
            "(Lemma('produce.v.04.produce') produced)\n",
            "['``']\n",
            "['no']\n",
            "(Lemma('evidence.n.01.evidence') evidence)\n",
            "[\"''\"]\n",
            "['that']\n",
            "['any']\n",
            "(Lemma('abnormality.n.04.irregularity') irregularities)\n",
            "(Lemma('happen.v.01.take_place') took place)\n",
            "['.']\n"
          ]
        }
      ],
      "source": [
        "# tags[0] is the tags for the first sentence, sentence[0]\n",
        "for tag in tags[0]:\n",
        "    print(tag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RegZPRhjlCfk"
      },
      "source": [
        "Notice\n",
        "* Some tokens don't have a tag - stopwords, punctuation, etc. - these show up as a string inside a list\n",
        "* \"Fulton County Grand Jury\" is grouped under Lemma('group.n.01.group')\n",
        "* \"primary election\" is grouped as a compound word with Lemma('primary.n.01.primary_election')\n",
        "\n",
        "This is going to be tough to work with. Here's an attempt to loop through them, match them up wit the word from the sentence, and handle these issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5J0sMKzlCfk",
        "outputId": "da7d83a1-e2c3-46a8-83e2-d4bbc78ea6d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word: The\n",
            "Tag: ['The']\n",
            "\n",
            "Word: Fulton\n",
            "Tag: (Lemma('group.n.01.group') (NE Fulton County Grand Jury))\n",
            "Words in this group: ['Fulton', 'County', 'Grand', 'Jury']\n",
            "\n",
            "Word: said\n",
            "Tag: (Lemma('state.v.01.say') said)\n",
            "\n",
            "Word: Friday\n",
            "Tag: (Lemma('friday.n.01.Friday') Friday)\n",
            "\n",
            "Word: an\n",
            "Tag: ['an']\n",
            "\n",
            "Word: investigation\n",
            "Tag: (Lemma('probe.n.01.investigation') investigation)\n",
            "\n",
            "Word: of\n",
            "Tag: ['of']\n",
            "\n",
            "Word: Atlanta\n",
            "Tag: (Lemma('atlanta.n.01.Atlanta') Atlanta)\n",
            "\n",
            "Word: 's\n",
            "Tag: [\"'s\"]\n",
            "\n",
            "Word: recent\n",
            "Tag: (Lemma('late.s.03.recent') recent)\n",
            "\n",
            "Word: ['primary']\n",
            "Tag: (Lemma('primary.n.01.primary_election') primary election)\n",
            "\n",
            "Word: produced\n",
            "Tag: (Lemma('produce.v.04.produce') produced)\n",
            "\n",
            "Word: ``\n",
            "Tag: ['``']\n",
            "\n",
            "Word: no\n",
            "Tag: ['no']\n",
            "\n",
            "Word: evidence\n",
            "Tag: (Lemma('evidence.n.01.evidence') evidence)\n",
            "\n",
            "Word: ''\n",
            "Tag: [\"''\"]\n",
            "\n",
            "Word: that\n",
            "Tag: ['that']\n",
            "\n",
            "Word: any\n",
            "Tag: ['any']\n",
            "\n",
            "Word: irregularities\n",
            "Tag: (Lemma('abnormality.n.04.irregularity') irregularities)\n",
            "\n",
            "Word: ['took']\n",
            "Tag: (Lemma('happen.v.01.take_place') took place)\n",
            "\n",
            "Word: .\n",
            "Tag: ['.']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# for keeping track of which word and tag we're on\n",
        "word_idx = 0\n",
        "tag_idx = 0\n",
        "\n",
        "while tag_idx < len(tags[0]) and word_idx < len(sentences[0]):\n",
        "    word = sentences[0][word_idx] #the current word\n",
        "    tag = tags[0][tag_idx] #the tag for the current word\n",
        "\n",
        "    # check for tags that got assigned to compound words like primary_election\n",
        "    if len(tag) > 1:\n",
        "        print(\"Word:\",sentences[0][word_idx:(word_idx+len(tag)-1)])\n",
        "        print(\"Tag:\",tag)\n",
        "        word_idx += len(tag) #move to the next word that isn't part of the compound\n",
        "\n",
        "    # for Tree objects, check if it really tagged a word and not a group\n",
        "    elif type(tag) is nltk.Tree and type(tag[0]) is str:\n",
        "        print(\"Word:\",word)\n",
        "        print(\"Tag:\",tag)\n",
        "\n",
        "        # here's how we can get the synset for tags that give us a Lemma\n",
        "        if  type(tag.label()) != str:\n",
        "            actual_sense = tag.label().synset()\n",
        "            #pred_sense = simplified_lesk(word,sentences[0])\n",
        "            #this is where you could check if you correctly matched the actual sense\n",
        "\n",
        "        word_idx += 1 #advance to next word\n",
        "\n",
        "    # check if it's a punctuation/stopword - if we got here, it means tag was not of type nltk.Tree\n",
        "    elif type(tag[0]) is str:\n",
        "        print(\"Word:\",word)\n",
        "        print(\"Tag:\",tag)\n",
        "        word_idx += 1\n",
        "\n",
        "    # If we get gerem it means the Tree contained a group of words, and we can count\n",
        "    # how many with len( tag.leaves() )\n",
        "    else:\n",
        "        print(\"Word:\",word)\n",
        "        print(\"Tag:\",tag)\n",
        "        print(\"Words in this group:\",tag.leaves())\n",
        "        word_idx += len(tag.leaves())\n",
        "    tag_idx += 1\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT58dx301-Et",
        "outputId": "c46c5216-ed9f-4fba-dee8-b0271eb836b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------\n",
            "Here is the tokenized and lemmatized sentence:  ['went', 'bank', 'deposit', 'money', '.']\n",
            "-----------------------------------------------\n",
            "Word Synsets:  [Synset('bank.n.01'), Synset('depository_financial_institution.n.01'), Synset('bank.n.03'), Synset('bank.n.04'), Synset('bank.n.05'), Synset('bank.n.06'), Synset('bank.n.07'), Synset('savings_bank.n.02'), Synset('bank.n.09'), Synset('bank.n.10'), Synset('bank.v.01'), Synset('bank.v.02'), Synset('bank.v.03'), Synset('bank.v.04'), Synset('bank.v.05'), Synset('deposit.v.02'), Synset('bank.v.07'), Synset('trust.v.01')]\n",
            "-----------------------------------------------\n",
            "Signature:  {')', 'sloping', 'on', 'the', 'water', 'canoe', 'beside', 'a', 'up', 'bank', 'land', 'pulled', '(', 'body', 'they', 'especially', 'of', 'slope'}\n",
            "Signature:  {'bank', 'land', 'body', 'especially', 'of', 'sloping', 'beside', 'river', 'watched', 'canoe', 'current', 'and', 'he', '(', 'they', ')', 'on', 'the', 'water', 'a', 'up', 'sat', 'pulled', 'slope'}\n",
            "Signature:  {'check', 'the', 'cashed', 'deposit', 'financial', 'that', 'a', 'institution', 'and', 'channel', 'money', 'into', 'activity', 'he', 'lending', 'bank', 'at', 'accepts'}\n",
            "Signature:  {'hold', 'deposit', 'cashed', 'institution', 'bank', 'channel', 'into', 'activity', 'home', 'lending', 'financial', 'that', 'mortgage', 'my', 'and', 'money', 'he', 'check', 'on', 'the', 'a', 'at', 'accepts'}\n",
            "Signature:  {'or', 'earth', 'a', 'bank', 'huge', 'long', 'pile', 'of', 'ridge'}\n",
            "Signature:  {'similar', 'or', 'object', 'row', 'a', 'an', 'bank', 'arrangement', 'he', 'in', 'tier', 'operated', 'of', 'switch'}\n",
            "Signature:  {'tried', 'Monte', 'bank', 'dealer', 'break', 'Carlo', 'held', 'some', 'gambling', 'house', 'by', 'he', 'fund', 'to', 'or', 'the', 'a', 'in', 'game', 'at'}\n",
            "Signature:  {'slot', 'bank', 'with', 'home', 'empty', 'keeping', 'for', 'wa', 'usually', 'money', 'coin', '(', 'top', ')', 'the', 'a', 'in', 'container', 'at'}\n",
            "Signature:  {'on', 'Witherspoon', 'the', 'transacted', 'business', 'and', 'a', 'bank', 'in', 'banking', 'building', 'is', 'Nassau', 'corner', 'of', 'which'}\n",
            "Signature:  {'bank', 'into', 'especially', 'about', 'aircraft', 'it', 'turning', 'longitudinal', 'laterally', ';', 'steep', 'flight', 'axis', 'went', '(', 'tip', 'plane', ')', 'the', 'a', 'maneuver', 'in'}\n",
            "Signature:  {'aircraft', 'the', 'laterally', 'pilot', 'bank', 'tip', 'to', 'had'}\n",
            "Signature:  {'a', 'enclose', 'road', 'bank', 'with'}\n",
            "Signature:  {'or', 'this', 'business', 'account', 'do', 'a', 'bank', 'an', 'you', 'in', 'with', 'Where', '?', 'keep', 'at', 'town'}\n",
            "Signature:  {'her', 'deposit', 'put', 'account', 'every', 'bank', 'a', 'into', 'month', 'She', 'paycheck'}\n",
            "Signature:  {'ash', 'the', 'burning', 'bank', 'a', 'fire', 'with', 'cover', 'to', 'rate', 'so', 'of', 'control'}\n",
            "Signature:  {'or', 'confidence', 'God', 'in', 'can', 'trust', 'We', 'have', 'faith'}\n",
            "Signature:  {'on', 'or', 'confidence', 'your', 'God', 'in', 'can', 'trust', 'friend', 'Rely', 'We', 'have', 'faith'}\n",
            "Signature:  {'on', 'or', 'confidence', 'education', 'your', 'bank', 'God', 'in', 'good', 'can', 'trust', 'friend', 'Rely', 'We', 'have', 'faith'}\n",
            "Signature:  {'education', 'bank', \"'s\", 'can', 'God', 'good', 'friend', 'We', 'by', 'my', 'your', 'trust', 'Rely', 'faith', 'on', 'or', 'confidence', 'I', 'swear', 'in', 'grandmother', 'recipe', 'have'}\n",
            "-----------------------------------------------\n",
            "Best Sense: depository_financial_institution.n.01\n",
            "-----------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "#nltk.download('punkt')      # Only need to do this once\n",
        "#nltk.download('wordnet')    # Only need to do this once\n",
        "#nltk.download('stopwords')  # Only need to do this once\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def simplified_lesk(word, sentence):\n",
        "    best_sense = None\n",
        "    max_overlap = 0\n",
        "\n",
        "    # Tokenize and lemmatize the sentence\n",
        "    sentence_words = [lemmatizer.lemmatize(word) for word in word_tokenize(sentence) if word.lower() not in stop_words]\n",
        "    print('-----------------------------------------------')\n",
        "    print('Here is the tokenized and lemmatized sentence: ', sentence_words)\n",
        "    print('-----------------------------------------------')\n",
        "\n",
        "    # Get the synsets for the target word\n",
        "    word_synsets = wn.synsets(word)\n",
        "    print('Word Synsets: ', word_synsets)\n",
        "    print('-----------------------------------------------')\n",
        "\n",
        "    for synset in word_synsets:\n",
        "        # Create a signature for the current synset\n",
        "        definition = [lemmatizer.lemmatize(word) for word in word_tokenize(synset.definition())]\n",
        "        signature = set(definition)\n",
        "\n",
        "        for example in synset.examples():\n",
        "            example_words = [lemmatizer.lemmatize(word) for word in word_tokenize(example)]\n",
        "            signature.update(example_words)\n",
        "            print('Signature: ', signature)\n",
        "\n",
        "        # Calculate the overlap between the signature and the word context\n",
        "        overlap = len(signature.intersection(sentence_words))\n",
        "\n",
        "        # If the current overlap is better than the previous best, update best_sense\n",
        "        if overlap > max_overlap:\n",
        "            max_overlap = overlap\n",
        "            best_sense = synset\n",
        "\n",
        "    return best_sense\n",
        "\n",
        "# Example usage:\n",
        "word = \"bank\"\n",
        "sentence = \"I went to the bank to deposit my money.\"\n",
        "\n",
        "best_sense = simplified_lesk(word, sentence)\n",
        "if best_sense:\n",
        "    print('-----------------------------------------------')\n",
        "    print(\"Best Sense:\", best_sense.name())\n",
        "    print('-----------------------------------------------')\n",
        "else:\n",
        "    print('-----------------------------------------------')\n",
        "    print(\"No sense found for the word in the given context.\")\n",
        "    print('-----------------------------------------------')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGebiGOIlCfk"
      },
      "source": [
        "## Applied Exploration\n",
        "\n",
        "For cases where the SemCor dataset has a single word tagged with a WordNet sense, run your `simplified_lesk` code on it and see if it matches. Go through all of the sentences in a particular file_id and compute an accuracy score.\n",
        "\n",
        "Write notes here on what you did and the results you got."
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
