{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "39/39 [==============================] - 0s 794us/step - loss: 321.1952 - accuracy: 0.1653\n",
      "Epoch 2/50\n",
      "39/39 [==============================] - 0s 698us/step - loss: -115.1658 - accuracy: 0.1653\n",
      "Epoch 3/50\n",
      "39/39 [==============================] - 0s 677us/step - loss: -240.7643 - accuracy: 0.1653\n",
      "Epoch 4/50\n",
      "39/39 [==============================] - 0s 612us/step - loss: -371.4720 - accuracy: 0.1653\n",
      "Epoch 5/50\n",
      "39/39 [==============================] - 0s 603us/step - loss: -563.4799 - accuracy: 0.1653\n",
      "Epoch 6/50\n",
      "39/39 [==============================] - 0s 625us/step - loss: -802.7951 - accuracy: 0.1653\n",
      "Epoch 7/50\n",
      "39/39 [==============================] - 0s 623us/step - loss: -1125.9249 - accuracy: 0.1653\n",
      "Epoch 8/50\n",
      "39/39 [==============================] - 0s 617us/step - loss: -1516.2748 - accuracy: 0.1653\n",
      "Epoch 9/50\n",
      "39/39 [==============================] - 0s 1ms/step - loss: -2051.6299 - accuracy: 0.1653\n",
      "Epoch 10/50\n",
      "39/39 [==============================] - 0s 624us/step - loss: -2782.3599 - accuracy: 0.1653\n",
      "Epoch 11/50\n",
      "39/39 [==============================] - 0s 586us/step - loss: -3453.2043 - accuracy: 0.1653\n",
      "Epoch 12/50\n",
      "39/39 [==============================] - 0s 587us/step - loss: -4248.8184 - accuracy: 0.1653\n",
      "Epoch 13/50\n",
      "39/39 [==============================] - 0s 594us/step - loss: -5416.1523 - accuracy: 0.1653\n",
      "Epoch 14/50\n",
      "39/39 [==============================] - 0s 558us/step - loss: -6674.1245 - accuracy: 0.1653\n",
      "Epoch 15/50\n",
      "39/39 [==============================] - 0s 550us/step - loss: -8272.4531 - accuracy: 0.1653\n",
      "Epoch 16/50\n",
      "39/39 [==============================] - 0s 546us/step - loss: -9924.6689 - accuracy: 0.1653\n",
      "Epoch 17/50\n",
      "39/39 [==============================] - 0s 685us/step - loss: -12203.6475 - accuracy: 0.1653\n",
      "Epoch 18/50\n",
      "39/39 [==============================] - 0s 613us/step - loss: -14605.0850 - accuracy: 0.1653\n",
      "Epoch 19/50\n",
      "39/39 [==============================] - 0s 598us/step - loss: -17853.4004 - accuracy: 0.1653\n",
      "Epoch 20/50\n",
      "39/39 [==============================] - 0s 567us/step - loss: -21345.9062 - accuracy: 0.1653\n",
      "Epoch 21/50\n",
      "39/39 [==============================] - 0s 583us/step - loss: -25005.9551 - accuracy: 0.1653\n",
      "Epoch 22/50\n",
      "39/39 [==============================] - 0s 755us/step - loss: -29493.6641 - accuracy: 0.1653\n",
      "Epoch 23/50\n",
      "39/39 [==============================] - 0s 741us/step - loss: -34252.1094 - accuracy: 0.1653\n",
      "Epoch 24/50\n",
      "39/39 [==============================] - 0s 1ms/step - loss: -39242.4648 - accuracy: 0.1653\n",
      "Epoch 25/50\n",
      "39/39 [==============================] - 0s 635us/step - loss: -45096.2930 - accuracy: 0.1653\n",
      "Epoch 26/50\n",
      "39/39 [==============================] - 0s 550us/step - loss: -52074.3242 - accuracy: 0.1653\n",
      "Epoch 27/50\n",
      "39/39 [==============================] - 0s 570us/step - loss: -58980.8477 - accuracy: 0.1653\n",
      "Epoch 28/50\n",
      "39/39 [==============================] - 0s 525us/step - loss: -66051.1562 - accuracy: 0.1653\n",
      "Epoch 29/50\n",
      "39/39 [==============================] - 0s 528us/step - loss: -74397.2734 - accuracy: 0.1653\n",
      "Epoch 30/50\n",
      "39/39 [==============================] - 0s 528us/step - loss: -82623.8516 - accuracy: 0.1653\n",
      "Epoch 31/50\n",
      "39/39 [==============================] - 0s 559us/step - loss: -92119.4141 - accuracy: 0.1653\n",
      "Epoch 32/50\n",
      "39/39 [==============================] - 0s 555us/step - loss: -101472.0000 - accuracy: 0.1653\n",
      "Epoch 33/50\n",
      "39/39 [==============================] - 0s 592us/step - loss: -112501.3516 - accuracy: 0.1653\n",
      "Epoch 34/50\n",
      "39/39 [==============================] - 0s 625us/step - loss: -124298.1094 - accuracy: 0.1653\n",
      "Epoch 35/50\n",
      "39/39 [==============================] - 0s 646us/step - loss: -135765.2344 - accuracy: 0.1653\n",
      "Epoch 36/50\n",
      "39/39 [==============================] - 0s 562us/step - loss: -148528.1562 - accuracy: 0.1653\n",
      "Epoch 37/50\n",
      "39/39 [==============================] - 0s 557us/step - loss: -161584.2812 - accuracy: 0.1653\n",
      "Epoch 38/50\n",
      "39/39 [==============================] - 0s 571us/step - loss: -175850.6094 - accuracy: 0.1653\n",
      "Epoch 39/50\n",
      "39/39 [==============================] - 0s 564us/step - loss: -190652.6719 - accuracy: 0.1653\n",
      "Epoch 40/50\n",
      "39/39 [==============================] - 0s 578us/step - loss: -206321.0938 - accuracy: 0.1653\n",
      "Epoch 41/50\n",
      "39/39 [==============================] - 0s 583us/step - loss: -222676.4531 - accuracy: 0.1653\n",
      "Epoch 42/50\n",
      "39/39 [==============================] - 0s 589us/step - loss: -238665.9062 - accuracy: 0.1653\n",
      "Epoch 43/50\n",
      "39/39 [==============================] - 0s 578us/step - loss: -255604.7188 - accuracy: 0.1653\n",
      "Epoch 44/50\n",
      "39/39 [==============================] - 0s 664us/step - loss: -274299.8750 - accuracy: 0.1653\n",
      "Epoch 45/50\n",
      "39/39 [==============================] - 0s 1ms/step - loss: -294104.1875 - accuracy: 0.1653\n",
      "Epoch 46/50\n",
      "39/39 [==============================] - 0s 722us/step - loss: -312801.0312 - accuracy: 0.1653\n",
      "Epoch 47/50\n",
      "39/39 [==============================] - 0s 559us/step - loss: -333775.9688 - accuracy: 0.1653\n",
      "Epoch 48/50\n",
      "39/39 [==============================] - 0s 643us/step - loss: -355962.1875 - accuracy: 0.1653\n",
      "Epoch 49/50\n",
      "39/39 [==============================] - 0s 573us/step - loss: -378158.5938 - accuracy: 0.1653\n",
      "Epoch 50/50\n",
      "39/39 [==============================] - 0s 570us/step - loss: -399579.2812 - accuracy: 0.1653\n",
      "10/10 [==============================] - 0s 752us/step - loss: -24216.2344 - accuracy: 0.1629\n",
      "Test accuracy: 16.29%\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Define a function to prepare text data using the tokenizer and specified encoded length\n",
    "def prepare_text_data(texts, tokenizer, encoded_length):\n",
    "    tokenized = tokenizer(texts, padding='max_length', truncation=True, max_length=encoded_length, return_tensors=\"np\")\n",
    "    return tokenized\n",
    "\n",
    "# Import the necessary libraries\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "data = load_dataset(\"climate_fever\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_data, test_data = train_test_split(data['test'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the encoding length (e.g., 128, 256, 384, or any value)\n",
    "encoded_length = 384\n",
    "\n",
    "# Prepare data for the neural network\n",
    "train_encoding = prepare_text_data(train_data[\"claim\"], tokenizer, encoded_length)\n",
    "test_encoding = prepare_text_data(test_data[\"claim\"], tokenizer, encoded_length)\n",
    "train_labels = train_data[\"claim_label\"]\n",
    "test_labels = test_data[\"claim_label\"]\n",
    "\n",
    "# Convert data to arrays\n",
    "train_vectors_arrays = train_encoding['input_ids']\n",
    "test_vectors_arrays = test_encoding['input_ids']\n",
    "train_labels_array = np.array(train_labels)\n",
    "test_labels_array = np.array(test_labels)\n",
    "\n",
    "# Create a neural network architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=encoded_length, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the neural network\n",
    "model.fit(train_vectors_arrays, train_labels_array, epochs=50, verbose=1)\n",
    "\n",
    "# Evaluate the neural network on test data\n",
    "loss, accuracy = model.evaluate(test_vectors_arrays, test_labels_array)\n",
    "print(f\"Test accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 120000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 7600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"ag_news\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", 'label': 2}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "\n",
    "# Access the 'train' split\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "# Print the first observation\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DatasetDict' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part 5 - Neural Network for NLP/NeuralNetworkClassificationApp.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part%205%20-%20Neural%20Network%20for%20NLP/NeuralNetworkClassificationApp.ipynb#X10sdnNjb2RlLXZmcw%3D%3D?line=0'>1</a>\u001b[0m dataset\u001b[39m.\u001b[39;49mtrain[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DatasetDict' object has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "dataset.train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part 5 - Neural Network for NLP/NeuralNetworkClassificationApp.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part%205%20-%20Neural%20Network%20for%20NLP/NeuralNetworkClassificationApp.ipynb#W4sdnNjb2RlLXZmcw%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Convert labels into integers (0, 1, 2, 3)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part%205%20-%20Neural%20Network%20for%20NLP/NeuralNetworkClassificationApp.ipynb#W4sdnNjb2RlLXZmcw%3D%3D?line=26'>27</a>\u001b[0m label_mapping \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mworld\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSports\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mBusiness\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m2\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSci/Tech\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m3\u001b[39m}\n\u001b[0;32m---> <a href='vscode-notebook-cell://github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part%205%20-%20Neural%20Network%20for%20NLP/NeuralNetworkClassificationApp.ipynb#W4sdnNjb2RlLXZmcw%3D%3D?line=27'>28</a>\u001b[0m train_labels_binary \u001b[39m=\u001b[39m [label_mapping[label] \u001b[39mfor\u001b[39;49;00m label \u001b[39min\u001b[39;49;00m train_labels]\n\u001b[1;32m     <a href='vscode-notebook-cell://github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part%205%20-%20Neural%20Network%20for%20NLP/NeuralNetworkClassificationApp.ipynb#W4sdnNjb2RlLXZmcw%3D%3D?line=28'>29</a>\u001b[0m test_labels_binary \u001b[39m=\u001b[39m [label_mapping[label] \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m test_labels]\n\u001b[1;32m     <a href='vscode-notebook-cell://github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part%205%20-%20Neural%20Network%20for%20NLP/NeuralNetworkClassificationApp.ipynb#W4sdnNjb2RlLXZmcw%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# Convert values into arrays\u001b[39;00m\n",
      "\u001b[1;32m/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part 5 - Neural Network for NLP/NeuralNetworkClassificationApp.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part%205%20-%20Neural%20Network%20for%20NLP/NeuralNetworkClassificationApp.ipynb#W4sdnNjb2RlLXZmcw%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Convert labels into integers (0, 1, 2, 3)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part%205%20-%20Neural%20Network%20for%20NLP/NeuralNetworkClassificationApp.ipynb#W4sdnNjb2RlLXZmcw%3D%3D?line=26'>27</a>\u001b[0m label_mapping \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mworld\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSports\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mBusiness\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m2\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSci/Tech\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m3\u001b[39m}\n\u001b[0;32m---> <a href='vscode-notebook-cell://github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part%205%20-%20Neural%20Network%20for%20NLP/NeuralNetworkClassificationApp.ipynb#W4sdnNjb2RlLXZmcw%3D%3D?line=27'>28</a>\u001b[0m train_labels_binary \u001b[39m=\u001b[39m [label_mapping[label] \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m train_labels]\n\u001b[1;32m     <a href='vscode-notebook-cell://github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part%205%20-%20Neural%20Network%20for%20NLP/NeuralNetworkClassificationApp.ipynb#W4sdnNjb2RlLXZmcw%3D%3D?line=28'>29</a>\u001b[0m test_labels_binary \u001b[39m=\u001b[39m [label_mapping[label] \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m test_labels]\n\u001b[1;32m     <a href='vscode-notebook-cell://github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part%205%20-%20Neural%20Network%20for%20NLP/NeuralNetworkClassificationApp.ipynb#W4sdnNjb2RlLXZmcw%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# Convert values into arrays\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Load your dataset\n",
    "data= load_dataset(\"ag_news\")\n",
    "\n",
    "train_texts = data[\"train\"][\"text\"]\n",
    "train_labels = data[\"train\"][\"label\"]\n",
    "test_texts = data[\"test\"][\"text\"]\n",
    "test_labels = data[\"test\"][\"label\"]\n",
    "\n",
    "# Consider top 5000 frequent words\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  \n",
    "vectorizer.fit(train_texts)\n",
    "\n",
    "train_vectors = vectorizer.transform(train_texts)\n",
    "test_vectors = vectorizer.transform(test_texts)\n",
    "\n",
    "# Convert the sklearn vectors to numpy arrays\n",
    "train_vectors_arrays = train_vectors.toarray()\n",
    "test_vectors_arrays = test_vectors.toarray()\n",
    "\n",
    "# Convert labels into integers (0, 1, 2, 3)\n",
    "label_mapping = {\"world\": 0, \"Sports\": 1, \"Business\": 2, \"Sci/Tech\": 3}\n",
    "train_labels_binary = [label_mapping[label] for label in train_labels]\n",
    "test_labels_binary = [label_mapping[label] for label in test_labels]\n",
    "\n",
    "# Convert values into arrays\n",
    "train_labels_array = np.array(train_labels_binary)\n",
    "test_labels_array = np.array(test_labels_binary)\n",
    "\n",
    "# Model Architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=5000, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))  # Output layer with 4 classes\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Convert labels to one-hot encoded vectors for multi-class classification\n",
    "from keras.utils import to_categorical\n",
    "train_labels_onehot = to_categorical(train_labels_array, num_classes=4)\n",
    "test_labels_onehot = to_categorical(test_labels_array, num_classes=4)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_vectors_arrays, train_labels_onehot, epochs=10, verbose=1)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(test_vectors_arrays, test_labels_onehot)\n",
    "print(f\"Test accuracy: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part 5 - Neural Network for NLP/NeuralNetworkClassificationApp.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part%205%20-%20Neural%20Network%20for%20NLP/NeuralNetworkClassificationApp.ipynb#X11sdnNjb2RlLXZmcw%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mText:\u001b[39m\u001b[39m\"\u001b[39m, train_texts[\u001b[39m0\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell://github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part%205%20-%20Neural%20Network%20for%20NLP/NeuralNetworkClassificationApp.ipynb#X11sdnNjb2RlLXZmcw%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLabel:\u001b[39m\u001b[39m\"\u001b[39m, train_labels[\u001b[39m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_texts' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Text:\", train_texts[0])\n",
    "print(\"Label:\", train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in the dataset: {0, 1, 2, 3}\n"
     ]
    }
   ],
   "source": [
    "unique_labels = set(test_labels)\n",
    "print(\"Unique labels in the dataset:\", unique_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in the dataset: {0, 1, 2, 3}\n"
     ]
    }
   ],
   "source": [
    "unique_labels = set(train_labels)\n",
    "print(\"Unique labels in the dataset:\", unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3750/3750 [==============================] - 3s 700us/step - loss: 0.3583 - accuracy: 0.8835\n",
      "Epoch 2/10\n",
      "3750/3750 [==============================] - 3s 672us/step - loss: 0.2324 - accuracy: 0.9203\n",
      "Epoch 3/10\n",
      "3750/3750 [==============================] - 3s 672us/step - loss: 0.2104 - accuracy: 0.9270\n",
      "Epoch 4/10\n",
      "3750/3750 [==============================] - 3s 671us/step - loss: 0.1948 - accuracy: 0.9320\n",
      "Epoch 5/10\n",
      "3750/3750 [==============================] - 3s 672us/step - loss: 0.1814 - accuracy: 0.9362\n",
      "Epoch 6/10\n",
      "3750/3750 [==============================] - 3s 699us/step - loss: 0.1694 - accuracy: 0.9401\n",
      "Epoch 7/10\n",
      "3750/3750 [==============================] - 3s 699us/step - loss: 0.1588 - accuracy: 0.9437\n",
      "Epoch 8/10\n",
      "3750/3750 [==============================] - 3s 677us/step - loss: 0.1484 - accuracy: 0.9480\n",
      "Epoch 9/10\n",
      "3750/3750 [==============================] - 2s 662us/step - loss: 0.1388 - accuracy: 0.9512\n",
      "Epoch 10/10\n",
      "3750/3750 [==============================] - 2s 656us/step - loss: 0.1294 - accuracy: 0.9551\n",
      "238/238 [==============================] - 0s 456us/step - loss: 0.3684 - accuracy: 0.8982\n",
      "Test accuracy: 89.82%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Load your dataset\n",
    "data = load_dataset(\"ag_news\")\n",
    "\n",
    "train_texts  = data[\"train\"][\"text\"]\n",
    "train_labels = data[\"train\"][\"label\"]\n",
    "test_texts   = data[\"test\"][\"text\"]\n",
    "test_labels  = data[\"test\"][\"label\"]\n",
    "\n",
    "# Consider top 5000 frequent words\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "vectorizer.fit(train_texts)\n",
    "\n",
    "train_vectors = vectorizer.transform(train_texts)\n",
    "test_vectors  = vectorizer.transform(test_texts)\n",
    "\n",
    "# Convert the sklearn vectors to numpy arrays\n",
    "train_vectors_arrays = train_vectors.toarray()\n",
    "test_vectors_arrays  = test_vectors.toarray()\n",
    "\n",
    "# Model Architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=5000, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(4,  activation='softmax'))  # Output layer with 4 classes\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Convert labels to one-hot encoded vectors for multi-class classification\n",
    "train_labels_onehot = to_categorical(train_labels, num_classes=4)\n",
    "test_labels_onehot  = to_categorical(test_labels,   num_classes=4)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_vectors_arrays, train_labels_onehot, epochs=10, verbose=1)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(test_vectors_arrays, test_labels_onehot)\n",
    "print(f\"Test accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3000/3000 [==============================] - 6s 2ms/step - loss: 0.3416 - accuracy: 0.8825 - val_loss: 0.2611 - val_accuracy: 0.9127\n",
      "Epoch 2/100\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.2311 - accuracy: 0.9206 - val_loss: 0.2597 - val_accuracy: 0.9122\n",
      "Epoch 3/100\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.1928 - accuracy: 0.9344 - val_loss: 0.2642 - val_accuracy: 0.9110\n",
      "Epoch 4/100\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.1621 - accuracy: 0.9454 - val_loss: 0.2732 - val_accuracy: 0.9121\n",
      "Epoch 5/100\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: 0.1375 - accuracy: 0.9535 - val_loss: 0.2887 - val_accuracy: 0.9101\n",
      "238/238 [==============================] - 0s 604us/step - loss: 0.2686 - accuracy: 0.9103\n",
      "Test accuracy: 91.03%\n",
      "238/238 [==============================] - 0s 544us/step\n",
      "Test accuracy (after argmax): 91.03%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load your dataset\n",
    "data = load_dataset(\"ag_news\")\n",
    "\n",
    "train_texts  = data[\"train\"][\"text\"]\n",
    "train_labels = data[\"train\"][\"label\"]\n",
    "test_texts   = data[\"test\"][\"text\"]\n",
    "test_labels  = data[\"test\"][\"label\"]\n",
    "\n",
    "# Tokenization and TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "vectorizer.fit(train_texts)\n",
    "\n",
    "train_vectors = vectorizer.transform(train_texts)\n",
    "test_vectors  = vectorizer.transform(test_texts)\n",
    "\n",
    "# Convert the sklearn vectors to numpy arrays\n",
    "train_vectors_arrays = train_vectors.toarray()\n",
    "test_vectors_arrays  = test_vectors.toarray()\n",
    "\n",
    "# Convert labels to one-hot encoded vectors for multi-class classification\n",
    "train_labels_onehot = to_categorical(train_labels, num_classes=4)\n",
    "test_labels_onehot  = to_categorical(test_labels, num_classes=4)\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_vectors_arrays, train_labels_onehot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model Architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=5000, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))  # Output layer with 4 classes\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(patience=3, monitor='val_loss', restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(test_vectors_arrays, test_labels_onehot)\n",
    "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(test_vectors_arrays)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "test_accuracy = accuracy_score(test_labels, y_pred_labels)\n",
    "print(f\"Test accuracy (after argmax): {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "Predicted label: 3\n"
     ]
    }
   ],
   "source": [
    "new_text = [\"Your input text goes here\"]\n",
    "new_text_vectors = vectorizer.transform(new_text)\n",
    "new_text_vectors_array = new_text_vectors.toarray()\n",
    "\n",
    "predictions = model.predict(new_text_vectors_array)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "def predict_text(input_text):\n",
    "    new_text = [input_text]\n",
    "    new_text_vectors = vectorizer.transform(new_text)\n",
    "    new_text_vectors_array = new_text_vectors.toarray()\n",
    "    \n",
    "    predictions = model.predict(new_text_vectors_array)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    return predicted_labels\n",
    "\n",
    "\n",
    "input_text = \"Your input text goes here\"\n",
    "predicted_label = predict_text(input_text)\n",
    "print(f\"Predicted label: {predicted_label[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3395 - accuracy: 0.8850 - val_loss: 0.2615 - val_accuracy: 0.9108\n",
      "Epoch 2/100\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.2311 - accuracy: 0.9213 - val_loss: 0.2537 - val_accuracy: 0.9132\n",
      "Epoch 3/100\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.1902 - accuracy: 0.9351 - val_loss: 0.2624 - val_accuracy: 0.9110\n",
      "Epoch 4/100\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: 0.1605 - accuracy: 0.9451 - val_loss: 0.2732 - val_accuracy: 0.9124\n",
      "Epoch 5/100\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: 0.1362 - accuracy: 0.9543 - val_loss: 0.2850 - val_accuracy: 0.9116\n",
      "238/238 [==============================] - 0s 623us/step - loss: 0.2654 - accuracy: 0.9138\n",
      "Test accuracy: 91.38%\n",
      "238/238 [==============================] - 0s 571us/step\n",
      "Test accuracy (after argmax): 91.38%\n",
      "Predicted category: Sci/Tech\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load your dataset\n",
    "data = load_dataset(\"ag_news\")\n",
    "\n",
    "train_texts  = data[\"train\"][\"text\"]\n",
    "train_labels = data[\"train\"][\"label\"]\n",
    "test_texts   = data[\"test\"][\"text\"]\n",
    "test_labels  = data[\"test\"][\"label\"]\n",
    "\n",
    "# Tokenization and TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "vectorizer.fit(train_texts)\n",
    "\n",
    "train_vectors = vectorizer.transform(train_texts)\n",
    "test_vectors  = vectorizer.transform(test_texts)\n",
    "\n",
    "# Convert the sklearn vectors to numpy arrays\n",
    "train_vectors_arrays = train_vectors.toarray()\n",
    "test_vectors_arrays  = test_vectors.toarray()\n",
    "\n",
    "# Convert labels to one-hot encoded vectors for multi-class classification\n",
    "train_labels_onehot = to_categorical(train_labels, num_classes=4)\n",
    "test_labels_onehot  = to_categorical(test_labels, num_classes=4)\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_vectors_arrays, train_labels_onehot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model Architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=5000, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))  # Output layer with 4 classes\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(patience=3, monitor='val_loss', restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(test_vectors_arrays, test_labels_onehot)\n",
    "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(test_vectors_arrays)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "test_accuracy = accuracy_score(test_labels, y_pred_labels)\n",
    "print(f\"Test accuracy (after argmax): {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "category_mapping = {0: 'World', 1: 'Sports', 2: 'Business', 3: 'Sci/Tech'}\n",
    "\n",
    "# Function to predict text\n",
    "def predict_text(input_text):\n",
    "    new_text = [input_text]\n",
    "    new_text_vectors = vectorizer.transform(new_text)\n",
    "    new_text_vectors_array = new_text_vectors.toarray()\n",
    "    \n",
    "    predictions = model.predict(new_text_vectors_array, verbose=0)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Map numerical labels to category names\n",
    "    predicted_category = category_mapping.get(predicted_labels[0], 'Unknown')\n",
    "    \n",
    "    return predicted_category\n",
    "\n",
    "# Prompt for user input and make predictions\n",
    "input_text = input(\"Enter your text: \")\n",
    "predicted_category = predict_text(input_text)\n",
    "print(f\"Predicted category: {predicted_category}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model results and architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/Users/gonzalovaldenebro/Library/CloudStorage/OneDrive-DrakeUniversity/CS 195/Fortnight 5/NeuralNetwork.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/gonzalovaldenebro/Library/CloudStorage/OneDrive-DrakeUniversity/CS 195/Fortnight 5/vectorizer.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the vectorizer\n",
    "joblib.dump(vectorizer, '/Users/gonzalovaldenebro/Library/CloudStorage/OneDrive-DrakeUniversity/CS 195/Fortnight 5/vectorizer.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
