{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C192SOmJS6lw"
      },
      "source": [
        "# CS 195: Natural Language Processing\n",
        "## Conversational Models\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ericmanley/f23-CS195NLP/blob/main/F7_3_ConversationalModels.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9pKTgCQJfn_"
      },
      "source": [
        "## Reference\n",
        "\n",
        "Hugging Face documentation on Blenderbot small: https://huggingface.co/docs/transformers/model_doc/blenderbot-small"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQhsuF9KJfn_"
      },
      "source": [
        "## Reminder: Applied Exploration\n",
        "\n",
        "The applied exploration for this fortnight will be a little different. I want everyone to get some experience fine-tuning an existing model, so this will be the task for the entire fortnight.\n",
        "\n",
        "See the [workshop from last time](https://github.com/ericmanley/F23-CS195NLP/blob/main/F7_1_TransferLearning.ipynb)\n",
        "\n",
        "Fine-tune an existing model with the following requirements\n",
        "* Choose a different starting model - you can use any Hugging Face model, but consider starting with a general one like BART or Llama2.\n",
        "* Choose a different data set - think about something that would be good to include in an application that interests you\n",
        "* Evaluate how well it performed. For sequence-to-sequence model, try going back and using Rouge from Fortnight 1.\n",
        "\n",
        "The Hugging Face NLP course has [examples of fine-tuning for many different tasks](https://huggingface.co/learn/nlp-course/chapter7/1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AEBDZHW2Jfn_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.14.5)\n",
            "Requirement already satisfied: keras in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.13.1)\n",
            "Requirement already satisfied: tensorflow in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.13.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (1.24.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (13.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<2023.9.0,>=2023.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from fsspec[http]<2023.9.0,>=2023.1.0->datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /Users/gonzalovaldenebro/Library/Python/3.11/lib/python/site-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: tensorflow-macos==2.13.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (2.13.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (4.24.3)\n",
            "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /Users/gonzalovaldenebro/Library/Python/3.11/lib/python/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.58.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.13.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (3.0.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/gonzalovaldenebro/Library/Python/3.11/lib/python/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.13.0->tensorflow) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.22.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.3.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install datasets keras tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrpTuO1BJfoA"
      },
      "source": [
        "## Before we get started: Attention Visualizations\n",
        "\n",
        "These are all from the **Attention is all you Need** paper here: https://arxiv.org/pdf/1706.03762.pdf\n",
        "\n",
        "This shows how much attention the word `making` gave to other words in the sequence. Different heads are shown in different hues\n",
        "\n",
        "<div>\n",
        "    <center>\n",
        "        <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/attention_vis1.png?raw=1\">\n",
        "    </center>\n",
        "</div>\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDU4jSx5JfoA"
      },
      "source": [
        "## Three different heads for the same sentence\n",
        "\n",
        "<div>\n",
        "    <center>\n",
        "        <table>\n",
        "            <tr>\n",
        "                <td><img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/attention_vis2a.png?raw=1\" width=350></td>\n",
        "                <td><img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/attention_vis2b.png?raw=1\" width=350></td>\n",
        "                <td><img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/attention_vis2c.png?raw=1\" width=350></td>\n",
        "            </tr>\n",
        "        </table>\n",
        "    </center>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mWUAASYJfoB"
      },
      "source": [
        "## Conversational Models\n",
        "\n",
        "Models used by chat bots are similar to other sequence-to-sequence models (summarization, translation, question answering), but they have been trained on transcripts of dialog."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOc7gH6FJfoB"
      },
      "source": [
        "## Loading up a Conversational Model\n",
        "\n",
        "Blenderbot Small is a small variation that should be relatively fast to fine tune.\n",
        "\n",
        "You can find other variants on the Hugging Face models repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "703LfHvFJfoB",
        "outputId": "6270b389-3db3-4ffc-c02b-c5cc9ddf1095"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBlenderbotSmallForConditionalGeneration.\n",
            "\n",
            "Some layers of TFBlenderbotSmallForConditionalGeneration were not initialized from the model checkpoint at facebook/blenderbot_small-90M and are newly initialized: ['final_logits_bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import tensorflow as tf\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "model_name = \"facebook/blenderbot_small-90M\"\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-a2EQvpdJfoC"
      },
      "source": [
        "### Creating the first input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SGxnyx0IJfoC",
        "outputId": "c676e00b-8624-4486-a403-309c531cf31f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'My friends are cool but they eat too many carbs.'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "UTTERANCE = \"My friends are cool but they eat too many carbs.\"\n",
        "UTTERANCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muBt02ZoJfoC"
      },
      "source": [
        "### Tokenizing the input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0tyyEm9HJfoC",
        "outputId": "4784b959-00c0-4d25-8b67-fb5dd3cb0137"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': <tf.Tensor: shape=(1, 12), dtype=int32, numpy=\n",
              "array([[  42,  643,   46, 1430,   45,   52, 1176,  146,  177,  753, 2430,\n",
              "           5]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 12), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = tokenizer([UTTERANCE], return_tensors=\"tf\")\n",
        "inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YilUom89JfoC"
      },
      "source": [
        "### Generating the model's response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EopBDxJQJfoC",
        "outputId": "831fb50d-e3ed-414c-9caa-1bf4c9cc4abf"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part 7 - Fine-Tunning Pretrained Models/Core Practice/F7_3_ConversationalModels.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part%207%20-%20Fine-Tunning%20Pretrained%20Models/Core%20Practice/F7_3_ConversationalModels.ipynb#X20sdnNjb2RlLXZmcw%3D%3D?line=0'>1</a>\u001b[0m reply_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(input_ids\u001b[39m=\u001b[39;49minputs[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m],attention_mask\u001b[39m=\u001b[39;49minputs[\u001b[39m\"\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell://github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part%207%20-%20Fine-Tunning%20Pretrained%20Models/Core%20Practice/F7_3_ConversationalModels.ipynb#X20sdnNjb2RlLXZmcw%3D%3D?line=1'>2</a>\u001b[0m reply_ids\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/generation/tf_utils.py:976\u001b[0m, in \u001b[0;36mTFGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, seed, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m    968\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    969\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m    973\u001b[0m     )\n\u001b[1;32m    975\u001b[0m     \u001b[39m# 12. run beam search\u001b[39;00m\n\u001b[0;32m--> 976\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_search(\n\u001b[1;32m    977\u001b[0m         input_ids,\n\u001b[1;32m    978\u001b[0m         max_length\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mmax_length,\n\u001b[1;32m    979\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m    980\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m    981\u001b[0m         length_penalty\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mlength_penalty,\n\u001b[1;32m    982\u001b[0m         early_stopping\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mearly_stopping,\n\u001b[1;32m    983\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m    984\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m    985\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m    986\u001b[0m         num_return_sequences\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mnum_return_sequences,\n\u001b[1;32m    987\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m    988\u001b[0m     )\n\u001b[1;32m    990\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m    991\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_beams \u001b[39m<\u001b[39m generation_config\u001b[39m.\u001b[39mnum_return_sequences:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/generation/tf_utils.py:2555\u001b[0m, in \u001b[0;36mTFGenerationMixin.beam_search\u001b[0;34m(self, input_ids, do_sample, max_length, pad_token_id, eos_token_id, length_penalty, early_stopping, logits_processor, logits_warper, num_return_sequences, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)\u001b[0m\n\u001b[1;32m   2531\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m   2532\u001b[0m         cur_len,\n\u001b[1;32m   2533\u001b[0m         next_running_sequences,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2540\u001b[0m         next_model_kwargs,\n\u001b[1;32m   2541\u001b[0m     )\n\u001b[1;32m   2543\u001b[0m \u001b[39m# 5. run generation\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[39m# 1st generation step has to be run before to initialize `past_key_values` (if active)\u001b[39;00m\n\u001b[1;32m   2545\u001b[0m (\n\u001b[1;32m   2546\u001b[0m     cur_len,\n\u001b[1;32m   2547\u001b[0m     running_sequences,\n\u001b[1;32m   2548\u001b[0m     running_scores,\n\u001b[1;32m   2549\u001b[0m     running_beam_indices,\n\u001b[1;32m   2550\u001b[0m     sequences,\n\u001b[1;32m   2551\u001b[0m     scores,\n\u001b[1;32m   2552\u001b[0m     beam_indices,\n\u001b[1;32m   2553\u001b[0m     is_sent_finished,\n\u001b[1;32m   2554\u001b[0m     model_kwargs,\n\u001b[0;32m-> 2555\u001b[0m ) \u001b[39m=\u001b[39m beam_search_body_fn(\n\u001b[1;32m   2556\u001b[0m     cur_len,\n\u001b[1;32m   2557\u001b[0m     running_sequences,\n\u001b[1;32m   2558\u001b[0m     running_scores,\n\u001b[1;32m   2559\u001b[0m     running_beam_indices,\n\u001b[1;32m   2560\u001b[0m     sequences,\n\u001b[1;32m   2561\u001b[0m     scores,\n\u001b[1;32m   2562\u001b[0m     beam_indices,\n\u001b[1;32m   2563\u001b[0m     is_sent_finished,\n\u001b[1;32m   2564\u001b[0m     model_kwargs,\n\u001b[1;32m   2565\u001b[0m )\n\u001b[1;32m   2567\u001b[0m \u001b[39m# 2-to-n generation steps can then be run in autoregressive fashion (only in case 1st generation step does\u001b[39;00m\n\u001b[1;32m   2568\u001b[0m \u001b[39m# NOT yield EOS token though)\u001b[39;00m\n\u001b[1;32m   2569\u001b[0m maximum_iterations \u001b[39m=\u001b[39m max_length \u001b[39m-\u001b[39m cur_len\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/generation/tf_utils.py:2363\u001b[0m, in \u001b[0;36mTFGenerationMixin.beam_search.<locals>.beam_search_body_fn\u001b[0;34m(cur_len, running_sequences, running_scores, running_beam_indices, sequences, scores, beam_indices, is_sent_finished, model_kwargs)\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[39m# 2. Compute log probs\u001b[39;00m\n\u001b[1;32m   2360\u001b[0m \u001b[39m# get log probabilities from logits, process logits with processors (*e.g.* min_length, ...), and\u001b[39;00m\n\u001b[1;32m   2361\u001b[0m \u001b[39m# add new logprobs to existing running logprobs scores.\u001b[39;00m\n\u001b[1;32m   2362\u001b[0m log_probs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mlog_softmax(logits)\n\u001b[0;32m-> 2363\u001b[0m log_probs \u001b[39m=\u001b[39m logits_processor(flatten_beam_dim(running_sequences), flatten_beam_dim(log_probs), cur_len)\n\u001b[1;32m   2364\u001b[0m log_probs \u001b[39m=\u001b[39m unflatten_beam_dim(log_probs, num_beams)\n\u001b[1;32m   2365\u001b[0m log_probs_processed \u001b[39m=\u001b[39m log_probs\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/generation/tf_logits_process.py:94\u001b[0m, in \u001b[0;36mTFLogitsProcessorList.__call__\u001b[0;34m(self, input_ids, scores, cur_len, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m         scores \u001b[39m=\u001b[39m processor(input_ids, scores, cur_len, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     93\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m         scores \u001b[39m=\u001b[39m processor(input_ids, scores, cur_len)\n\u001b[1;32m     95\u001b[0m \u001b[39mreturn\u001b[39;00m scores\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/generation/tf_logits_process.py:443\u001b[0m, in \u001b[0;36mTFNoRepeatNGramLogitsProcessor.__call__\u001b[0;34m(self, input_ids, scores, cur_len)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39mfor\u001b[39;00m banned_tokens_slice \u001b[39min\u001b[39;00m banned_tokens:\n\u001b[1;32m    439\u001b[0m     banned_tokens_indices_mask\u001b[39m.\u001b[39mappend(\n\u001b[1;32m    440\u001b[0m         [\u001b[39mTrue\u001b[39;00m \u001b[39mif\u001b[39;00m token \u001b[39min\u001b[39;00m banned_tokens_slice \u001b[39melse\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(vocab_size)]\n\u001b[1;32m    441\u001b[0m     )\n\u001b[0;32m--> 443\u001b[0m scores \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mwhere(tf\u001b[39m.\u001b[39;49mconvert_to_tensor(banned_tokens_indices_mask, dtype\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mbool), \u001b[39m-\u001b[39m\u001b[39mfloat\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39minf\u001b[39m\u001b[39m\"\u001b[39m), scores)\n\u001b[1;32m    445\u001b[0m \u001b[39mreturn\u001b[39;00m scores\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion.py:160\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39m@tf_export\u001b[39m\u001b[39m.\u001b[39mtf_export(\u001b[39m\"\u001b[39m\u001b[39mconvert_to_tensor\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m     96\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39madd_dispatch_support\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert_to_tensor_v2_with_dispatch\u001b[39m(\n\u001b[1;32m     98\u001b[0m     value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype_hint\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m     99\u001b[0m ):\n\u001b[1;32m    100\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Converts the given `value` to a `Tensor`.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \n\u001b[1;32m    102\u001b[0m \u001b[39m  This function converts Python objects of various types to `Tensor`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39m    ValueError: If the `value` is a tensor not of given `dtype` in graph mode.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m   \u001b[39mreturn\u001b[39;00m convert_to_tensor_v2(\n\u001b[1;32m    161\u001b[0m       value, dtype\u001b[39m=\u001b[39;49mdtype, dtype_hint\u001b[39m=\u001b[39;49mdtype_hint, name\u001b[39m=\u001b[39;49mname\n\u001b[1;32m    162\u001b[0m   )\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion.py:168\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39m# preferred_dtype = preferred_dtype or dtype_hint\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m \u001b[39mreturn\u001b[39;00m tensor_conversion_registry\u001b[39m.\u001b[39;49mconvert(\n\u001b[1;32m    169\u001b[0m     value, dtype, name, preferred_dtype\u001b[39m=\u001b[39;49mdtype_hint\n\u001b[1;32m    170\u001b[0m )\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    225\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    226\u001b[0m           _add_error_prefix(\n\u001b[1;32m    227\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConversion function \u001b[39m\u001b[39m{\u001b[39;00mconversion_func\u001b[39m!r}\u001b[39;00m\u001b[39m for type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactual = \u001b[39m\u001b[39m{\u001b[39;00mret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    231\u001b[0m               name\u001b[39m=\u001b[39mname))\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[1;32m    236\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m    237\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/ops/array_ops.py:1536\u001b[0m, in \u001b[0;36m_autopacking_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1534\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_autopacking_conversion_function\u001b[39m(v, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, as_ref\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   1535\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Tensor conversion function that automatically packs arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1536\u001b[0m   \u001b[39mif\u001b[39;00m as_ref \u001b[39mor\u001b[39;00m _should_not_autopack(v):\n\u001b[1;32m   1537\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m   1538\u001b[0m   inferred_dtype \u001b[39m=\u001b[39m _get_dtype_from_nested_lists(v)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/ops/array_ops.py:1530\u001b[0m, in \u001b[0;36m_should_not_autopack\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_should_not_autopack\u001b[39m(v):\n\u001b[1;32m   1525\u001b[0m   \u001b[39m# The condition we really want is\u001b[39;00m\n\u001b[1;32m   1526\u001b[0m   \u001b[39m#    any(isinstance(elem, core.Tensor))\u001b[39;00m\n\u001b[1;32m   1527\u001b[0m   \u001b[39m# but it is >5x slower due to abc.ABCMeta.__instancecheck__.\u001b[39;00m\n\u001b[1;32m   1528\u001b[0m   \u001b[39m# pylint: disable=unidiomatic-typecheck\u001b[39;00m\n\u001b[1;32m   1529\u001b[0m   \u001b[39m# TODO(slebedev): add nest.all?\u001b[39;00m\n\u001b[0;32m-> 1530\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mtype\u001b[39m(elem) \u001b[39min\u001b[39;00m _NON_AUTOPACKABLE_TYPES \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m nest\u001b[39m.\u001b[39;49mflatten(v))\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/util/nest.py:289\u001b[0m, in \u001b[0;36mflatten\u001b[0;34m(structure, expand_composites)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mnest.flatten\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    195\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflatten\u001b[39m(structure, expand_composites\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    196\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Returns a flat list from a given structure.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \n\u001b[1;32m    198\u001b[0m \u001b[39m  Refer to [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m    TypeError: The nest is or contains a dict with non-sortable keys.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m   \u001b[39mreturn\u001b[39;00m nest_util\u001b[39m.\u001b[39;49mflatten(\n\u001b[1;32m    290\u001b[0m       nest_util\u001b[39m.\u001b[39;49mModality\u001b[39m.\u001b[39;49mCORE, structure, expand_composites\n\u001b[1;32m    291\u001b[0m   )\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py:701\u001b[0m, in \u001b[0;36mflatten\u001b[0;34m(modality, structure, expand_composites)\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Flattens a nested structure.\u001b[39;00m\n\u001b[1;32m    606\u001b[0m \n\u001b[1;32m    607\u001b[0m \u001b[39m- For Modality.CORE: refer to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[39m  TypeError: The nest is or contains a dict with non-sortable keys.\u001b[39;00m\n\u001b[1;32m    699\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    700\u001b[0m \u001b[39mif\u001b[39;00m modality \u001b[39m==\u001b[39m Modality\u001b[39m.\u001b[39mCORE:\n\u001b[0;32m--> 701\u001b[0m   \u001b[39mreturn\u001b[39;00m _tf_core_flatten(structure, expand_composites)\n\u001b[1;32m    702\u001b[0m \u001b[39melif\u001b[39;00m modality \u001b[39m==\u001b[39m Modality\u001b[39m.\u001b[39mDATA:\n\u001b[1;32m    703\u001b[0m   \u001b[39mreturn\u001b[39;00m _tf_data_flatten(structure)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py:715\u001b[0m, in \u001b[0;36m_tf_core_flatten\u001b[0;34m(structure, expand_composites)\u001b[0m\n\u001b[1;32m    713\u001b[0m   \u001b[39mreturn\u001b[39;00m [\u001b[39mNone\u001b[39;00m]\n\u001b[1;32m    714\u001b[0m expand_composites \u001b[39m=\u001b[39m \u001b[39mbool\u001b[39m(expand_composites)\n\u001b[0;32m--> 715\u001b[0m \u001b[39mreturn\u001b[39;00m _pywrap_utils\u001b[39m.\u001b[39;49mFlatten(structure, expand_composites)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "reply_ids = model.generate(input_ids=inputs[\"input_ids\"],attention_mask=inputs[\"attention_mask\"])\n",
        "reply_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ru9j3jHJfoC",
        "outputId": "e57636ae-e03b-46fd-904f-4cb04656a505"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"what kind of carbs do they eat? i don't know much about carbs.\""
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decoded_reply = tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0]\n",
        "decoded_reply"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVeqASvKJfoD"
      },
      "source": [
        "### Continued turns in the conversation\n",
        "\n",
        "For dialogue, you need to pass the model the entire chat history\n",
        "\n",
        "This model separates the chat messages with special `__start__` and `__end__` tokens to help the model figure out the flow of conversation.\n",
        "\n",
        "Other models might use different separators like `<sep>` or just `\\n`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFxsOU6tJfoD",
        "outputId": "e802b2c4-2b0f-4685-99e3-1ef088e6f5b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"My friends are cool but they eat too many carbs.__end____start__what kind of carbs do they eat? i don't know much about carbs__end__ __start__I'm not sure\""
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "REPLY = \"I'm not sure\"\n",
        "\n",
        "NEXT_UTTERANCE = \"My friends are cool but they eat too many carbs.__end__\"\n",
        "NEXT_UTTERANCE += \"__start__what kind of carbs do they eat? i don't know much about carbs__end__ \"\n",
        "NEXT_UTTERANCE += \"__start__\"+REPLY\n",
        "\n",
        "NEXT_UTTERANCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdnLuI86JfoD",
        "outputId": "348ed54e-9ba2-4715-fc9e-1411b86d0cc7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'they eat a lot of carbs. carbs are high in protein, fats, and fats.'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = tokenizer([NEXT_UTTERANCE], return_tensors=\"tf\")\n",
        "next_reply_ids = model.generate(input_ids=inputs[\"input_ids\"],attention_mask=inputs[\"attention_mask\"])\n",
        "tokenizer.batch_decode(next_reply_ids, skip_special_tokens=True)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hmqt5YMJfoD"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "Write a loop that repeats this automatically. Prompt the user, add the user's input onto the conversation, get the model's reply, add it to the conversation, and so on.\n",
        "\n",
        "Make sure that each time you generate a new response, you pass in the inputs for the entire conversation (the tokenizer should truncate it automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import tensorflow as tf\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from datasets import load_dataset\n",
        "\n",
        "user_input = input(\"Human: \")\n",
        "next_utterance = \"__start__\" + user_input + \"__end__\"\n",
        "while user_input.lower() != \"end\":\n",
        "\n",
        "    inputs = tokenizer([next_utterance], return_tensors=\"tf\")\n",
        "    next_reply_ids = model.generate(input_ids=inputs[\"input_ids\"],attention_mask=inputs[\"attention_mask\"])\n",
        "    bot_response = tokenizer.batch_decode(next_reply_ids, skip_special_tokens=True)[0]\n",
        "    print(\"Bot: \", bot_response)\n",
        "    next_utterance = \"__start__\" + bot_response + \"__end__\"\n",
        "    user_input = input(\"Human: \")\n",
        "    next_utterance = \"__start__\" + user_input + \"__end__\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rfl7jzzCJfoD"
      },
      "source": [
        "## Training for Conversation\n",
        "\n",
        "To train for conversation, you need data that consists of user inputs and responses.\n",
        "\n",
        "This code is essentially the same as our original Fine-Tuning code, but we'll use it with a conversational model `\"facebook/blenderbot_small-90M\"` and a dataset consisting of ChatGPT transcripts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "leLflezpJfoD",
        "outputId": "4a88d442-cfb9-48b7-a47d-fe0b36d6e3c2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBlenderbotSmallForConditionalGeneration.\n",
            "\n",
            "Some layers of TFBlenderbotSmallForConditionalGeneration were not initialized from the model checkpoint at facebook/blenderbot_small-90M and are newly initialized: ['final_logits_bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "838215152b2f4425a96135f8161b13c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/2.15k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "628a119dabcb4ae79dd26d592414dbff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "097f90105b0d46ba8386fe8401fa81c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/986M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05922599792f4c849cbdf792bf3ef68e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98732ad451c14569a04926d0f67245eb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import tensorflow as tf\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "model_name = \"facebook/blenderbot_small-90M\"\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")\n",
        "\n",
        "# I'm using the test split because it is much smaller\n",
        "dataset = load_dataset(\"Open-Orca/SlimOrca\",split=\"train\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0LyYFlDhJfoD"
      },
      "outputs": [],
      "source": [
        "# Shuffle the dataset\n",
        "shuffled_dataset = dataset.shuffle(seed=42)\n",
        "\n",
        "# Select a small sample\n",
        "sample_size = 50  # Define your sample size\n",
        "sample_dataset = shuffled_dataset.select(range(sample_size))\n",
        "\n",
        "#if you want to use the entire dataset just uncomment the following\n",
        "#sample_dataset = shuffled_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "J4_As7qFJfoD",
        "outputId": "2c163e87-0062-4d29-89d8-835deaa9e640"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['conversations'],\n",
              "    num_rows: 50\n",
              "})"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "e0BqZp9-JfoE",
        "outputId": "e5645d71-270e-4f78-a8ef-dd773600dae3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'conversations': [{'from': 'system',\n",
              "   'value': 'You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.',\n",
              "   'weight': None},\n",
              "  {'from': 'human',\n",
              "   'value': 'Alan B. Miller Hall, location, Virginia; Alan B. Miller Hall, owner, College of William & Mary; Mason School of Business, country, United States; Alan B. Miller Hall, currentTenants, Mason School of Business\\n\\nWhat is sentence that verbalizes this data?',\n",
              "   'weight': 0.0},\n",
              "  {'from': 'gpt',\n",
              "   'value': 'Alan B. Miller Hall is a building located in Virginia, United States, and is owned by the College of William & Mary. The Mason School of Business is currently the main tenant of the hall, and they are also part of the same college in the United States.',\n",
              "   'weight': 1.0}]}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#displaying an example conversation\n",
        "sample_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYNHxLs_JfoE"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "The preprocessing step is the biggest difference\n",
        "\n",
        "In this example, I'm choosing to concatenate the system and human prompts with the GPT output as the target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Cz2uCBQjJfoE"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(example):\n",
        "    input_texts = []\n",
        "    target_texts = []\n",
        "\n",
        "    for curr_conv in example['conversations']:\n",
        "\n",
        "        prompt = \"\"\n",
        "\n",
        "        for idx in range(len(curr_conv)-1):\n",
        "            prompt += curr_conv[idx][\"from\"] + \" \"  #should be either \"system\" or \"human\" - theoretically could be an earlier \"gpt\" if there is more than one gpt response\n",
        "            prompt += curr_conv[idx][\"value\"] + \" \" #associated prompt\n",
        "\n",
        "        response = curr_conv[-1][\"value\"] #should be the gpt response\n",
        "\n",
        "        input_texts.append(prompt)\n",
        "        target_texts.append(response)\n",
        "\n",
        "    # Tokenize inputs and targets\n",
        "    model_inputs = tokenizer(input_texts, max_length=512, truncation=True, padding='max_length')\n",
        "    labels = tokenizer(target_texts, max_length=512, truncation=True, padding='max_length')\n",
        "    #move the target tokens into the model_inputs as the \"decoder_input_ids\"\n",
        "    model_inputs[\"decoder_input_ids\"] = labels[\"input_ids\"]\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs_E8SxLJfoE"
      },
      "source": [
        "### Here's what one example looks like preprocessed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YTUsdwWzJfoE",
        "outputId": "ec1e391d-6ee8-4df7-91b0-0b0506609a28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [[423, 15, 46, 12, 10078, 2023, 6, 73, 300, 1492, 5644, 5, 124, 71, 15, 46, 8070, 11, 12, 323, 169, 217, 5, 650, 3546, 354, 5, 3732, 775, 6, 1664, 6, 25176, 318, 337, 118, 3546, 354, 5, 3732, 775, 6, 2380, 6, 422, 10, 894, 553, 694, 332, 118, 5464, 153, 10, 455, 6, 544, 6, 247, 9326, 987, 118, 3546, 354, 5, 3732, 775, 6, 21111, 1602, 12479, 6, 5464, 153, 10, 455, 4, 44, 24, 4720, 22, 1196, 372, 27848, 36, 1419, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'decoder_input_ids': [[3546, 354, 5, 3732, 775, 24, 12, 440, 499, 13, 2067, 6, 247, 271, 6, 9, 24, 1515, 35, 7, 422, 10, 894, 553, 1310, 5, 7, 5464, 153, 10, 455, 24, 1262, 7, 550, 17013, 10, 7, 775, 6, 9, 52, 46, 98, 191, 10, 7, 209, 422, 13, 7, 247, 271, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'labels': [[3546, 354, 5, 3732, 775, 24, 12, 440, 499, 13, 2067, 6, 247, 271, 6, 9, 24, 1515, 35, 7, 422, 10, 894, 553, 1310, 5, 7, 5464, 153, 10, 455, 24, 1262, 7, 550, 17013, 10, 7, 775, 6, 9, 52, 46, 98, 191, 10, 7, 209, 422, 13, 7, 247, 271, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preprocess_function(sample_dataset[0:1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cqk5XRDmJfoE"
      },
      "source": [
        "### We'll use `map` to apply it to the whole dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "0f2ef9a3443943119118a7883f084978"
          ]
        },
        "id": "cAPRiVINJfoE",
        "outputId": "4acffb68-2885-4fe5-f013-34327fbde4be"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7a0e970217143e4a3f00652afba33d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized_dataset = sample_dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "LWqhpQfJJfoE",
        "outputId": "c5b932dd-7a13-4cfe-f593-2f62d4aea881"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['conversations', 'input_ids', 'attention_mask', 'decoder_input_ids', 'labels'],\n",
              "    num_rows: 50\n",
              "})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "MTYdR3LdJfoE",
        "outputId": "b269e389-9cc4-4887-82b9-ccad5ad13182"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'decoder_input_ids', 'labels'],\n",
              "    num_rows: 50\n",
              "})"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_dataset_no_text = tokenized_dataset.remove_columns([\"conversations\"])\n",
        "tokenized_dataset_no_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "L3-tc33KJfoE"
      },
      "outputs": [],
      "source": [
        "tf_train_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_dataset_no_text,\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=True,\n",
        "    batch_size=8,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKaZwVmtJfoE"
      },
      "source": [
        "### Setting up the optimizer in the same way as before\n",
        "\n",
        "The main difference here is that this model needed the SparseCategoricalCrossentropy loss function defined explicitly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "gmpO7LviJfoE"
      },
      "outputs": [],
      "source": [
        "from transformers import create_optimizer\n",
        "import tensorflow as tf\n",
        "\n",
        "num_train_epochs = 8\n",
        "num_train_steps = len(tf_train_dataset) * num_train_epochs\n",
        "\n",
        "optimizer, schedule = create_optimizer(\n",
        "    init_lr=5.6e-5,\n",
        "    num_warmup_steps=0,\n",
        "    num_train_steps=num_train_steps,\n",
        "    weight_decay_rate=0.01,\n",
        ")\n",
        "\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "model.compile(optimizer=optimizer,loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "tnD-8as0JfoE",
        "outputId": "e6f72139-8b6c-4b84-dd6b-c760a8ba8a1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "6/6 [==============================] - 74s 11s/step - loss: 10.7713\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 66s 11s/step - loss: 8.7370\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 65s 11s/step - loss: 7.2659\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 62s 11s/step - loss: 5.9297\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 62s 10s/step - loss: 4.8167\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 66s 11s/step - loss: 3.9749\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 62s 10s/step - loss: 3.4413\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 62s 10s/step - loss: 3.1213\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x2e2dee850>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(tf_train_dataset, epochs=num_train_epochs)"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
