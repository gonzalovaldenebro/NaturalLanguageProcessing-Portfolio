{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C192SOmJS6lw"
      },
      "source": [
        "# CS 195: Natural Language Processing\n",
        "## Conversational Models\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ericmanley/f23-CS195NLP/blob/main/F7_3_ConversationalModels.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9pKTgCQJfn_"
      },
      "source": [
        "## Reference\n",
        "\n",
        "Hugging Face documentation on Blenderbot small: https://huggingface.co/docs/transformers/model_doc/blenderbot-small"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQhsuF9KJfn_"
      },
      "source": [
        "## Reminder: Applied Exploration\n",
        "\n",
        "The applied exploration for this fortnight will be a little different. I want everyone to get some experience fine-tuning an existing model, so this will be the task for the entire fortnight.\n",
        "\n",
        "See the [workshop from last time](https://github.com/ericmanley/F23-CS195NLP/blob/main/F7_1_TransferLearning.ipynb)\n",
        "\n",
        "Fine-tune an existing model with the following requirements\n",
        "* Choose a different starting model - you can use any Hugging Face model, but consider starting with a general one like BART or Llama2.\n",
        "* Choose a different data set - think about something that would be good to include in an application that interests you\n",
        "* Evaluate how well it performed. For sequence-to-sequence model, try going back and using Rouge from Fortnight 1.\n",
        "\n",
        "The Hugging Face NLP course has [examples of fine-tuning for many different tasks](https://huggingface.co/learn/nlp-course/chapter7/1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AEBDZHW2Jfn_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.14.5)\n",
            "Requirement already satisfied: keras in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.13.1)\n",
            "Requirement already satisfied: tensorflow in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.13.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (1.24.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (13.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<2023.9.0,>=2023.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from fsspec[http]<2023.9.0,>=2023.1.0->datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /Users/gonzalovaldenebro/Library/Python/3.11/lib/python/site-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: tensorflow-macos==2.13.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (2.13.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (4.24.3)\n",
            "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /Users/gonzalovaldenebro/Library/Python/3.11/lib/python/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.58.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.13.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (3.0.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/gonzalovaldenebro/Library/Python/3.11/lib/python/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.13.0->tensorflow) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.22.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.3.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install datasets keras tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrpTuO1BJfoA"
      },
      "source": [
        "## Before we get started: Attention Visualizations\n",
        "\n",
        "These are all from the **Attention is all you Need** paper here: https://arxiv.org/pdf/1706.03762.pdf\n",
        "\n",
        "This shows how much attention the word `making` gave to other words in the sequence. Different heads are shown in different hues\n",
        "\n",
        "<div>\n",
        "    <center>\n",
        "        <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/attention_vis1.png?raw=1\">\n",
        "    </center>\n",
        "</div>\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDU4jSx5JfoA"
      },
      "source": [
        "## Three different heads for the same sentence\n",
        "\n",
        "<div>\n",
        "    <center>\n",
        "        <table>\n",
        "            <tr>\n",
        "                <td><img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/attention_vis2a.png?raw=1\" width=350></td>\n",
        "                <td><img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/attention_vis2b.png?raw=1\" width=350></td>\n",
        "                <td><img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/attention_vis2c.png?raw=1\" width=350></td>\n",
        "            </tr>\n",
        "        </table>\n",
        "    </center>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mWUAASYJfoB"
      },
      "source": [
        "## Conversational Models\n",
        "\n",
        "Models used by chat bots are similar to other sequence-to-sequence models (summarization, translation, question answering), but they have been trained on transcripts of dialog."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOc7gH6FJfoB"
      },
      "source": [
        "## Loading up a Conversational Model\n",
        "\n",
        "Blenderbot Small is a small variation that should be relatively fast to fine tune.\n",
        "\n",
        "You can find other variants on the Hugging Face models repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "703LfHvFJfoB",
        "outputId": "6270b389-3db3-4ffc-c02b-c5cc9ddf1095"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBlenderbotSmallForConditionalGeneration.\n",
            "\n",
            "Some layers of TFBlenderbotSmallForConditionalGeneration were not initialized from the model checkpoint at facebook/blenderbot_small-90M and are newly initialized: ['final_logits_bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import tensorflow as tf\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "model_name = \"facebook/blenderbot_small-90M\"\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-a2EQvpdJfoC"
      },
      "source": [
        "### Creating the first input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SGxnyx0IJfoC",
        "outputId": "c676e00b-8624-4486-a403-309c531cf31f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'My friends are cool but they eat too many carbs.'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "UTTERANCE = \"My friends are cool but they eat too many carbs.\"\n",
        "UTTERANCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muBt02ZoJfoC"
      },
      "source": [
        "### Tokenizing the input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0tyyEm9HJfoC",
        "outputId": "4784b959-00c0-4d25-8b67-fb5dd3cb0137"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': <tf.Tensor: shape=(1, 12), dtype=int32, numpy=\n",
              "array([[  42,  643,   46, 1430,   45,   52, 1176,  146,  177,  753, 2430,\n",
              "           5]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 12), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = tokenizer([UTTERANCE], return_tensors=\"tf\")\n",
        "inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YilUom89JfoC"
      },
      "source": [
        "### Generating the model's response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EopBDxJQJfoC",
        "outputId": "831fb50d-e3ed-414c-9caa-1bf4c9cc4abf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 30), dtype=int32, numpy=\n",
              "array([[   1,   44,  444,   10,  753, 2430,   59,   52, 1176,   20,   14,\n",
              "          67,    8,   30,   70,  165,   72,  753, 2430,    5,    2,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0]], dtype=int32)>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reply_ids = model.generate(input_ids=inputs[\"input_ids\"],attention_mask=inputs[\"attention_mask\"])\n",
        "reply_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6ru9j3jHJfoC",
        "outputId": "e57636ae-e03b-46fd-904f-4cb04656a505"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"what kind of carbs do they eat? i don't know much about carbs.\""
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decoded_reply = tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0]\n",
        "decoded_reply"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVeqASvKJfoD"
      },
      "source": [
        "### Continued turns in the conversation\n",
        "\n",
        "For dialogue, you need to pass the model the entire chat history\n",
        "\n",
        "This model separates the chat messages with special `__start__` and `__end__` tokens to help the model figure out the flow of conversation.\n",
        "\n",
        "Other models might use different separators like `<sep>` or just `\\n`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cFxsOU6tJfoD",
        "outputId": "e802b2c4-2b0f-4685-99e3-1ef088e6f5b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"My friends are cool but they eat too many carbs.__end____start__what kind of carbs do they eat? i don't know much about carbs__end__ __start__I'm not sure\""
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "REPLY = \"I'm not sure\"\n",
        "\n",
        "NEXT_UTTERANCE = \"My friends are cool but they eat too many carbs.__end__\"\n",
        "NEXT_UTTERANCE += \"__start__what kind of carbs do they eat? i don't know much about carbs__end__ \"\n",
        "NEXT_UTTERANCE += \"__start__\"+REPLY\n",
        "\n",
        "NEXT_UTTERANCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "AdnLuI86JfoD",
        "outputId": "348ed54e-9ba2-4715-fc9e-1411b86d0cc7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'they eat a lot of carbs. carbs are high in protein, fats, and fats.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = tokenizer([NEXT_UTTERANCE], return_tensors=\"tf\")\n",
        "next_reply_ids = model.generate(input_ids=inputs[\"input_ids\"],attention_mask=inputs[\"attention_mask\"])\n",
        "tokenizer.batch_decode(next_reply_ids, skip_special_tokens=True)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hmqt5YMJfoD"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "Write a loop that repeats this automatically. Prompt the user, add the user's input onto the conversation, get the model's reply, add it to the conversation, and so on.\n",
        "\n",
        "Make sure that each time you generate a new response, you pass in the inputs for the entire conversation (the tokenizer should truncate it automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import tensorflow as tf\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from datasets import load_dataset\n",
        "\n",
        "user_input = input(\"Human: \")\n",
        "next_utterance = \"__start__\" + user_input + \"__end__\"\n",
        "while user_input.lower() != \"end\":\n",
        "\n",
        "    inputs = tokenizer([next_utterance], return_tensors=\"tf\")\n",
        "    next_reply_ids = model.generate(input_ids=inputs[\"input_ids\"],attention_mask=inputs[\"attention_mask\"])\n",
        "    bot_response = tokenizer.batch_decode(next_reply_ids, skip_special_tokens=True)[0]\n",
        "    print(\"Bot: \", bot_response)\n",
        "    next_utterance = \"__start__\" + bot_response + \"__end__\"\n",
        "    user_input = input(\"Human: \")\n",
        "    next_utterance = \"__start__\" + user_input + \"__end__\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rfl7jzzCJfoD"
      },
      "source": [
        "## Training for Conversation\n",
        "\n",
        "To train for conversation, you need data that consists of user inputs and responses.\n",
        "\n",
        "This code is essentially the same as our original Fine-Tuning code, but we'll use it with a conversational model `\"facebook/blenderbot_small-90M\"` and a dataset consisting of ChatGPT transcripts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "leLflezpJfoD",
        "outputId": "4a88d442-cfb9-48b7-a47d-fe0b36d6e3c2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBlenderbotSmallForConditionalGeneration.\n",
            "\n",
            "Some layers of TFBlenderbotSmallForConditionalGeneration were not initialized from the model checkpoint at facebook/blenderbot_small-90M and are newly initialized: ['final_logits_bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import tensorflow as tf\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "model_name = \"facebook/blenderbot_small-90M\"\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")\n",
        "\n",
        "# I'm using the test split because it is much smaller\n",
        "dataset = load_dataset(\"Open-Orca/SlimOrca\",split=\"train\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0LyYFlDhJfoD"
      },
      "outputs": [],
      "source": [
        "# Shuffle the dataset\n",
        "shuffled_dataset = dataset.shuffle(seed=42)\n",
        "\n",
        "# Select a small sample\n",
        "sample_size = 50  # Define your sample size\n",
        "sample_dataset = shuffled_dataset.select(range(sample_size))\n",
        "\n",
        "#if you want to use the entire dataset just uncomment the following\n",
        "#sample_dataset = shuffled_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "J4_As7qFJfoD",
        "outputId": "2c163e87-0062-4d29-89d8-835deaa9e640"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['conversations'],\n",
              "    num_rows: 50\n",
              "})"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "e0BqZp9-JfoE",
        "outputId": "e5645d71-270e-4f78-a8ef-dd773600dae3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'conversations': [{'from': 'system',\n",
              "   'value': 'You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.',\n",
              "   'weight': None},\n",
              "  {'from': 'human',\n",
              "   'value': 'Alan B. Miller Hall, location, Virginia; Alan B. Miller Hall, owner, College of William & Mary; Mason School of Business, country, United States; Alan B. Miller Hall, currentTenants, Mason School of Business\\n\\nWhat is sentence that verbalizes this data?',\n",
              "   'weight': 0.0},\n",
              "  {'from': 'gpt',\n",
              "   'value': 'Alan B. Miller Hall is a building located in Virginia, United States, and is owned by the College of William & Mary. The Mason School of Business is currently the main tenant of the hall, and they are also part of the same college in the United States.',\n",
              "   'weight': 1.0}]}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#displaying an example conversation\n",
        "sample_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYNHxLs_JfoE"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "The preprocessing step is the biggest difference\n",
        "\n",
        "In this example, I'm choosing to concatenate the system and human prompts with the GPT output as the target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Cz2uCBQjJfoE"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(example):\n",
        "    input_texts = []\n",
        "    target_texts = []\n",
        "\n",
        "    for curr_conv in example['conversations']:\n",
        "\n",
        "        prompt = \"\"\n",
        "\n",
        "        for idx in range(len(curr_conv)-1):\n",
        "            prompt += curr_conv[idx][\"from\"] + \" \"  #should be either \"system\" or \"human\" - theoretically could be an earlier \"gpt\" if there is more than one gpt response\n",
        "            prompt += curr_conv[idx][\"value\"] + \" \" #associated prompt\n",
        "\n",
        "        response = curr_conv[-1][\"value\"] #should be the gpt response\n",
        "\n",
        "        input_texts.append(prompt)\n",
        "        target_texts.append(response)\n",
        "\n",
        "    # Tokenize inputs and targets\n",
        "    model_inputs = tokenizer(input_texts, max_length=512, truncation=True, padding='max_length')\n",
        "    labels = tokenizer(target_texts, max_length=512, truncation=True, padding='max_length')\n",
        "    #move the target tokens into the model_inputs as the \"decoder_input_ids\"\n",
        "    model_inputs[\"decoder_input_ids\"] = labels[\"input_ids\"]\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs_E8SxLJfoE"
      },
      "source": [
        "### Here's what one example looks like preprocessed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "YTUsdwWzJfoE",
        "outputId": "ec1e391d-6ee8-4df7-91b0-0b0506609a28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [[423, 15, 46, 12, 10078, 2023, 6, 73, 300, 1492, 5644, 5, 124, 71, 15, 46, 8070, 11, 12, 323, 169, 217, 5, 650, 3546, 354, 5, 3732, 775, 6, 1664, 6, 25176, 318, 337, 118, 3546, 354, 5, 3732, 775, 6, 2380, 6, 422, 10, 894, 553, 694, 332, 118, 5464, 153, 10, 455, 6, 544, 6, 247, 9326, 987, 118, 3546, 354, 5, 3732, 775, 6, 21111, 1602, 12479, 6, 5464, 153, 10, 455, 4, 44, 24, 4720, 22, 1196, 372, 27848, 36, 1419, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'decoder_input_ids': [[3546, 354, 5, 3732, 775, 24, 12, 440, 499, 13, 2067, 6, 247, 271, 6, 9, 24, 1515, 35, 7, 422, 10, 894, 553, 1310, 5, 7, 5464, 153, 10, 455, 24, 1262, 7, 550, 17013, 10, 7, 775, 6, 9, 52, 46, 98, 191, 10, 7, 209, 422, 13, 7, 247, 271, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'labels': [[3546, 354, 5, 3732, 775, 24, 12, 440, 499, 13, 2067, 6, 247, 271, 6, 9, 24, 1515, 35, 7, 422, 10, 894, 553, 1310, 5, 7, 5464, 153, 10, 455, 24, 1262, 7, 550, 17013, 10, 7, 775, 6, 9, 52, 46, 98, 191, 10, 7, 209, 422, 13, 7, 247, 271, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preprocess_function(sample_dataset[0:1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cqk5XRDmJfoE"
      },
      "source": [
        "### We'll use `map` to apply it to the whole dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "0f2ef9a3443943119118a7883f084978"
          ]
        },
        "id": "cAPRiVINJfoE",
        "outputId": "4acffb68-2885-4fe5-f013-34327fbde4be"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = sample_dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "LWqhpQfJJfoE",
        "outputId": "c5b932dd-7a13-4cfe-f593-2f62d4aea881"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['conversations', 'input_ids', 'attention_mask', 'decoder_input_ids', 'labels'],\n",
              "    num_rows: 50\n",
              "})"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "MTYdR3LdJfoE",
        "outputId": "b269e389-9cc4-4887-82b9-ccad5ad13182"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'decoder_input_ids', 'labels'],\n",
              "    num_rows: 50\n",
              "})"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_dataset_no_text = tokenized_dataset.remove_columns([\"conversations\"])\n",
        "tokenized_dataset_no_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "L3-tc33KJfoE"
      },
      "outputs": [],
      "source": [
        "tf_train_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_dataset_no_text,\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=True,\n",
        "    batch_size=8,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKaZwVmtJfoE"
      },
      "source": [
        "### Setting up the optimizer in the same way as before\n",
        "\n",
        "The main difference here is that this model needed the SparseCategoricalCrossentropy loss function defined explicitly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "gmpO7LviJfoE"
      },
      "outputs": [],
      "source": [
        "from transformers import create_optimizer\n",
        "import tensorflow as tf\n",
        "\n",
        "num_train_epochs = 8\n",
        "num_train_steps = len(tf_train_dataset) * num_train_epochs\n",
        "\n",
        "optimizer, schedule = create_optimizer(\n",
        "    init_lr=5.6e-5,\n",
        "    num_warmup_steps=0,\n",
        "    num_train_steps=num_train_steps,\n",
        "    weight_decay_rate=0.01,\n",
        ")\n",
        "\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "model.compile(optimizer=optimizer,loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "tnD-8as0JfoE",
        "outputId": "e6f72139-8b6c-4b84-dd6b-c760a8ba8a1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "6/6 [==============================] - 77s 11s/step - loss: 10.9234\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 67s 11s/step - loss: 8.5955\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 67s 11s/step - loss: 7.1589\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 64s 11s/step - loss: 5.8670\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 68s 11s/step - loss: 4.6445\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 64s 11s/step - loss: 3.7786\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 63s 10s/step - loss: 3.2574\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 63s 11s/step - loss: 2.9622\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x171a63210>"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(tf_train_dataset, epochs=num_train_epochs)"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
