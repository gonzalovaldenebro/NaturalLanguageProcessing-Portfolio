{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/blob/main/F7_1_TransferLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C192SOmJS6lw"
      },
      "source": [
        "# CS 195: Natural Language Processing\n",
        "## Handling Long-Term Information in Recurrent Neural Networks\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ericmanley/f23-CS195NLP/blob/main/F7_1_TransferLearning.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdF8jAluU3ST"
      },
      "source": [
        "## Reference\n",
        "\n",
        "Hugging Face NLP Course Chapter 1: Transformer Models https://huggingface.co/learn/nlp-course/chapter1/1\n",
        "\n",
        "Hugging Face NLP Course Chapter 3: Fine-tuning a model with the Trainer API or Keras https://huggingface.co/learn/nlp-course/chapter3/1\n",
        "\n",
        "Hugging Face NLP Course Chapter 7, Section 5: Summarization https://huggingface.co/learn/nlp-course/chapter7/5?fw=tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "TGPT9A3UU3SU",
        "outputId": "de47b23f-da79-4876-f1b6-f4e562c03483"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.14.5)\n",
            "Requirement already satisfied: keras in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.13.1)\n",
            "Requirement already satisfied: tensorflow in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.13.0)\n",
            "Requirement already satisfied: sentencepiece in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.1.99)\n",
            "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (1.24.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (13.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<2023.9.0,>=2023.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from fsspec[http]<2023.9.0,>=2023.1.0->datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /Users/gonzalovaldenebro/Library/Python/3.11/lib/python/site-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: tensorflow-macos==2.13.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (2.13.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (4.24.3)\n",
            "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /Users/gonzalovaldenebro/Library/Python/3.11/lib/python/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.58.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.13.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (3.0.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/gonzalovaldenebro/Library/Python/3.11/lib/python/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.13.0->tensorflow) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.22.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.3.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install --no-cache-dir datasets keras tensorflow sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8m8cTXgU3SU"
      },
      "source": [
        "## Transfer Learning\n",
        "\n",
        "**Transfer Learning** is the process of taking a model that was trained (**pre-trained**) on one task and then **fine tuned** for another task.\n",
        "\n",
        "Today we're going to practice fine-tuning a pre-trained **transformer** model - we'll cover transformers in more detail next week, but they work a lot like the other neural network models we've looked at so far."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Finq7gVLU3SV"
      },
      "source": [
        "<div>\n",
        "    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/pretraining.svg?raw=1\" width=700>\n",
        "    <br />\n",
        "    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/finetuning.svg?raw=1\" width=700>\n",
        "</div>\n",
        "\n",
        "image source: https://huggingface.co/learn/nlp-course/chapter1/4?fw=tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTBu97yfU3SV"
      },
      "source": [
        "## Common pre-trained models\n",
        "\n",
        "There are a variety of pre-trained models out there\n",
        "* usually *very large*\n",
        "* pretrained on *massive amounts of data*\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/model_parameters.png?raw=1\" width=800>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy590YUYU3SV"
      },
      "source": [
        "**Encoders:** BERT, ALBERT, DistilBERT, ELECTRA, RoBERTa\n",
        "* Usually trained on masked input - model tries to predict the missing word in a sequence\n",
        "\n",
        "\n",
        "**Decoders:** CTRL, GPT, GPT-2, Transformer XL\n",
        "* Neural language models - usually trying to predict the next word in a sequence\n",
        "\n",
        "**Encoder-Decoder Models:** BART, mBART, Marian, T5\n",
        "* full sequence-to-sequence models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDEA1B-KU3SV"
      },
      "source": [
        "## Working Example\n",
        "\n",
        "We're going to work through our text-to-emoji example, fine-tuning a variant of T5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS6uNyR5U3SV"
      },
      "source": [
        "### Load and filter our dataset just like before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Jnzz2tFMU3SV",
        "outputId": "ce1dd9d5-4264-46af-b28a-a42d987af18a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'emoji', 'topic'],\n",
              "    num_rows: 503682\n",
              "})"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "# Define a function to check if 'text' is not None\n",
        "def is_not_none(example):\n",
        "    return example['text'] is not None\n",
        "\n",
        "dataset = load_dataset(\"KomeijiForce/Text2Emoji\",split=\"train\")\n",
        "\n",
        "# Filter the dataset\n",
        "dataset = dataset.filter(is_not_none)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOK4tobeU3SW"
      },
      "source": [
        "### choosing a sample to work with\n",
        "\n",
        "Even the smaller transformer models will take too long to train on in class\n",
        "\n",
        "Let's choose a small sample to work on in class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "q-TBJ3gGU3SW"
      },
      "outputs": [],
      "source": [
        "# Shuffle the dataset\n",
        "shuffled_dataset = dataset.shuffle(seed=42)\n",
        "\n",
        "# Select a small sample\n",
        "sample_size = 1000  # Define your sample size\n",
        "sample_dataset = shuffled_dataset.select(range(sample_size))\n",
        "\n",
        "#if you want to use the entire dataset just uncomment the following\n",
        "#sample_dataset = shuffled_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyP7hLmeU3SW"
      },
      "source": [
        "### Train/test split\n",
        "\n",
        "Hugging Face datasets actually include a `train_test_split` function for splitting into training and testing sets if you don't already have them split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "W19dXtjDU3SW",
        "outputId": "ce833376-a61a-41ee-a4df-870de418dfe0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'emoji', 'topic'],\n",
              "        num_rows: 800\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'emoji', 'topic'],\n",
              "        num_rows: 200\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_split = sample_dataset.train_test_split(test_size=0.2)\n",
        "dataset_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3-J7WZSU3SW"
      },
      "source": [
        "### Reminder of what the data looks like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "rKC-w5pcU3SW",
        "outputId": "eed9d603-2752-47ca-90b7-df0fea498d42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Add a pop of color to your windowsill with the vibrant and flamboyant Bromeliad plant - it's like a fireworks show happening all year round in your own home!\n",
            "üéÜüå∫üí•üåàüí´üåÜüî•üá∫üá∏\n"
          ]
        }
      ],
      "source": [
        "print(dataset_split[\"train\"][\"text\"][46])\n",
        "print(dataset_split[\"train\"][\"emoji\"][46])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueFs7ldcU3SX"
      },
      "source": [
        "### The Tokenizer\n",
        "\n",
        "Since we will be using an existing model to start, we need to make sure we prepare our data in the same way that model was trained on.\n",
        "\n",
        "**T5:** an encoder-decoder Transformer architecture suitable for sequence-to-sequences tasks\n",
        "\n",
        "**mT5:** A multilingual version of T5, pretrained on the multilingual Common Crawl corpus (mC4), covering 101 languages\n",
        "\n",
        "**mt5-small:** A small version of mT5, suitable for getting things working before attempting to train on a large model\n",
        "\n",
        "`mt5-small` uses the SentencePiece tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "h9buCsGSU3SX",
        "outputId": "2a7d53c3-0a76-4c1b-fafc-265a3a3e4c84"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "#uses the sentencepiece tokenizer\n",
        "model_checkpoint = \"google/mt5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLVMwiLTU3SX"
      },
      "source": [
        "### Looking at an example of the tokenization\n",
        "\n",
        "You'll see that the token ids get returned as `input_ids`\n",
        "\n",
        "It also includes an `attention_mask` which allows the algorithm to focus on specific important words using its attention mechanism - it's initialized to all 1s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "FoKiJNr4U3SX",
        "outputId": "823324bf-08b9-4a8e-8057-404587fd0583"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [12808, 259, 262, 9154, 304, 6775, 288, 772, 20727, 8174, 514, 287, 64642, 1052, 305, 97794, 12954, 1052, 24375, 149200, 285, 12017, 259, 264, 609, 277, 263, 1469, 259, 262, 10112, 51053, 3153, 13636, 347, 751, 3721, 25086, 281, 772, 5810, 2586, 309, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = tokenizer(dataset_split[\"train\"][\"text\"][46])\n",
        "inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivaCiDZJU3SX"
      },
      "source": [
        "### Converting ids back to tokens\n",
        "\n",
        "Here's what the tokens look like.\n",
        "\n",
        "The `‚ñÅ` and `</s>` are hallmarks of the SentencePiece tokenizer algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "KAqiJAbUU3SX",
        "outputId": "c5969794-c2a0-47fb-8667-1cfed529f018"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['‚ñÅAdd',\n",
              " '‚ñÅ',\n",
              " 'a',\n",
              " '‚ñÅpop',\n",
              " '‚ñÅof',\n",
              " '‚ñÅcolor',\n",
              " '‚ñÅto',\n",
              " '‚ñÅyour',\n",
              " '‚ñÅwindows',\n",
              " 'ill',\n",
              " '‚ñÅwith',\n",
              " '‚ñÅthe',\n",
              " '‚ñÅvibr',\n",
              " 'ant',\n",
              " '‚ñÅand',\n",
              " '‚ñÅflam',\n",
              " 'boy',\n",
              " 'ant',\n",
              " '‚ñÅBro',\n",
              " 'melia',\n",
              " 'd',\n",
              " '‚ñÅplant',\n",
              " '‚ñÅ',\n",
              " '-',\n",
              " '‚ñÅit',\n",
              " \"'\",\n",
              " 's',\n",
              " '‚ñÅlike',\n",
              " '‚ñÅ',\n",
              " 'a',\n",
              " '‚ñÅfire',\n",
              " 'works',\n",
              " '‚ñÅshow',\n",
              " '‚ñÅhappen',\n",
              " 'ing',\n",
              " '‚ñÅall',\n",
              " '‚ñÅyear',\n",
              " '‚ñÅround',\n",
              " '‚ñÅin',\n",
              " '‚ñÅyour',\n",
              " '‚ñÅown',\n",
              " '‚ñÅhome',\n",
              " '!',\n",
              " '</s>']"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.convert_ids_to_tokens(inputs.input_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUqkC4_dU3SX"
      },
      "source": [
        "### How does it work on the emojis?\n",
        "\n",
        "Fortunately, this seems to work pretty well for the emoji output too\n",
        "\n",
        "some may come back as `<unk>` for unknown tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "BuIYDgU1U3SX",
        "outputId": "b4c257f2-33af-43b0-c8eb-5c306228d44b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [259, 244125, 239199, 191820, 238833, 239328, 248969, 96088, 223546, 238782, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target = tokenizer(dataset_split[\"train\"][\"emoji\"][46])\n",
        "target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "hzY5CW50U3SX",
        "outputId": "866a4430-5f6d-4143-82dc-ade358242ddb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['‚ñÅ', 'üéÜ', 'üå∫', 'üí•', 'üåà', 'üí´', 'üåÜ', 'üî•', 'üá∫', 'üá∏', '</s>']"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.convert_ids_to_tokens(target.input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "OtSvb4fgU3SX",
        "outputId": "4a47a580-8620-4866-b4b4-9593a2765ad3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'üéÜüå∫üí•üåàüí´üåÜüî•üá∫üá∏</s>'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(target.input_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMR8g9fgU3SY"
      },
      "source": [
        "### Let's define a preprocessing function\n",
        "\n",
        "This will allow us to tokenize both the text and labels while allow use to add the token ids from the emojis as the `\"labels\"` key in the overall data structure where it will be convenient to have them for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Y3TX5psMU3SY"
      },
      "outputs": [],
      "source": [
        "max_input_length = 100\n",
        "max_target_length = 20\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    model_inputs = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        max_length=max_input_length,\n",
        "        truncation=True,\n",
        "    )\n",
        "    labels = tokenizer(\n",
        "        examples[\"emoji\"], max_length=max_target_length, truncation=True\n",
        "    )\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXxgOd1aU3SY"
      },
      "source": [
        "Hugging Face datasets have a `map` method that allows you to apply a preprocessing function like this to every example in the data set.\n",
        "\n",
        "Notice that we get everything we had before (text, emoji, topic), but now we also have the input_ids (the tokens), the attention mask, and the labels (also token ids)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "6232d2ec04424a2c86c3010a35fcee6f",
            "993da05b58464d5285aebff90cb5ee75"
          ]
        },
        "id": "VHEqxfrbU3SY",
        "outputId": "53f2742a-21b7-4d13-bd2f-778d556dce23"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0a0e90afb2344dba5fe4d31a837dbce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e13152372c243858b506517a10c7094",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'emoji', 'topic', 'input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 800\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'emoji', 'topic', 'input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 200\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#turn the tokenized data back into a dataset\n",
        "tokenized_datasets = dataset_split.map(preprocess_function, batched=True)\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnUG9CeOU3SY"
      },
      "source": [
        "### Grabbing the pre-trained model\n",
        "\n",
        "as a reminder, `model_checkpoint` was defined earlier - it is `\"google/mt5-small\"`\n",
        "\n",
        "Note that this is an encoder-decoder transformer model the was pretrained on a 750 GB dataset which included tasks for summarization, translation, question answering, and classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "95RPu0nbU3SY",
        "outputId": "7d54fabc-20a5-4433-8e51-9a8bf1bdf2c8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFMT5ForConditionalGeneration.\n",
            "\n",
            "All the layers of TFMT5ForConditionalGeneration were initialized from the model checkpoint at google/mt5-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMT5ForConditionalGeneration for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFAutoModelForSeq2SeqLM\n",
        "\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvS1tZUnU3SY"
      },
      "source": [
        "### Using a data collator\n",
        "\n",
        "Hugging Face provides a Data Collator class which is used to collect the training data into batches and dynamically pad them so that each batch is appropriately padded but without an overall fixed length.\n",
        "\n",
        "With `return_tensors=\"tf\"` we're saying we want the data back in an appropriate data structure suitable for using with Keras/Tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "8OMFNIHXU3SY"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RMJplBHU3SY"
      },
      "source": [
        "Let's make a version of the dataset where the original text fields are removed so we can use it with the collator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "UnaQ_fPpU3SY",
        "outputId": "b411b7cc-b38b-48d8-e819-d519ac0932fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 800\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 200\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_datasets_no_text = tokenized_datasets.remove_columns([\"text\",\"emoji\",\"topic\"])\n",
        "tokenized_datasets_no_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "qLW10hu5U3SY",
        "outputId": "3a0d3b66-59d7-4070-f31e-bdefcac19ef8"
      },
      "outputs": [],
      "source": [
        "tf_train_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_datasets_no_text[\"train\"],\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=True,\n",
        "    batch_size=32,\n",
        ")\n",
        "tf_eval_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_datasets_no_text[\"test\"],\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=False,\n",
        "    batch_size=32,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBw8PAMYU3SY"
      },
      "source": [
        "### Setting up the optimizer\n",
        "\n",
        "When fine-tuning a pre-trained algorithm, you usually want to use a smaller learning rate.\n",
        "\n",
        "Note that we do not specify a loss function - it will use whatever was used in the base model.\n",
        "\n",
        "*NB:* I'm using values that were in the example on the website (https://huggingface.co/learn/nlp-course/chapter7/5?fw=tf ) for a different dataset - I don't know if these are the best for this problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Tfhmz73pU3SY"
      },
      "outputs": [],
      "source": [
        "from transformers import create_optimizer\n",
        "import tensorflow as tf\n",
        "\n",
        "num_train_epochs = 8\n",
        "num_train_steps = len(tf_train_dataset) * num_train_epochs\n",
        "\n",
        "optimizer, schedule = create_optimizer(\n",
        "    init_lr=5.6e-5,\n",
        "    num_warmup_steps=0,\n",
        "    num_train_steps=num_train_steps,\n",
        "    weight_decay_rate=0.01,\n",
        ")\n",
        "\n",
        "model.compile(optimizer=optimizer)\n",
        "\n",
        "# Train in mixed-precision float16 - can be helpful if running on a GPU\n",
        "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "BqKlYuyBU3Sc",
        "outputId": "75d0ab0c-be8d-4d40-e136-7d1da817534d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "25/25 [==============================] - 105s 4s/step - loss: 25.1502 - val_loss: 11.5147\n",
            "Epoch 2/8\n",
            "25/25 [==============================] - 89s 4s/step - loss: 18.4453 - val_loss: 8.4748\n",
            "Epoch 3/8\n",
            "25/25 [==============================] - 87s 3s/step - loss: 15.4057 - val_loss: 7.5587\n",
            "Epoch 4/8\n",
            "25/25 [==============================] - 72s 3s/step - loss: 13.4638 - val_loss: 7.0830\n",
            "Epoch 5/8\n",
            "25/25 [==============================] - 73s 3s/step - loss: 12.3922 - val_loss: 6.8561\n",
            "Epoch 6/8\n",
            "25/25 [==============================] - 70s 3s/step - loss: 11.6430 - val_loss: 6.7182\n",
            "Epoch 7/8\n",
            "25/25 [==============================] - 71s 3s/step - loss: 11.3209 - val_loss: 6.6436\n",
            "Epoch 8/8\n",
            "25/25 [==============================] - 66s 3s/step - loss: 10.9181 - val_loss: 6.6196\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x2ffc116d0>"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFAjwMmAU3Sc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5fQ-UpXU3Sc"
      },
      "source": [
        "### Saving a copy of the model's weights\n",
        "\n",
        "This will allow us to load the model later and work with it without completely retraining."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "PaZ91OFZU3Sc"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "[Errno 30] Read-only file system: 'models'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[1;32m/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/F7_1_TransferLearning.ipynb Cell 44\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/F7_1_TransferLearning.ipynb#X61sdnNjb2RlLXZmcw%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49msave_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mmodels/emoji-model-v2\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:2396\u001b[0m, in \u001b[0;36mTFPreTrainedModel.save_pretrained\u001b[0;34m(self, save_directory, saved_model, version, push_to_hub, signatures, max_shard_size, create_pr, safe_serialization, token, **kwargs)\u001b[0m\n\u001b[1;32m   2393\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProvided path (\u001b[39m\u001b[39m{\u001b[39;00msave_directory\u001b[39m}\u001b[39;00m\u001b[39m) should be a directory, not a file\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2394\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m-> 2396\u001b[0m os\u001b[39m.\u001b[39;49mmakedirs(save_directory, exist_ok\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   2398\u001b[0m \u001b[39mif\u001b[39;00m push_to_hub:\n\u001b[1;32m   2399\u001b[0m     commit_message \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mcommit_message\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
            "File \u001b[0;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
            "File \u001b[0;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: 'models'"
          ]
        }
      ],
      "source": [
        "model.save_pretrained(\"models/emoji-model-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k6vziYjU3Sc"
      },
      "source": [
        "### Reload a saved model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "LFU3PRywU3Sc"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "models/emoji-model-v1 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:261\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 261\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    262\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/requests/models.py:941\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m--> 941\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/models/emoji-model-v1/resolve/main/config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/hub.py:429\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    430\u001b[0m         path_or_repo_id,\n\u001b[1;32m    431\u001b[0m         filename,\n\u001b[1;32m    432\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    433\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    434\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    435\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    436\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    437\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    438\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    439\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    440\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    441\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    442\u001b[0m     )\n\u001b[1;32m    443\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:1195\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1196\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1197\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1198\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1199\u001b[0m         timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m   1200\u001b[0m     )\n\u001b[1;32m   1201\u001b[0m \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n\u001b[1;32m   1202\u001b[0m     \u001b[39m# Cache the non-existence of the file and raise\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:1541\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1532\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1533\u001b[0m     method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mHEAD\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1534\u001b[0m     url\u001b[39m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1539\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m   1540\u001b[0m )\n\u001b[0;32m-> 1541\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1543\u001b[0m \u001b[39m# Return\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:293\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    285\u001b[0m     message \u001b[39m=\u001b[39m (\n\u001b[1;32m    286\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Client Error.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m make sure you are authenticated.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    292\u001b[0m     )\n\u001b[0;32m--> 293\u001b[0m     \u001b[39mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[39melif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m400\u001b[39m:\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-655d2f69-6844401170b569ed34a0bf21;6558a910-0eb4-4234-b9c2-af0a27318fe6)\n\nRepository Not Found for url: https://huggingface.co/models/emoji-model-v1/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[1;32m/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/F7_1_TransferLearning.ipynb Cell 46\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/F7_1_TransferLearning.ipynb#X63sdnNjb2RlLXZmcw%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m TFAutoModelForSeq2SeqLM\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mmodels/emoji-model-v1\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:494\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[39mif\u001b[39;00m commit_hash \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    492\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m    493\u001b[0m         \u001b[39m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m--> 494\u001b[0m         resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    495\u001b[0m             pretrained_model_name_or_path,\n\u001b[1;32m    496\u001b[0m             CONFIG_NAME,\n\u001b[1;32m    497\u001b[0m             _raise_exceptions_for_missing_entries\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m             _raise_exceptions_for_connection_errors\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    499\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs,\n\u001b[1;32m    500\u001b[0m         )\n\u001b[1;32m    501\u001b[0m         commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    502\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/hub.py:450\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    445\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to access a gated repo.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMake sure to request access at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    446\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m and pass a token having permission to this repo either \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    447\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 450\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    451\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    454\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`token=<your_token>`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    455\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[39mexcept\u001b[39;00m RevisionNotFoundError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    457\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    458\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mrevision\u001b[39m}\u001b[39;00m\u001b[39m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfor this model name. Check the model page at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    460\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m for available revisions.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    461\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
            "\u001b[0;31mOSError\u001b[0m: models/emoji-model-v1 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
          ]
        }
      ],
      "source": [
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"models/emoji-model-v1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WM4Wsz4U3Sc"
      },
      "source": [
        "### Inference\n",
        "\n",
        "Let's suppose we have an example to get a prediction for. For now, let's grab one from the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "uL4rDcDoU3Sc",
        "outputId": "af5f3784-25c3-4b43-a6b6-42578db3a13f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking for the perfect gadget to streamline your home organization? You'll love these versatile storage bins.\n",
            "üì¶‚ú®üè†üíï\n",
            "[259, 61408, 332, 287, 5571, 259, 88586, 288, 38101, 1397, 772, 2586, 29660, 291, 1662, 277, 1578, 3869, 259, 3824, 259, 170280, 468, 25973, 3111, 263, 260, 1]\n"
          ]
        }
      ],
      "source": [
        "print( tokenized_datasets[\"test\"][\"text\"][15] )\n",
        "print( tokenized_datasets[\"test\"][\"emoji\"][15] )\n",
        "print( tokenized_datasets[\"test\"][\"input_ids\"][15] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U27sMAi8U3Sc"
      },
      "source": [
        "Use the `generate` method to get a prediction sequence from the intput IDs.\n",
        "\n",
        "If you don't already have the tokens, make sure to use your tokenizer first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "jwG9L7QkU3Sc",
        "outputId": "7de19c0d-d734-4610-ff5e-e6df27091bbb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-21 16:30:08.213844: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x34050cb60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2023-11-21 16:30:08.214001: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2023-11-21 16:30:08.236120: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2023-11-21 16:30:08.338200: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['<pad>', '‚ñÅ<extra_id_0>', '.', '</s>']"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prediction = model.generate([tokenized_datasets[\"test\"][\"input_ids\"][15]], max_length=max_target_length)\n",
        "tokenizer.convert_ids_to_tokens(prediction[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "lX4ZLcb6U3Sc",
        "outputId": "95f595c3-9ecd-4252-d314-ccd5a9e2424a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<extra_id_0>.'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decoded_output = tokenizer.decode(prediction[0], skip_special_tokens=True)\n",
        "decoded_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Sw3k8q-U3Sd"
      },
      "source": [
        "## Applied Exploration\n",
        "\n",
        "The applied exploration for this fortnight will be a little different. I want everyone to get some experience fine-tuning an existing model, so this will be the task for the entire fortnight.\n",
        "\n",
        "Fine-tune an existing model with the following requirements\n",
        "* Choose a different starting model - you can use any Hugging Face model, but consider starting with a general one like BART or Llama2.\n",
        "* Choose a different data set - think about something that would be good to include in an application that interests you\n",
        "* Evaluate how well it performed. For sequence-to-sequence model, try going back and using Rouge from Fortnight 1."
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
