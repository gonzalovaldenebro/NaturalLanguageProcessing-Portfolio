{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/blob/main/F7_1_TransferLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C192SOmJS6lw"
      },
      "source": [
        "# CS 195: Natural Language Processing\n",
        "## Handling Long-Term Information in Recurrent Neural Networks\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ericmanley/f23-CS195NLP/blob/main/F7_1_TransferLearning.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdF8jAluU3ST"
      },
      "source": [
        "## Reference\n",
        "\n",
        "Hugging Face NLP Course Chapter 1: Transformer Models https://huggingface.co/learn/nlp-course/chapter1/1\n",
        "\n",
        "Hugging Face NLP Course Chapter 3: Fine-tuning a model with the Trainer API or Keras https://huggingface.co/learn/nlp-course/chapter3/1\n",
        "\n",
        "Hugging Face NLP Course Chapter 7, Section 5: Summarization https://huggingface.co/learn/nlp-course/chapter7/5?fw=tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "TGPT9A3UU3SU",
        "outputId": "de47b23f-da79-4876-f1b6-f4e562c03483"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.14.5)\n",
            "Requirement already satisfied: keras in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.13.1)\n",
            "Requirement already satisfied: tensorflow in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.13.0)\n",
            "Requirement already satisfied: sentencepiece in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.1.99)\n",
            "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (1.24.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (13.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<2023.9.0,>=2023.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from fsspec[http]<2023.9.0,>=2023.1.0->datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /Users/gonzalovaldenebro/Library/Python/3.11/lib/python/site-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: tensorflow-macos==2.13.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (2.13.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (4.24.3)\n",
            "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /Users/gonzalovaldenebro/Library/Python/3.11/lib/python/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.58.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.13.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (3.0.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/gonzalovaldenebro/Library/Python/3.11/lib/python/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.13.0->tensorflow) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.22.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.3.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install --no-cache-dir datasets keras tensorflow sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8m8cTXgU3SU"
      },
      "source": [
        "## Transfer Learning\n",
        "\n",
        "**Transfer Learning** is the process of taking a model that was trained (**pre-trained**) on one task and then **fine tuned** for another task.\n",
        "\n",
        "Today we're going to practice fine-tuning a pre-trained **transformer** model - we'll cover transformers in more detail next week, but they work a lot like the other neural network models we've looked at so far."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Finq7gVLU3SV"
      },
      "source": [
        "<div>\n",
        "    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/pretraining.svg?raw=1\" width=700>\n",
        "    <br />\n",
        "    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/finetuning.svg?raw=1\" width=700>\n",
        "</div>\n",
        "\n",
        "image source: https://huggingface.co/learn/nlp-course/chapter1/4?fw=tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTBu97yfU3SV"
      },
      "source": [
        "## Common pre-trained models\n",
        "\n",
        "There are a variety of pre-trained models out there\n",
        "* usually *very large*\n",
        "* pretrained on *massive amounts of data*\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/model_parameters.png?raw=1\" width=800>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy590YUYU3SV"
      },
      "source": [
        "**Encoders:** BERT, ALBERT, DistilBERT, ELECTRA, RoBERTa\n",
        "* Usually trained on masked input - model tries to predict the missing word in a sequence\n",
        "\n",
        "\n",
        "**Decoders:** CTRL, GPT, GPT-2, Transformer XL\n",
        "* Neural language models - usually trying to predict the next word in a sequence\n",
        "\n",
        "**Encoder-Decoder Models:** BART, mBART, Marian, T5\n",
        "* full sequence-to-sequence models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDEA1B-KU3SV"
      },
      "source": [
        "## Working Example\n",
        "\n",
        "We're going to work through our text-to-emoji example, fine-tuning a variant of T5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS6uNyR5U3SV"
      },
      "source": [
        "### Load and filter our dataset just like before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "Jnzz2tFMU3SV",
        "outputId": "ce1dd9d5-4264-46af-b28a-a42d987af18a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'emoji', 'topic'],\n",
              "    num_rows: 503682\n",
              "})"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "# Define a function to check if 'text' is not None\n",
        "def is_not_none(example):\n",
        "    return example['text'] is not None\n",
        "\n",
        "dataset = load_dataset(\"KomeijiForce/Text2Emoji\",split=\"train\")\n",
        "\n",
        "# Filter the dataset\n",
        "dataset = dataset.filter(is_not_none)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOK4tobeU3SW"
      },
      "source": [
        "### choosing a sample to work with\n",
        "\n",
        "Even the smaller transformer models will take too long to train on in class\n",
        "\n",
        "Let's choose a small sample to work on in class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "q-TBJ3gGU3SW"
      },
      "outputs": [],
      "source": [
        "# Shuffle the dataset\n",
        "shuffled_dataset = dataset.shuffle(seed=42)\n",
        "\n",
        "# Select a small sample\n",
        "sample_size = 1000  # Define your sample size\n",
        "sample_dataset = shuffled_dataset.select(range(sample_size))\n",
        "\n",
        "#if you want to use the entire dataset just uncomment the following\n",
        "#sample_dataset = shuffled_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyP7hLmeU3SW"
      },
      "source": [
        "### Train/test split\n",
        "\n",
        "Hugging Face datasets actually include a `train_test_split` function for splitting into training and testing sets if you don't already have them split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "W19dXtjDU3SW",
        "outputId": "ce833376-a61a-41ee-a4df-870de418dfe0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'emoji', 'topic'],\n",
              "        num_rows: 800\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'emoji', 'topic'],\n",
              "        num_rows: 200\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_split = sample_dataset.train_test_split(test_size=0.2)\n",
        "dataset_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3-J7WZSU3SW"
      },
      "source": [
        "### Reminder of what the data looks like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "rKC-w5pcU3SW",
        "outputId": "eed9d603-2752-47ca-90b7-df0fea498d42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La lluvia est llegando con fuerza, es mejor quedarse adentro y disfrutar de una buena jornada de pelculas y mantita.\n",
            "‚òîüè†üé•üçøüåà\n"
          ]
        }
      ],
      "source": [
        "print(dataset_split[\"train\"][\"text\"][46])\n",
        "print(dataset_split[\"train\"][\"emoji\"][46])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueFs7ldcU3SX"
      },
      "source": [
        "### The Tokenizer\n",
        "\n",
        "Since we will be using an existing model to start, we need to make sure we prepare our data in the same way that model was trained on.\n",
        "\n",
        "**T5:** an encoder-decoder Transformer architecture suitable for sequence-to-sequences tasks\n",
        "\n",
        "**mT5:** A multilingual version of T5, pretrained on the multilingual Common Crawl corpus (mC4), covering 101 languages\n",
        "\n",
        "**mt5-small:** A small version of mT5, suitable for getting things working before attempting to train on a large model\n",
        "\n",
        "`mt5-small` uses the SentencePiece tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "h9buCsGSU3SX",
        "outputId": "2a7d53c3-0a76-4c1b-fafc-265a3a3e4c84"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "#uses the sentencepiece tokenizer\n",
        "model_checkpoint = \"google/mt5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLVMwiLTU3SX"
      },
      "source": [
        "### Looking at an example of the tokenization\n",
        "\n",
        "You'll see that the token ids get returned as `input_ids`\n",
        "\n",
        "It also includes an `attention_mask` which allows the algorithm to focus on specific important words using its attention mechanism - it's initialized to all 1s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "FoKiJNr4U3SX",
        "outputId": "823324bf-08b9-4a8e-8057-404587fd0583"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [12808, 259, 262, 9154, 304, 6775, 288, 772, 20727, 8174, 514, 287, 64642, 1052, 305, 97794, 12954, 1052, 24375, 149200, 285, 12017, 259, 264, 609, 277, 263, 1469, 259, 262, 10112, 51053, 3153, 13636, 347, 751, 3721, 25086, 281, 772, 5810, 2586, 309, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = tokenizer(dataset_split[\"train\"][\"text\"][46])\n",
        "inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivaCiDZJU3SX"
      },
      "source": [
        "### Converting ids back to tokens\n",
        "\n",
        "Here's what the tokens look like.\n",
        "\n",
        "The `‚ñÅ` and `</s>` are hallmarks of the SentencePiece tokenizer algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "KAqiJAbUU3SX",
        "outputId": "c5969794-c2a0-47fb-8667-1cfed529f018"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['‚ñÅAdd',\n",
              " '‚ñÅ',\n",
              " 'a',\n",
              " '‚ñÅpop',\n",
              " '‚ñÅof',\n",
              " '‚ñÅcolor',\n",
              " '‚ñÅto',\n",
              " '‚ñÅyour',\n",
              " '‚ñÅwindows',\n",
              " 'ill',\n",
              " '‚ñÅwith',\n",
              " '‚ñÅthe',\n",
              " '‚ñÅvibr',\n",
              " 'ant',\n",
              " '‚ñÅand',\n",
              " '‚ñÅflam',\n",
              " 'boy',\n",
              " 'ant',\n",
              " '‚ñÅBro',\n",
              " 'melia',\n",
              " 'd',\n",
              " '‚ñÅplant',\n",
              " '‚ñÅ',\n",
              " '-',\n",
              " '‚ñÅit',\n",
              " \"'\",\n",
              " 's',\n",
              " '‚ñÅlike',\n",
              " '‚ñÅ',\n",
              " 'a',\n",
              " '‚ñÅfire',\n",
              " 'works',\n",
              " '‚ñÅshow',\n",
              " '‚ñÅhappen',\n",
              " 'ing',\n",
              " '‚ñÅall',\n",
              " '‚ñÅyear',\n",
              " '‚ñÅround',\n",
              " '‚ñÅin',\n",
              " '‚ñÅyour',\n",
              " '‚ñÅown',\n",
              " '‚ñÅhome',\n",
              " '!',\n",
              " '</s>']"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.convert_ids_to_tokens(inputs.input_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUqkC4_dU3SX"
      },
      "source": [
        "### How does it work on the emojis?\n",
        "\n",
        "Fortunately, this seems to work pretty well for the emoji output too\n",
        "\n",
        "some may come back as `<unk>` for unknown tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "BuIYDgU1U3SX",
        "outputId": "b4c257f2-33af-43b0-c8eb-5c306228d44b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [259, 244125, 239199, 191820, 238833, 239328, 248969, 96088, 223546, 238782, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target = tokenizer(dataset_split[\"train\"][\"emoji\"][46])\n",
        "target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "hzY5CW50U3SX",
        "outputId": "866a4430-5f6d-4143-82dc-ade358242ddb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['‚ñÅ', 'üéÜ', 'üå∫', 'üí•', 'üåà', 'üí´', 'üåÜ', 'üî•', 'üá∫', 'üá∏', '</s>']"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.convert_ids_to_tokens(target.input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "OtSvb4fgU3SX",
        "outputId": "4a47a580-8620-4866-b4b4-9593a2765ad3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'üéÜüå∫üí•üåàüí´üåÜüî•üá∫üá∏</s>'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(target.input_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMR8g9fgU3SY"
      },
      "source": [
        "### Let's define a preprocessing function\n",
        "\n",
        "This will allow us to tokenize both the text and labels while allow use to add the token ids from the emojis as the `\"labels\"` key in the overall data structure where it will be convenient to have them for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Y3TX5psMU3SY"
      },
      "outputs": [],
      "source": [
        "max_input_length = 100\n",
        "max_target_length = 20\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    model_inputs = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        max_length=max_input_length,\n",
        "        truncation=True,\n",
        "    )\n",
        "    labels = tokenizer(\n",
        "        examples[\"emoji\"], max_length=max_target_length, truncation=True\n",
        "    )\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXxgOd1aU3SY"
      },
      "source": [
        "Hugging Face datasets have a `map` method that allows you to apply a preprocessing function like this to every example in the data set.\n",
        "\n",
        "Notice that we get everything we had before (text, emoji, topic), but now we also have the input_ids (the tokens), the attention mask, and the labels (also token ids)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "6232d2ec04424a2c86c3010a35fcee6f",
            "993da05b58464d5285aebff90cb5ee75"
          ]
        },
        "id": "VHEqxfrbU3SY",
        "outputId": "53f2742a-21b7-4d13-bd2f-778d556dce23"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0a0e90afb2344dba5fe4d31a837dbce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e13152372c243858b506517a10c7094",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'emoji', 'topic', 'input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 800\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'emoji', 'topic', 'input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 200\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#turn the tokenized data back into a dataset\n",
        "tokenized_datasets = dataset_split.map(preprocess_function, batched=True)\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnUG9CeOU3SY"
      },
      "source": [
        "### Grabbing the pre-trained model\n",
        "\n",
        "as a reminder, `model_checkpoint` was defined earlier - it is `\"google/mt5-small\"`\n",
        "\n",
        "Note that this is an encoder-decoder transformer model the was pretrained on a 750 GB dataset which included tasks for summarization, translation, question answering, and classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "95RPu0nbU3SY",
        "outputId": "7d54fabc-20a5-4433-8e51-9a8bf1bdf2c8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFMT5ForConditionalGeneration.\n",
            "\n",
            "All the layers of TFMT5ForConditionalGeneration were initialized from the model checkpoint at google/mt5-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMT5ForConditionalGeneration for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFAutoModelForSeq2SeqLM\n",
        "\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvS1tZUnU3SY"
      },
      "source": [
        "### Using a data collator\n",
        "\n",
        "Hugging Face provides a Data Collator class which is used to collect the training data into batches and dynamically pad them so that each batch is appropriately padded but without an overall fixed length.\n",
        "\n",
        "With `return_tensors=\"tf\"` we're saying we want the data back in an appropriate data structure suitable for using with Keras/Tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "8OMFNIHXU3SY"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RMJplBHU3SY"
      },
      "source": [
        "Let's make a version of the dataset where the original text fields are removed so we can use it with the collator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "UnaQ_fPpU3SY",
        "outputId": "b411b7cc-b38b-48d8-e819-d519ac0932fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 800\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 200\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_datasets_no_text = tokenized_datasets.remove_columns([\"text\",\"emoji\",\"topic\"])\n",
        "tokenized_datasets_no_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "qLW10hu5U3SY",
        "outputId": "3a0d3b66-59d7-4070-f31e-bdefcac19ef8"
      },
      "outputs": [],
      "source": [
        "tf_train_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_datasets_no_text[\"train\"],\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=True,\n",
        "    batch_size=32,\n",
        ")\n",
        "\n",
        "tf_eval_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_datasets_no_text[\"test\"],\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=False,\n",
        "    batch_size=32,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBw8PAMYU3SY"
      },
      "source": [
        "### Setting up the optimizer\n",
        "\n",
        "When fine-tuning a pre-trained algorithm, you usually want to use a smaller learning rate.\n",
        "\n",
        "Note that we do not specify a loss function - it will use whatever was used in the base model.\n",
        "\n",
        "*NB:* I'm using values that were in the example on the website (https://huggingface.co/learn/nlp-course/chapter7/5?fw=tf ) for a different dataset - I don't know if these are the best for this problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Tfhmz73pU3SY"
      },
      "outputs": [],
      "source": [
        "from transformers import create_optimizer\n",
        "import tensorflow as tf\n",
        "\n",
        "num_train_epochs = 8\n",
        "num_train_steps = len(tf_train_dataset) * num_train_epochs\n",
        "\n",
        "optimizer, schedule = create_optimizer(\n",
        "    init_lr=5.6e-5,\n",
        "    num_warmup_steps=0,\n",
        "    num_train_steps=num_train_steps,\n",
        "    weight_decay_rate=0.01,\n",
        ")\n",
        "\n",
        "model.compile(optimizer=optimizer)\n",
        "\n",
        "# Train in mixed-precision float16 - can be helpful if running on a GPU\n",
        "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "BqKlYuyBU3Sc",
        "outputId": "75d0ab0c-be8d-4d40-e136-7d1da817534d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "25/25 [==============================] - 105s 4s/step - loss: 25.1502 - val_loss: 11.5147\n",
            "Epoch 2/8\n",
            "25/25 [==============================] - 89s 4s/step - loss: 18.4453 - val_loss: 8.4748\n",
            "Epoch 3/8\n",
            "25/25 [==============================] - 87s 3s/step - loss: 15.4057 - val_loss: 7.5587\n",
            "Epoch 4/8\n",
            "25/25 [==============================] - 72s 3s/step - loss: 13.4638 - val_loss: 7.0830\n",
            "Epoch 5/8\n",
            "25/25 [==============================] - 73s 3s/step - loss: 12.3922 - val_loss: 6.8561\n",
            "Epoch 6/8\n",
            "25/25 [==============================] - 70s 3s/step - loss: 11.6430 - val_loss: 6.7182\n",
            "Epoch 7/8\n",
            "25/25 [==============================] - 71s 3s/step - loss: 11.3209 - val_loss: 6.6436\n",
            "Epoch 8/8\n",
            "25/25 [==============================] - 66s 3s/step - loss: 10.9181 - val_loss: 6.6196\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x2ffc116d0>"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5fQ-UpXU3Sc"
      },
      "source": [
        "### Saving a copy of the model's weights\n",
        "\n",
        "This will allow us to load the model later and work with it without completely retraining."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaZ91OFZU3Sc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"models/emoji-model-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k6vziYjU3Sc"
      },
      "source": [
        "### Reload a saved model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFU3PRywU3Sc"
      },
      "outputs": [],
      "source": [
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"models/emoji-model-v1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WM4Wsz4U3Sc"
      },
      "source": [
        "### Inference\n",
        "\n",
        "Let's suppose we have an example to get a prediction for. For now, let's grab one from the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "uL4rDcDoU3Sc",
        "outputId": "af5f3784-25c3-4b43-a6b6-42578db3a13f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking for the perfect gadget to streamline your home organization? You'll love these versatile storage bins.\n",
            "üì¶‚ú®üè†üíï\n",
            "[259, 61408, 332, 287, 5571, 259, 88586, 288, 38101, 1397, 772, 2586, 29660, 291, 1662, 277, 1578, 3869, 259, 3824, 259, 170280, 468, 25973, 3111, 263, 260, 1]\n"
          ]
        }
      ],
      "source": [
        "print( tokenized_datasets[\"test\"][\"text\"][15] )\n",
        "print( tokenized_datasets[\"test\"][\"emoji\"][15])\n",
        "print( tokenized_datasets[\"test\"][\"input_ids\"][15])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U27sMAi8U3Sc"
      },
      "source": [
        "Use the `generate` method to get a prediction sequence from the intput IDs.\n",
        "\n",
        "If you don't already have the tokens, make sure to use your tokenizer first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "jwG9L7QkU3Sc",
        "outputId": "7de19c0d-d734-4610-ff5e-e6df27091bbb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-21 16:30:08.213844: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x34050cb60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2023-11-21 16:30:08.214001: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2023-11-21 16:30:08.236120: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2023-11-21 16:30:08.338200: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['<pad>', '‚ñÅ<extra_id_0>', '.', '</s>']"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prediction = model.generate([tokenized_datasets[\"test\"][\"input_ids\"][15]], max_length=max_target_length)\n",
        "tokenizer.convert_ids_to_tokens(prediction[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "lX4ZLcb6U3Sc",
        "outputId": "95f595c3-9ecd-4252-d314-ccd5a9e2424a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<extra_id_0>.'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decoded_output = tokenizer.decode(prediction[0], skip_special_tokens=True)\n",
        "decoded_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Sw3k8q-U3Sd"
      },
      "source": [
        "## Applied Exploration\n",
        "\n",
        "The applied exploration for this fortnight will be a little different. I want everyone to get some experience fine-tuning an existing model, so this will be the task for the entire fortnight.\n",
        "\n",
        "Fine-tune an existing model with the following requirements\n",
        "* Choose a different starting model - you can use any Hugging Face model, but consider starting with a general one like BART or Llama2.\n",
        "* Choose a different data set - think about something that would be good to include in an application that interests you\n",
        "* Evaluate how well it performed. For sequence-to-sequence model, try going back and using Rouge from Fortnight 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tunning the Dynamic Tinibert Model with Medical Data\n",
        "\n",
        "\n",
        "I will be working on fine-tunning the existing [Dynamic-TinyBERT](https://huggingface.co/Intel/dynamic_tinybert) with medical data, this is the [dataset](https://huggingface.co/datasets/GonzaloValdenebro/MedicalQuestionAnsweringDataset) that I will be using for that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['Question', 'Answer', 'topic', 'split'],\n",
              "        num_rows: 13124\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['Question', 'Answer', 'topic', 'split'],\n",
              "        num_rows: 3282\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "MedData = (load_dataset(\"GonzaloValdenebro/MedicalQuestionAnsweringDataset\", split='train')\n",
        "        .train_test_split(train_size=0.8, test_size=0.2))\n",
        "MedData"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What are the genetic changes related to transthyretin amyloidosis ?\n",
            "Mutations in the TTR gene cause transthyretin amyloidosis. The TTR gene provides instructions for producing a protein called transthyretin. Transthyretin transports vitamin A (retinol) and a hormone called thyroxine throughout the body. To transport retinol and thyroxine, four transthyretin proteins must be attached (bound) to each other to form a four-protein unit (tetramer). Transthyretin is produced primarily in the liver. A small amount of this protein is produced in an area of the brain called the choroid plexus and in the light-sensitive tissue that lines the back of the eye (the retina).  TTR gene mutations are thought to alter the structure of transthyretin, impairing its ability to bind to other transthyretin proteins and altering its normal function.\n",
            "What is (are) Bruises ?\n",
            "A bruise is a mark on your skin caused by blood trapped under the surface. It happens when an injury crushes small blood vessels but does not break the skin. Those vessels break open and leak blood under the skin.     Bruises are often painful and swollen. You can get skin, muscle and bone bruises. Bone bruises are the most serious.    It can take months for a bruise to fade, but most last about two weeks. They start off a reddish color, and then turn bluish-purple and greenish-yellow before returning to normal. To reduce bruising, ice the injured area and elevate it above your heart. See your healthcare provider if you seem to bruise for no reason, or if the bruise appears to be infected.\n"
          ]
        }
      ],
      "source": [
        "print(MedData[\"train\"][\"Question\"][0])\n",
        "print(MedData[\"train\"][\"Answer\"][0])\n",
        "\n",
        "print(MedData[\"test\"][\"Question\"][0])\n",
        "print(MedData[\"test\"][\"Answer\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the Intel/dynamic_tinybert model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<transformers.pipelines.question_answering.QuestionAnsweringPipeline at 0x2b3ae0e10>"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "model = pipeline(\"question-answering\", model=\"Intel/dynamic_tinybert\")\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForQuestionAnswering: ['fit_dense.bias', 'bert.embeddings.position_ids', 'fit_dense.weight']\n",
            "- This IS expected if you are initializing TFBertForQuestionAnswering from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForQuestionAnswering from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertForQuestionAnswering were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForQuestionAnswering for predictions without further training.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56d145f52ec24972ba3b2eface1e8483",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/16406 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Column start_positions not found in dataset!",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part 7 - Fine-Tunning Pretrained Models/Applied Exploration/F7_1_TransferLearning.ipynb Cell 57\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part%207%20-%20Fine-Tunning%20Pretrained%20Models/Applied%20Exploration/F7_1_TransferLearning.ipynb#Y126sdnNjb2RlLXZmcw%3D%3D?line=33'>34</a>\u001b[0m data_collator \u001b[39m=\u001b[39m DataCollatorWithPadding(tokenizer)\n\u001b[1;32m     <a href='vscode-notebook-cell://github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part%207%20-%20Fine-Tunning%20Pretrained%20Models/Applied%20Exploration/F7_1_TransferLearning.ipynb#Y126sdnNjb2RlLXZmcw%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# Prepare TF datasets\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part%207%20-%20Fine-Tunning%20Pretrained%20Models/Applied%20Exploration/F7_1_TransferLearning.ipynb#Y126sdnNjb2RlLXZmcw%3D%3D?line=36'>37</a>\u001b[0m tf_train_dataset \u001b[39m=\u001b[39m tokenized_datasets\u001b[39m.\u001b[39;49mremove_columns([\u001b[39m\"\u001b[39;49m\u001b[39mQuestion\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mAnswer\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtopic\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39;49mto_tf_dataset(\n\u001b[1;32m     <a href='vscode-notebook-cell://github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part%207%20-%20Fine-Tunning%20Pretrained%20Models/Applied%20Exploration/F7_1_TransferLearning.ipynb#Y126sdnNjb2RlLXZmcw%3D%3D?line=37'>38</a>\u001b[0m     columns\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstart_positions\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mend_positions\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part%207%20-%20Fine-Tunning%20Pretrained%20Models/Applied%20Exploration/F7_1_TransferLearning.ipynb#Y126sdnNjb2RlLXZmcw%3D%3D?line=38'>39</a>\u001b[0m     shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part%207%20-%20Fine-Tunning%20Pretrained%20Models/Applied%20Exploration/F7_1_TransferLearning.ipynb#Y126sdnNjb2RlLXZmcw%3D%3D?line=39'>40</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part%207%20-%20Fine-Tunning%20Pretrained%20Models/Applied%20Exploration/F7_1_TransferLearning.ipynb#Y126sdnNjb2RlLXZmcw%3D%3D?line=40'>41</a>\u001b[0m     collate_fn\u001b[39m=\u001b[39;49mdata_collator,\n\u001b[1;32m     <a href='vscode-notebook-cell://github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part%207%20-%20Fine-Tunning%20Pretrained%20Models/Applied%20Exploration/F7_1_TransferLearning.ipynb#Y126sdnNjb2RlLXZmcw%3D%3D?line=41'>42</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part%207%20-%20Fine-Tunning%20Pretrained%20Models/Applied%20Exploration/F7_1_TransferLearning.ipynb#Y126sdnNjb2RlLXZmcw%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m# Define optimizer and training parameters\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part%207%20-%20Fine-Tunning%20Pretrained%20Models/Applied%20Exploration/F7_1_TransferLearning.ipynb#Y126sdnNjb2RlLXZmcw%3D%3D?line=44'>45</a>\u001b[0m num_train_epochs \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/arrow_dataset.py:466\u001b[0m, in \u001b[0;36mTensorflowDatasetMixin.to_tf_dataset\u001b[0;34m(self, batch_size, columns, shuffle, collate_fn, drop_remainder, collate_fn_args, label_cols, prefetch, num_workers, num_test_batches)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m columns:\n\u001b[1;32m    465\u001b[0m     \u001b[39mif\u001b[39;00m col \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m output_signature:\n\u001b[0;32m--> 466\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mColumn \u001b[39m\u001b[39m{\u001b[39;00mcol\u001b[39m}\u001b[39;00m\u001b[39m not found in dataset!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    468\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m label_cols:\n\u001b[1;32m    469\u001b[0m     \u001b[39mif\u001b[39;00m col \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m output_signature:\n",
            "\u001b[0;31mValueError\u001b[0m: Column start_positions not found in dataset!"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering\n",
        "from transformers import DataCollatorWithPadding\n",
        "from transformers import pipeline\n",
        "from datasets import load_dataset\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the medical question answering dataset\n",
        "dataset = load_dataset(\"GonzaloValdenebro/MedicalQuestionAnsweringDataset\")\n",
        "train_dataset = dataset[\"train\"]\n",
        "\n",
        "# Define the model and tokenizer\n",
        "model_checkpoint = \"Intel/dynamic_tinybert\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint, from_pt=True)  # Set from_pt=True\n",
        "\n",
        "# Define the preprocessing function\n",
        "max_input_length = 384\n",
        "max_target_length = 20\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = tokenizer(\n",
        "        examples[\"Question\"],\n",
        "        examples[\"Answer\"],\n",
        "        max_length=max_input_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    return inputs\n",
        "\n",
        "# Apply preprocessing to the dataset\n",
        "tokenized_datasets = train_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Define data collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer)\n",
        "\n",
        "# Prepare TF datasets\n",
        "tf_train_dataset = tokenized_datasets.remove_columns([\"Question\", \"Answer\", \"topic\"]).to_tf_dataset(\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"start_positions\", \"end_positions\"],\n",
        "    shuffle=True,\n",
        "    batch_size=8,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "# Define optimizer and training parameters\n",
        "num_train_epochs = 3\n",
        "num_train_steps = len(tf_train_dataset) * num_train_epochs\n",
        "\n",
        "optimizer, schedule = tf.keras.optimizers.AdamW(\n",
        "    learning_rate=5e-5, epsilon=1e-8\n",
        "), tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "    initial_learning_rate=5e-5, decay_steps=num_train_steps, end_learning_rate=0\n",
        ")\n",
        "\n",
        "model.compile(optimizer=optimizer)\n",
        "\n",
        "# Train the model\n",
        "model.fit(tf_train_dataset, epochs=num_train_epochs)\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"fine_tuned_medical_qa_model\")\n",
        "\n",
        "# Use the fine-tuned model for inference\n",
        "qa_pipeline = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=\"fine_tuned_medical_qa_model\",\n",
        "    tokenizer=\"fine_tuned_medical_qa_model\",\n",
        ")\n",
        "\n",
        "# Example usage\n",
        "example_question = \"What is the treatment for diabetes?\"\n",
        "example_context = \"Diabetes is a chronic condition that affects the way the body processes blood sugar.\"\n",
        "\n",
        "result = qa_pipeline(question=example_question, context=example_context)\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
