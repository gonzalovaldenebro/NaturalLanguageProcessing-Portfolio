{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/blob/main/F6_1_NeuralLanguageModeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C192SOmJS6lw"
      },
      "source": [
        "<div style=\"display: flex; align-items: flex-start;\">\n",
        "  <div>\n",
        "      <h1>CS 195: Natural Language Processing</h1>\n",
        "      <h2>Neural Language Modeling</h2>\n",
        "      </br>\n",
        "    <a href=\"https://colab.research.google.com/github/ericmanley/f23-CS195NLP/blob/main/F6_1_NeuralLanguageModeling.ipynb\">\n",
        "      <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\">\n",
        "    </a>\n",
        "  </div>\n",
        "  <div style=\"margin-left: 20px;\">\n",
        "    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/dalle_neural_net_viz.png?raw=1\" width=\"500\" style=\"display: block;\">\n",
        "  </div>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0boTgP-rW3c"
      },
      "source": [
        "**Cover Illustration:** generated by Dall E using the ChatGPT 4 interface, prompted for a visualization of the network used in the code below. *That's not what I meant.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_8nMuuYrW3c"
      },
      "source": [
        "## Announcement\n",
        "\n",
        "AI - English Faculty Candidate: Gabriel Ford\n",
        "\n",
        "Meeting with students: Thursday at 3:30pm in Howard 309\n",
        "\n",
        "Scholarly Presentation: Friday at 9:00am in Howard ???"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCOtv1vNrW3c"
      },
      "source": [
        "## Reference\n",
        "\n",
        "SLP: Neural Networks and Neural Language Models, Chapter 7 of Speech and Language Processing by Daniel Jurafsky & James H. Martin https://web.stanford.edu/~jurafsky/slp3/7.pdf\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzO6Z7d3rW3d",
        "outputId": "1848f099-db66-4ea9-808f-0f2f71ab3f94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.6)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.2)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install datasets keras tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3okXK_q0rW3d"
      },
      "source": [
        "## Dataset for today\n",
        "\n",
        "AG News dataset\n",
        "* short news articles\n",
        "* four classes: World, Sports, Business, Sci/Tech\n",
        "\n",
        "https://huggingface.co/datasets/ag_news\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4__-MJWrW3d",
        "outputId": "b027ccba-dbb3-4793-e368-921c6c0c4e18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "data = load_dataset(\"ag_news\")\n",
        "\n",
        "print(data[\"train\"][\"text\"][0])\n",
        "\n",
        "# 0 is World\n",
        "# 1 is Sports\n",
        "# 2 is Business\n",
        "# 3 is Sci/Tech\n",
        "print(data[\"train\"][\"label\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chisEI49rW3e"
      },
      "source": [
        "## Review: Text Classification with Keras\n",
        "\n",
        "Last time, we saw\n",
        "* how to do text classification when there are more than 2 classes\n",
        "    - one hot encoded output layer, one node per class, *softmax* output\n",
        "    - categorical crossentropy loss\n",
        "* embedding layer\n",
        "    - pad sequences to all be same length\n",
        "    - learn vector for each word representing word semantics\n",
        "    \n",
        "<div>\n",
        "    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/neural_text_classification.png?raw=1\">\n",
        "</div>\n",
        "\n",
        "image source: SLP Fig. 7.11, https://web.stanford.edu/~jurafsky/slp3/7.pdf\n",
        "\n",
        "*pooling* combines/aggregates all of the embeddings in some way"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "id": "utG0LjaarW3e",
        "outputId": "7ba0b07f-441f-4456-8463-e758699d9c8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "3750/3750 [==============================] - 168s 45ms/step - loss: 0.3211 - accuracy: 0.8834\n",
            "Epoch 2/10\n",
            "3750/3750 [==============================] - 164s 44ms/step - loss: 0.1137 - accuracy: 0.9621\n",
            "Epoch 3/10\n",
            "3750/3750 [==============================] - 159s 42ms/step - loss: 0.0316 - accuracy: 0.9907\n",
            "Epoch 4/10\n",
            "3750/3750 [==============================] - 168s 45ms/step - loss: 0.0158 - accuracy: 0.9962\n",
            "Epoch 5/10\n",
            "1980/3750 [==============>...............] - ETA: 1:24 - loss: 0.0094 - accuracy: 0.9977"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-e378c2d1c7d0>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_encoding_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_encoding_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1781\u001b[0m                         ):\n\u001b[1;32m   1782\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1783\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1784\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    868\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1262\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1263\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mflat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;34m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    253\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1480\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     ]\n\u001b[0;32m---> 60\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     61\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Flatten, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "data = load_dataset(\"ag_news\")\n",
        "\n",
        "# Prepare the tokenizer and fit on the training text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data[\"train\"][\"text\"])\n",
        "vocabulary_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Convert text to sequence of integers\n",
        "train_sequences = tokenizer.texts_to_sequences(data[\"train\"][\"text\"])\n",
        "test_sequences = tokenizer.texts_to_sequences(data[\"test\"][\"text\"])\n",
        "\n",
        "# Pad sequences to ensure uniform length; you can decide the max length based on your dataset's characteristics\n",
        "max_length = 100  # This should be adjusted based on the dataset\n",
        "train_encoding_array = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
        "test_encoding_array = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# Convert labels to one-hot vectors\n",
        "train_labels = data[\"train\"][\"label\"]\n",
        "test_labels = data[\"test\"][\"label\"]\n",
        "train_labels_array = to_categorical(train_labels, num_classes=4)\n",
        "test_labels_array = to_categorical(test_labels, num_classes=4)\n",
        "\n",
        "#create a neural network architecture\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocabulary_size, output_dim=50, input_length=max_length))\n",
        "model.add(Flatten())\n",
        "#use one of these instead of Flatten() to try a pooling method\n",
        "#model.add(GlobalMaxPooling1D())\n",
        "#model.add(GlobalAveragePooling1D())\n",
        "model.add(Dense(20, input_dim=max_length, activation='relu'))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_encoding_array, train_labels_array, epochs=10, verbose=1)\n",
        "\n",
        "loss, accuracy = model.evaluate(test_encoding_array, test_labels_array)\n",
        "print(f\"Test accuracy: {accuracy*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHphpfNqrW3f"
      },
      "source": [
        "## Neural Language Modeling\n",
        "\n",
        "**Neural Language Modeling:** predict next word(s) from previous ones - like what we did with Markov models\n",
        "\n",
        "Like classification, but output is softmax of every possible word in the vocabulary\n",
        "\n",
        "Often a first step before extending the model to do summarization, translation, dialog, and other NLP tasks\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/neural_language_modeling.png?raw=1\">\n",
        "</div>\n",
        "\n",
        "image source: SLP Fig. 7.13, https://web.stanford.edu/~jurafsky/slp3/7.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZ7rQnmrrW3f"
      },
      "source": [
        "## A Neural Language Model in Keras\n",
        "\n",
        "Let's start by sampling some data from our news dataset\n",
        "\n",
        "Then split into a training and testing set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyhN7feurW3f",
        "outputId": "11faeb8c-28f2-488a-a74a-247a75b0b87f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 18479\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Flatten\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "data = load_dataset(\"ag_news\")\n",
        "\n",
        "data_subset, _ = train_test_split(data[\"train\"][\"text\"],train_size=5000)\n",
        "train_data, test_data = train_test_split(data_subset,train_size=0.8)\n",
        "\n",
        "# Prepare the tokenizer and fit on the training text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data_subset)\n",
        "vocabulary_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocabulary size:\",vocabulary_size)\n",
        "\n",
        "# Convert text to sequences of integers\n",
        "train_texts = tokenizer.texts_to_sequences(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIz-JJylrW3f"
      },
      "source": [
        "## Preparing training examples\n",
        "\n",
        "We want to take the sequences like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTBtJBbTrW3f",
        "outputId": "6bd096ce-2143-445d-8311-737dcc1205a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2968, 115, 8, 10628, 6, 77, 18323, 858, 3130, 10628, 107, 24, 4859, 3250, 84, 44, 355, 1, 18324, 200, 139, 99, 316, 44, 31, 7782, 48, 79]\n"
          ]
        }
      ],
      "source": [
        "print( train_texts[0] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNykDXM_rW3f"
      },
      "source": [
        "and slide a window across to predict the next word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9PWlw0brW3f",
        "outputId": "0e95aa89-9632-4bed-f4ba-7625508d0ed3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Use [2968, 115, 8, 10628, 6] to predict 77\n",
            "Use [115, 8, 10628, 6, 77] to predict 18323\n",
            "Use [8, 10628, 6, 77, 18323] to predict 858\n",
            "Use [10628, 6, 77, 18323, 858] to predict 3130\n",
            "etc.\n"
          ]
        }
      ],
      "source": [
        "print(\"Use\",train_texts[0][0:5],\"to predict\",train_texts[0][5])\n",
        "print(\"Use\",train_texts[0][1:6],\"to predict\",train_texts[0][6])\n",
        "print(\"Use\",train_texts[0][2:7],\"to predict\",train_texts[0][7])\n",
        "print(\"Use\",train_texts[0][3:8],\"to predict\",train_texts[0][8])\n",
        "print(\"etc.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCFOFH4irW3g"
      },
      "source": [
        "## Group Discussion\n",
        "\n",
        "What data structures (lists, matrixes, etc.) do we need to prepare to make this a classification problem?\n",
        "\n",
        "\n",
        "I will need a vector that has got one-hot encoded values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0yhF2lRrW3g"
      },
      "source": [
        "## Preparing all of the examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6Ul6r2nrW3g",
        "outputId": "72251a6f-96cc-4993-a47d-e3025dcdb207"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sequences: 137889\n",
            "First train text: [2968, 115, 8, 10628, 6, 77, 18323, 858, 3130, 10628, 107, 24, 4859, 3250, 84, 44, 355, 1, 18324, 200, 139, 99, 316, 44, 31, 7782, 48, 79]\n",
            "Example sequence 0: [2968, 115, 8, 10628, 6]  target: 77\n",
            "Example sequence 1: [115, 8, 10628, 6, 77]  target: 18323\n",
            "Example sequence 2: [8, 10628, 6, 77, 18323]  target: 858\n",
            "Example sequence 3: [10628, 6, 77, 18323, 858]  target: 3130\n",
            "Example sequence 4: [6, 77, 18323, 858, 3130]  target: 10628\n",
            "Example sequence 5: [77, 18323, 858, 3130, 10628]  target: 107\n"
          ]
        }
      ],
      "source": [
        "# Decide the sequence length\n",
        "sequence_length = 5  # Length of the input sequence before predicting the next word\n",
        "\n",
        "# Create the sequences\n",
        "predictor_sequences = []\n",
        "targets = []\n",
        "for text in train_texts:\n",
        "    for i in range(sequence_length, len(text)):\n",
        "        # Take the sequence of tokens as input and the next token as target\n",
        "        curr_target = text[i]\n",
        "        curr_predictor_sequence = text[i-sequence_length:i]\n",
        "        predictor_sequences.append(curr_predictor_sequence)\n",
        "        targets.append(curr_target)\n",
        "\n",
        "\n",
        "print(\"Number of sequences:\",len(predictor_sequences))\n",
        "\n",
        "\n",
        "print(\"First train text:\",train_texts[0])\n",
        "print(\"Example sequence 0:\",predictor_sequences[0],\" target:\",targets[0])\n",
        "print(\"Example sequence 1:\",predictor_sequences[1],\" target:\",targets[1])\n",
        "print(\"Example sequence 2:\",predictor_sequences[2],\" target:\",targets[2])\n",
        "print(\"Example sequence 3:\",predictor_sequences[3],\" target:\",targets[3])\n",
        "print(\"Example sequence 4:\",predictor_sequences[4],\" target:\",targets[4])\n",
        "print(\"Example sequence 5:\",predictor_sequences[5],\" target:\",targets[5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1Q2jOf-rW3g"
      },
      "source": [
        "## Padding\n",
        "\n",
        "Some of the sequences might be really short - so we'll pad them just in case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2KUCsMfrW3g"
      },
      "outputs": [],
      "source": [
        "# Pad sequences to ensure uniform length\n",
        "predictor_sequences_padded = pad_sequences(predictor_sequences, maxlen=sequence_length, padding='pre')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B773A6DrW3g"
      },
      "source": [
        "## The target output\n",
        "\n",
        "Since we're making this into a classification problem, the output layer needs to have one node for each word in the vocabulary.\n",
        "\n",
        "Target values need to be transformed into a one-hot encoded vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc3On74BrW3g",
        "outputId": "9b6ae3c2-fddd-400e-f76a-7d1d3f5bee84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictors words 0: [ 2968   115     8 10628     6]\n",
            "target word 0: 77\n",
            "target word 0 one hot encoded: [0. 0. 0. ... 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Convert output to one-hot encoding\n",
        "target_word_one_hot = to_categorical(targets, num_classes=vocabulary_size)\n",
        "\n",
        "print(\"Predictors words 0:\", predictor_sequences_padded[0])\n",
        "print(\"target word 0:\", targets[0])\n",
        "print(\"target word 0 one hot encoded:\", target_word_one_hot[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAL7lW-orW3g"
      },
      "source": [
        "## Preparing the test set\n",
        "\n",
        "We have to do all of those same things for the test set.\n",
        "\n",
        "**Group Exercise:** Turn this into a function so that you can use it to prepare both the training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xo2zLQM-rW3g"
      },
      "outputs": [],
      "source": [
        "test_texts = tokenizer.texts_to_sequences(test_data)\n",
        "\n",
        "# Create the sequences\n",
        "predictor_sequences_test = []\n",
        "targets_test = []\n",
        "for text in test_texts:\n",
        "    for i in range(sequence_length, len(text)):\n",
        "        # Take the sequence of tokens as input and the next token as target\n",
        "        curr_target = text[i]\n",
        "        curr_predictor_sequence = text[i-sequence_length:i]\n",
        "        predictor_sequences_test.append(curr_predictor_sequence)\n",
        "        targets_test.append(curr_target)\n",
        "\n",
        "# Pad sequences to ensure uniform length\n",
        "predictor_sequences_padded_test = pad_sequences(predictor_sequences_test, maxlen=sequence_length, padding='pre')\n",
        "\n",
        "# Convert target to one-hot encoding\n",
        "target_word_one_hot_test = to_categorical(targets_test, num_classes=vocabulary_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5mCryhm5P10"
      },
      "source": [
        "## Making the above code a function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmiaYCdK5R40"
      },
      "outputs": [],
      "source": [
        "def process_data(data,sequence,tokenizer):\n",
        "\n",
        "  texts = tokenizer.texts_to_sequences(test_data)\n",
        "  vocabulary_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "  # Create the sequences\n",
        "  predictor_sequences = []\n",
        "  targets = []\n",
        "  for text in texts:\n",
        "      for i in range(sequence_length, len(text)):\n",
        "          # Take the sequence of tokens as input and the next token as target\n",
        "          curr_target = text[i]\n",
        "          curr_predictor_sequence = text[i-sequence_length:i]\n",
        "          predictor_sequences.append(curr_predictor_sequence)\n",
        "          targets.append(curr_target)\n",
        "\n",
        "  # Pad sequences to ensure uniform length\n",
        "  predictor_sequences_padded = pad_sequences(predictor_sequences, maxlen=sequence_length, padding='pre')\n",
        "\n",
        "  # Convert target to one-hot encoding\n",
        "  target_word_one_hot = to_categorical(targets, num_classes=vocabulary_size)\n",
        "\n",
        "  return predictor_sequences_padded, target_word_one_hot\n",
        "\n",
        "predictor_sequences_padded, target_word_one_hot = process_data(train_data, 5, tokenizer)\n",
        "predictor_sequences_padded_test, target_word_one_hot_test = process_data(test_data,  5, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9xW6PtXrW3g"
      },
      "source": [
        "## Designing the Neural Network\n",
        "\n",
        "We'll start with a simple network like the one we used for text classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "szrs1-NtrW3h",
        "outputId": "31dbe30f-cd0a-4374-bb85-dfb41f4c8685"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1073/1073 [==============================] - 65s 60ms/step - loss: 7.9410 - accuracy: 0.0467 - val_loss: 7.1632 - val_accuracy: 0.0548\n",
            "Epoch 2/10\n",
            "1073/1073 [==============================] - 57s 54ms/step - loss: 7.0340 - accuracy: 0.0681 - val_loss: 6.6100 - val_accuracy: 0.0821\n",
            "Epoch 3/10\n",
            "1073/1073 [==============================] - 57s 53ms/step - loss: 6.5412 - accuracy: 0.0897 - val_loss: 6.1386 - val_accuracy: 0.1108\n",
            "Epoch 4/10\n",
            "1073/1073 [==============================] - 55s 51ms/step - loss: 6.0977 - accuracy: 0.1168 - val_loss: 5.6959 - val_accuracy: 0.1451\n",
            "Epoch 5/10\n",
            "1073/1073 [==============================] - 56s 52ms/step - loss: 5.6573 - accuracy: 0.1525 - val_loss: 5.2234 - val_accuracy: 0.1905\n",
            "Epoch 6/10\n",
            "1073/1073 [==============================] - 56s 52ms/step - loss: 5.1675 - accuracy: 0.1981 - val_loss: 4.6730 - val_accuracy: 0.2477\n",
            "Epoch 7/10\n",
            "1073/1073 [==============================] - 55s 51ms/step - loss: 4.6114 - accuracy: 0.2525 - val_loss: 4.0559 - val_accuracy: 0.3151\n",
            "Epoch 8/10\n",
            "1073/1073 [==============================] - 64s 60ms/step - loss: 3.9732 - accuracy: 0.3181 - val_loss: 3.3346 - val_accuracy: 0.4108\n",
            "Epoch 9/10\n",
            "1073/1073 [==============================] - 64s 60ms/step - loss: 3.2420 - accuracy: 0.4119 - val_loss: 2.5383 - val_accuracy: 0.5484\n",
            "Epoch 10/10\n",
            "1073/1073 [==============================] - 56s 52ms/step - loss: 2.4613 - accuracy: 0.5350 - val_loss: 1.7722 - val_accuracy: 0.6890\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f65184fdab0>"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocabulary_size, output_dim=50, input_length=sequence_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(100, activation=\"relu\"))\n",
        "model.add(Dense(vocabulary_size, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model - you can also pass in the test set\n",
        "model.fit(predictor_sequences_padded, target_word_one_hot, epochs=10, verbose=1, validation_data=(predictor_sequences_padded_test, target_word_one_hot_test))\n",
        "\n",
        "# The model can now be used to predict the next word in a sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olx5lbgRrW3h"
      },
      "source": [
        "## Testing the final model on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "2-L3DeBPrW3h",
        "outputId": "c66f15a2-d83e-4969-c167-87081ad2a798"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d7feaa7eb83d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor_sequences_padded_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_word_one_hot_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test accuracy: {accuracy*100:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "loss, accuracy = model.evaluate(predictor_sequences_padded_test, target_word_one_hot_test)\n",
        "print(f\"Test accuracy: {accuracy*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGqEDBHFrW3h"
      },
      "source": [
        "## Text Generation\n",
        "\n",
        "We can use this model to successively generate words based on previous ones - like our Markov sequence on steroids.\n",
        "\n",
        "Let's see how this works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oH_ss_E-rW3h",
        "outputId": "a7405fa2-0f20-4ec1-b200-eab7867e7fb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1, 84, 19, 11, 18]]\n",
            "[[ 1 84 19 11 18]]\n"
          ]
        }
      ],
      "source": [
        "starter_string = \"the government said that it\"\n",
        "tokens_list = tokenizer.texts_to_sequences([starter_string])\n",
        "print(tokens_list)\n",
        "\n",
        "tokens_array = np.array(tokens_list)\n",
        "print(tokens_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecK8xMmcrW3h"
      },
      "source": [
        "the model will predict probabilities for each possible word in the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqDa1noVrW3h",
        "outputId": "601fc466-f8cb-4455-f96e-fa97169e091b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[7.7898148e-12 6.4446380e-05 6.8999807e-06 ... 7.7608224e-12\n",
            "  7.1798600e-12 7.7865622e-12]]\n",
            "We get a probability for each of the 18682 words\n"
          ]
        }
      ],
      "source": [
        "predicted_probabilities = model.predict(tokens_array,verbose=0)\n",
        "print(predicted_probabilities)\n",
        "print(\"We get a probability for each of the\",len(predicted_probabilities[0]),\"words\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KYAj1sprW3h"
      },
      "source": [
        "then we could get the word associated with the highest probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKocqI7wrW3h",
        "outputId": "3390236a-6d22-4950-b77e-07660e9471b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word index: 99\n",
            "word: would\n"
          ]
        }
      ],
      "source": [
        "predicted_index = np.argmax(predicted_probabilities)\n",
        "print(\"word index:\",predicted_index)\n",
        "predicted_word = tokenizer.index_word[predicted_index]\n",
        "print(\"word:\",predicted_word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xW9Me5SorW3h"
      },
      "source": [
        "or you could generate a random word according the these probabilities (like with did with Markov text generation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qfwJXNarW3h"
      },
      "source": [
        "### putting it all together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        },
        "id": "0JXjrHiyrW3h",
        "outputId": "1239cc1f-f2ab-4d92-8bb3-a639ae4ce5c6"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-c211ee032638>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcurr_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcurr_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurr_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mpredicted_probabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mpredicted_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_probabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mpredicted_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredicted_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2416, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2401, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2389, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2357, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_4\" is incompatible with the layer: expected shape=(None, 5), found shape=(None, 0)\n"
          ]
        }
      ],
      "source": [
        "starter_string = \"the government said that it\"\n",
        "tokens_list = tokenizer.texts_to_sequences([starter_string])\n",
        "tokens = tokens_list[0]\n",
        "\n",
        "for i in range(50):\n",
        "    curr_seq = tokens[-sequence_length:]\n",
        "    curr_array = np.array([curr_seq])\n",
        "    predicted_probabilities = model.predict(curr_array,verbose=0)\n",
        "    predicted_index = np.argmax(predicted_probabilities)\n",
        "    predicted_word = tokenizer.index_word[predicted_index]\n",
        "    print(predicted_word+\" \",end=\"\")\n",
        "    tokens.append(predicted_index)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDPrWj1UrW3h"
      },
      "source": [
        "## Applied Exploration\n",
        "\n",
        "Pick another dataset and get it working with this code\n",
        "* you will likely need to prepare the text a little differently - do you need to first break it into sentences?\n",
        "* describe your dataset and what you did to prepare it\n",
        "\n",
        "Perform a neural language modeling experiment\n",
        "* experiment with something to try to find a well-performing model\n",
        "    * sliding window size\n",
        "    * number of hidden nodes in the network\n",
        "    * learning rate\n",
        "* describe what you did and write up an interpretation of your results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-THJF0ErW3l",
        "outputId": "088e4565-bfd7-4384-bffb-0fd6551772c9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
          ]
        }
      ],
      "source": [
        "from keras import optimizers\n",
        "\n",
        "#an example on changing the learning rate\n",
        "optimizer = optimizers.Adam(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy',optimizer=optimizer, metrics=[\"accuracy\"])"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}