{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/blob/main/F6_1_NeuralLanguageModeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C192SOmJS6lw"
      },
      "source": [
        "<div style=\"display: flex; align-items: flex-start;\">\n",
        "  <div>\n",
        "      <h1>CS 195: Natural Language Processing</h1>\n",
        "      <h2>Neural Language Modeling</h2>\n",
        "      </br>\n",
        "    <a href=\"https://colab.research.google.com/github/ericmanley/f23-CS195NLP/blob/main/F6_1_NeuralLanguageModeling.ipynb\">\n",
        "      <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\">\n",
        "    </a>\n",
        "  </div>\n",
        "  <div style=\"margin-left: 20px;\">\n",
        "    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/dalle_neural_net_viz.png?raw=1\" width=\"500\" style=\"display: block;\">\n",
        "  </div>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0boTgP-rW3c"
      },
      "source": [
        "**Cover Illustration:** generated by Dall E using the ChatGPT 4 interface, prompted for a visualization of the network used in the code below. *That's not what I meant.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_8nMuuYrW3c"
      },
      "source": [
        "## Announcement\n",
        "\n",
        "AI - English Faculty Candidate: Gabriel Ford\n",
        "\n",
        "Meeting with students: Thursday at 3:30pm in Howard 309\n",
        "\n",
        "Scholarly Presentation: Friday at 9:00am in Howard ???"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCOtv1vNrW3c"
      },
      "source": [
        "## Reference\n",
        "\n",
        "SLP: Neural Networks and Neural Language Models, Chapter 7 of Speech and Language Processing by Daniel Jurafsky & James H. Martin https://web.stanford.edu/~jurafsky/slp3/7.pdf\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzO6Z7d3rW3d",
        "outputId": "1848f099-db66-4ea9-808f-0f2f71ab3f94"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install datasets keras tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3okXK_q0rW3d"
      },
      "source": [
        "## Dataset for today\n",
        "\n",
        "AG News dataset\n",
        "* short news articles\n",
        "* four classes: World, Sports, Business, Sci/Tech\n",
        "\n",
        "https://huggingface.co/datasets/ag_news\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4__-MJWrW3d",
        "outputId": "b027ccba-dbb3-4793-e368-921c6c0c4e18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "data = load_dataset(\"ag_news\")\n",
        "\n",
        "print(data[\"train\"][\"text\"][0])\n",
        "\n",
        "# 0 is World\n",
        "# 1 is Sports\n",
        "# 2 is Business\n",
        "# 3 is Sci/Tech\n",
        "print(data[\"train\"][\"label\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chisEI49rW3e"
      },
      "source": [
        "## Review: Text Classification with Keras\n",
        "\n",
        "Last time, we saw\n",
        "* how to do text classification when there are more than 2 classes\n",
        "    - one hot encoded output layer, one node per class, *softmax* output\n",
        "    - categorical crossentropy loss\n",
        "* embedding layer\n",
        "    - pad sequences to all be same length\n",
        "    - learn vector for each word representing word semantics\n",
        "    \n",
        "<div>\n",
        "    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/neural_text_classification.png?raw=1\">\n",
        "</div>\n",
        "\n",
        "image source: SLP Fig. 7.11, https://web.stanford.edu/~jurafsky/slp3/7.pdf\n",
        "\n",
        "*pooling* combines/aggregates all of the embeddings in some way"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "id": "utG0LjaarW3e",
        "outputId": "7ba0b07f-441f-4456-8463-e758699d9c8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "3750/3750 [==============================] - 27s 7ms/step - loss: 0.3262 - accuracy: 0.8793\n",
            "Epoch 2/10\n",
            "3750/3750 [==============================] - 26s 7ms/step - loss: 0.1113 - accuracy: 0.9629\n",
            "Epoch 3/10\n",
            "3750/3750 [==============================] - 26s 7ms/step - loss: 0.0318 - accuracy: 0.9910\n",
            "Epoch 4/10\n",
            "3750/3750 [==============================] - 27s 7ms/step - loss: 0.0165 - accuracy: 0.9960\n",
            "Epoch 5/10\n",
            "3750/3750 [==============================] - 27s 7ms/step - loss: 0.0113 - accuracy: 0.9972\n",
            "Epoch 6/10\n",
            "3750/3750 [==============================] - 26s 7ms/step - loss: 0.0081 - accuracy: 0.9979\n",
            "Epoch 7/10\n",
            "3750/3750 [==============================] - 26s 7ms/step - loss: 0.0058 - accuracy: 0.9981\n",
            "Epoch 8/10\n",
            "3750/3750 [==============================] - 26s 7ms/step - loss: 0.0044 - accuracy: 0.9984\n",
            "Epoch 9/10\n",
            "3750/3750 [==============================] - 26s 7ms/step - loss: 0.0041 - accuracy: 0.9985\n",
            "Epoch 10/10\n",
            "3750/3750 [==============================] - 26s 7ms/step - loss: 0.0031 - accuracy: 0.9986\n",
            "238/238 [==============================] - 0s 524us/step - loss: 0.6353 - accuracy: 0.9074\n",
            "Test accuracy: 90.74%\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Flatten, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "data = load_dataset(\"ag_news\")\n",
        "\n",
        "# Prepare the tokenizer and fit on the training text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data[\"train\"][\"text\"])\n",
        "vocabulary_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Convert text to sequence of integers\n",
        "train_sequences = tokenizer.texts_to_sequences(data[\"train\"][\"text\"])\n",
        "test_sequences = tokenizer.texts_to_sequences(data[\"test\"][\"text\"])\n",
        "\n",
        "# Pad sequences to ensure uniform length; you can decide the max length based on your dataset's characteristics\n",
        "max_length = 100  # This should be adjusted based on the dataset\n",
        "train_encoding_array = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
        "test_encoding_array = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# Convert labels to one-hot vectors\n",
        "train_labels = data[\"train\"][\"label\"]\n",
        "test_labels = data[\"test\"][\"label\"]\n",
        "train_labels_array = to_categorical(train_labels, num_classes=4)\n",
        "test_labels_array = to_categorical(test_labels, num_classes=4)\n",
        "\n",
        "#create a neural network architecture\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocabulary_size, output_dim=50, input_length=max_length))\n",
        "model.add(Flatten())\n",
        "#use one of these instead of Flatten() to try a pooling method\n",
        "#model.add(GlobalMaxPooling1D())\n",
        "#model.add(GlobalAveragePooling1D())\n",
        "model.add(Dense(20, input_dim=max_length, activation='relu'))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_encoding_array, train_labels_array, epochs=10, verbose=1)\n",
        "\n",
        "loss, accuracy = model.evaluate(test_encoding_array, test_labels_array)\n",
        "print(f\"Test accuracy: {accuracy*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHphpfNqrW3f"
      },
      "source": [
        "## Neural Language Modeling\n",
        "\n",
        "**Neural Language Modeling:** predict next word(s) from previous ones - like what we did with Markov models\n",
        "\n",
        "Like classification, but output is softmax of every possible word in the vocabulary\n",
        "\n",
        "Often a first step before extending the model to do summarization, translation, dialog, and other NLP tasks\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/neural_language_modeling.png?raw=1\">\n",
        "</div>\n",
        "\n",
        "image source: SLP Fig. 7.13, https://web.stanford.edu/~jurafsky/slp3/7.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZ7rQnmrrW3f"
      },
      "source": [
        "## A Neural Language Model in Keras\n",
        "\n",
        "Let's start by sampling some data from our news dataset\n",
        "\n",
        "Then split into a training and testing set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyhN7feurW3f",
        "outputId": "11faeb8c-28f2-488a-a74a-247a75b0b87f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 18497\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Flatten\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "data = load_dataset(\"ag_news\")\n",
        "\n",
        "data_subset, _ = train_test_split(data[\"train\"][\"text\"],train_size=5000)\n",
        "train_data, test_data = train_test_split(data_subset,train_size=0.8)\n",
        "\n",
        "# Prepare the tokenizer and fit on the training text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data_subset)\n",
        "vocabulary_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocabulary size:\",vocabulary_size)\n",
        "\n",
        "# Convert text to sequences of integers\n",
        "train_texts = tokenizer.texts_to_sequences(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIz-JJylrW3f"
      },
      "source": [
        "## Preparing training examples\n",
        "\n",
        "We want to take the sequences like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTBtJBbTrW3f",
        "outputId": "6bd096ce-2143-445d-8311-737dcc1205a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3360, 8378, 16948, 16949, 17, 1, 47, 9, 10, 41, 405, 243, 16950, 187, 89, 785, 16951, 16952, 12, 24, 5426, 130, 4, 1, 2855, 405, 3754, 706]\n"
          ]
        }
      ],
      "source": [
        "print( train_texts[0] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNykDXM_rW3f"
      },
      "source": [
        "and slide a window across to predict the next word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9PWlw0brW3f",
        "outputId": "0e95aa89-9632-4bed-f4ba-7625508d0ed3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Use [3360, 8378, 16948, 16949, 17] to predict 1\n",
            "Use [8378, 16948, 16949, 17, 1] to predict 47\n",
            "Use [16948, 16949, 17, 1, 47] to predict 9\n",
            "Use [16949, 17, 1, 47, 9] to predict 10\n",
            "etc.\n"
          ]
        }
      ],
      "source": [
        "print(\"Use\",train_texts[0][0:5],\"to predict\",train_texts[0][5])\n",
        "print(\"Use\",train_texts[0][1:6],\"to predict\",train_texts[0][6])\n",
        "print(\"Use\",train_texts[0][2:7],\"to predict\",train_texts[0][7])\n",
        "print(\"Use\",train_texts[0][3:8],\"to predict\",train_texts[0][8])\n",
        "print(\"etc.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCFOFH4irW3g"
      },
      "source": [
        "## Group Discussion\n",
        "\n",
        "What data structures (lists, matrixes, etc.) do we need to prepare to make this a classification problem?\n",
        "\n",
        "\n",
        "I will need a vector that has got one-hot encoded values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0yhF2lRrW3g"
      },
      "source": [
        "## Preparing all of the examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6Ul6r2nrW3g",
        "outputId": "72251a6f-96cc-4993-a47d-e3025dcdb207"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sequences: 137533\n",
            "First train text: [3360, 8378, 16948, 16949, 17, 1, 47, 9, 10, 41, 405, 243, 16950, 187, 89, 785, 16951, 16952, 12, 24, 5426, 130, 4, 1, 2855, 405, 3754, 706]\n",
            "Example sequence 0: [3360, 8378, 16948, 16949, 17]  target: 1\n",
            "Example sequence 1: [8378, 16948, 16949, 17, 1]  target: 47\n",
            "Example sequence 2: [16948, 16949, 17, 1, 47]  target: 9\n",
            "Example sequence 3: [16949, 17, 1, 47, 9]  target: 10\n",
            "Example sequence 4: [17, 1, 47, 9, 10]  target: 41\n",
            "Example sequence 5: [1, 47, 9, 10, 41]  target: 405\n"
          ]
        }
      ],
      "source": [
        "# Decide the sequence length\n",
        "sequence_length = 5  # Length of the input sequence before predicting the next word\n",
        "\n",
        "# Create the sequences\n",
        "predictor_sequences = []\n",
        "targets = []\n",
        "for text in train_texts:\n",
        "    for i in range(sequence_length, len(text)):\n",
        "        # Take the sequence of tokens as input and the next token as target\n",
        "        curr_target = text[i]\n",
        "        curr_predictor_sequence = text[i-sequence_length:i]\n",
        "        predictor_sequences.append(curr_predictor_sequence)\n",
        "        targets.append(curr_target)\n",
        "\n",
        "\n",
        "print(\"Number of sequences:\",len(predictor_sequences))\n",
        "\n",
        "\n",
        "print(\"First train text:\",train_texts[0])\n",
        "print(\"Example sequence 0:\",predictor_sequences[0],\" target:\",targets[0])\n",
        "print(\"Example sequence 1:\",predictor_sequences[1],\" target:\",targets[1])\n",
        "print(\"Example sequence 2:\",predictor_sequences[2],\" target:\",targets[2])\n",
        "print(\"Example sequence 3:\",predictor_sequences[3],\" target:\",targets[3])\n",
        "print(\"Example sequence 4:\",predictor_sequences[4],\" target:\",targets[4])\n",
        "print(\"Example sequence 5:\",predictor_sequences[5],\" target:\",targets[5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1Q2jOf-rW3g"
      },
      "source": [
        "## Padding\n",
        "\n",
        "Some of the sequences might be really short - so we'll pad them just in case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "G2KUCsMfrW3g"
      },
      "outputs": [],
      "source": [
        "# Pad sequences to ensure uniform length\n",
        "predictor_sequences_padded = pad_sequences(predictor_sequences, maxlen=sequence_length, padding='pre')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B773A6DrW3g"
      },
      "source": [
        "## The target output\n",
        "\n",
        "Since we're making this into a classification problem, the output layer needs to have one node for each word in the vocabulary.\n",
        "\n",
        "Target values need to be transformed into a one-hot encoded vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc3On74BrW3g",
        "outputId": "9b6ae3c2-fddd-400e-f76a-7d1d3f5bee84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictors words 0: [ 3360  8378 16948 16949    17]\n",
            "target word 0: 1\n",
            "target word 0 one hot encoded: [0. 1. 0. ... 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Convert output to one-hot encoding\n",
        "target_word_one_hot = to_categorical(targets, num_classes=vocabulary_size)\n",
        "\n",
        "print(\"Predictors words 0:\", predictor_sequences_padded[0])\n",
        "print(\"target word 0:\", targets[0])\n",
        "print(\"target word 0 one hot encoded:\", target_word_one_hot[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAL7lW-orW3g"
      },
      "source": [
        "## Preparing the test set\n",
        "\n",
        "We have to do all of those same things for the test set.\n",
        "\n",
        "**Group Exercise:** Turn this into a function so that you can use it to prepare both the training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Xo2zLQM-rW3g"
      },
      "outputs": [],
      "source": [
        "test_texts = tokenizer.texts_to_sequences(test_data)\n",
        "\n",
        "# Create the sequences\n",
        "predictor_sequences_test = []\n",
        "targets_test = []\n",
        "for text in test_texts:\n",
        "    for i in range(sequence_length, len(text)):\n",
        "        # Take the sequence of tokens as input and the next token as target\n",
        "        curr_target = text[i]\n",
        "        curr_predictor_sequence = text[i-sequence_length:i]\n",
        "        predictor_sequences_test.append(curr_predictor_sequence)\n",
        "        targets_test.append(curr_target)\n",
        "\n",
        "# Pad sequences to ensure uniform length\n",
        "predictor_sequences_padded_test = pad_sequences(predictor_sequences_test, maxlen=sequence_length, padding='pre')\n",
        "\n",
        "# Convert target to one-hot encoding\n",
        "target_word_one_hot_test = to_categorical(targets_test, num_classes=vocabulary_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5mCryhm5P10"
      },
      "source": [
        "## Making the above code a function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hmiaYCdK5R40"
      },
      "outputs": [],
      "source": [
        "def process_data(data,sequence,tokenizer):\n",
        "\n",
        "  texts = tokenizer.texts_to_sequences(test_data)\n",
        "  vocabulary_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "  # Create the sequences\n",
        "  predictor_sequences = []\n",
        "  targets = []\n",
        "  for text in texts:\n",
        "      for i in range(sequence_length, len(text)):\n",
        "          # Take the sequence of tokens as input and the next token as target\n",
        "          curr_target = text[i]\n",
        "          curr_predictor_sequence = text[i-sequence_length:i]\n",
        "          predictor_sequences.append(curr_predictor_sequence)\n",
        "          targets.append(curr_target)\n",
        "\n",
        "  # Pad sequences to ensure uniform length\n",
        "  predictor_sequences_padded = pad_sequences(predictor_sequences, maxlen=sequence_length, padding='pre')\n",
        "\n",
        "  # Convert target to one-hot encoding\n",
        "  target_word_one_hot = to_categorical(targets, num_classes=vocabulary_size)\n",
        "\n",
        "  return predictor_sequences_padded, target_word_one_hot\n",
        "\n",
        "predictor_sequences_padded, target_word_one_hot = process_data(train_data, 5, tokenizer)\n",
        "predictor_sequences_padded_test, target_word_one_hot_test = process_data(test_data,  5, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9xW6PtXrW3g"
      },
      "source": [
        "## Designing the Neural Network\n",
        "\n",
        "We'll start with a simple network like the one we used for text classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "szrs1-NtrW3h",
        "outputId": "31dbe30f-cd0a-4374-bb85-dfb41f4c8685"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1066/1066 [==============================] - 13s 12ms/step - loss: 7.9272 - accuracy: 0.0478 - val_loss: 7.1475 - val_accuracy: 0.0536\n",
            "Epoch 2/10\n",
            "1066/1066 [==============================] - 13s 12ms/step - loss: 7.0124 - accuracy: 0.0682 - val_loss: 6.5881 - val_accuracy: 0.0828\n",
            "Epoch 3/10\n",
            "1066/1066 [==============================] - 14s 13ms/step - loss: 6.5280 - accuracy: 0.0897 - val_loss: 6.1298 - val_accuracy: 0.1104\n",
            "Epoch 4/10\n",
            "1066/1066 [==============================] - 16s 15ms/step - loss: 6.0881 - accuracy: 0.1205 - val_loss: 5.6770 - val_accuracy: 0.1518\n",
            "Epoch 5/10\n",
            "1066/1066 [==============================] - 16s 15ms/step - loss: 5.6302 - accuracy: 0.1565 - val_loss: 5.1736 - val_accuracy: 0.1981\n",
            "Epoch 6/10\n",
            "1066/1066 [==============================] - 16s 15ms/step - loss: 5.1139 - accuracy: 0.2053 - val_loss: 4.6100 - val_accuracy: 0.2523\n",
            "Epoch 7/10\n",
            "1066/1066 [==============================] - 17s 16ms/step - loss: 4.5397 - accuracy: 0.2607 - val_loss: 3.9712 - val_accuracy: 0.3268\n",
            "Epoch 8/10\n",
            "1066/1066 [==============================] - 14s 13ms/step - loss: 3.8931 - accuracy: 0.3280 - val_loss: 3.2547 - val_accuracy: 0.4276\n",
            "Epoch 9/10\n",
            "1066/1066 [==============================] - 13s 12ms/step - loss: 3.1659 - accuracy: 0.4259 - val_loss: 2.4789 - val_accuracy: 0.5677\n",
            "Epoch 10/10\n",
            "1066/1066 [==============================] - 14s 13ms/step - loss: 2.4000 - accuracy: 0.5507 - val_loss: 1.7332 - val_accuracy: 0.6968\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x294c90390>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocabulary_size, output_dim=50, input_length=sequence_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(100, activation=\"relu\"))\n",
        "model.add(Dense(vocabulary_size, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model - you can also pass in the test set\n",
        "model.fit(predictor_sequences_padded, target_word_one_hot, epochs=10, verbose=1, validation_data=(predictor_sequences_padded_test, target_word_one_hot_test))\n",
        "\n",
        "# The model can now be used to predict the next word in a sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olx5lbgRrW3h"
      },
      "source": [
        "## Testing the final model on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "2-L3DeBPrW3h",
        "outputId": "c66f15a2-d83e-4969-c167-87081ad2a798"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1066/1066 [==============================] - 3s 3ms/step - loss: 1.7332 - accuracy: 0.6968\n",
            "Test accuracy: 69.68%\n"
          ]
        }
      ],
      "source": [
        "loss, accuracy = model.evaluate(predictor_sequences_padded_test, target_word_one_hot_test)\n",
        "print(f\"Test accuracy: {accuracy*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGqEDBHFrW3h"
      },
      "source": [
        "## Text Generation\n",
        "\n",
        "We can use this model to successively generate words based on previous ones - like our Markov sequence on steroids.\n",
        "\n",
        "Let's see how this works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "oH_ss_E-rW3h",
        "outputId": "a7405fa2-0f20-4ec1-b200-eab7867e7fb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1, 100, 21, 11, 22]]\n",
            "[[  1 100  21  11  22]]\n"
          ]
        }
      ],
      "source": [
        "starter_string = \"the government said that it\"\n",
        "tokens_list = tokenizer.texts_to_sequences([starter_string])\n",
        "print(tokens_list)\n",
        "\n",
        "tokens_array = np.array(tokens_list)\n",
        "print(tokens_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecK8xMmcrW3h"
      },
      "source": [
        "the model will predict probabilities for each possible word in the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wqDa1noVrW3h",
        "outputId": "601fc466-f8cb-4455-f96e-fa97169e091b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.3087605e-22 8.5567290e-06 6.3590189e-05 ... 1.3707638e-22\n",
            "  1.1471919e-22 1.4834443e-22]]\n",
            "We get a probability for each of the 18497 words\n"
          ]
        }
      ],
      "source": [
        "predicted_probabilities = model.predict(tokens_array,verbose=0)\n",
        "print(predicted_probabilities)\n",
        "print(\"We get a probability for each of the\",len(predicted_probabilities[0]),\"words\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KYAj1sprW3h"
      },
      "source": [
        "then we could get the word associated with the highest probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jKocqI7wrW3h",
        "outputId": "3390236a-6d22-4950-b77e-07660e9471b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word index: 94\n",
            "word: would\n"
          ]
        }
      ],
      "source": [
        "predicted_index = np.argmax(predicted_probabilities)\n",
        "print(\"word index:\",predicted_index)\n",
        "predicted_word = tokenizer.index_word[predicted_index]\n",
        "print(\"word:\",predicted_word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xW9Me5SorW3h"
      },
      "source": [
        "or you could generate a random word according the these probabilities (like with did with Markov text generation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qfwJXNarW3h"
      },
      "source": [
        "### putting it all together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        },
        "id": "0JXjrHiyrW3h",
        "outputId": "1239cc1f-f2ab-4d92-8bb3-a639ae4ce5c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "would be investing attacks in a call for the islamic time this week is taken to be sen 000 capacity in his right who does not gone for the release of a quot assumption and rising in new york reuters russian baseball security council has asked that an executive of "
          ]
        }
      ],
      "source": [
        "starter_string = \"the government said that it\"\n",
        "tokens_list = tokenizer.texts_to_sequences([starter_string])\n",
        "tokens = tokens_list[0]\n",
        "\n",
        "for i in range(50):\n",
        "    curr_seq = tokens[-sequence_length:]\n",
        "    curr_array = np.array([curr_seq])\n",
        "    predicted_probabilities = model.predict(curr_array,verbose=0)\n",
        "    predicted_index = np.argmax(predicted_probabilities)\n",
        "    predicted_word = tokenizer.index_word[predicted_index]\n",
        "    print(predicted_word+\" \",end=\"\")\n",
        "    tokens.append(predicted_index)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDPrWj1UrW3h"
      },
      "source": [
        "## Applied Exploration\n",
        "\n",
        "Pick another dataset and get it working with this code\n",
        "* you will likely need to prepare the text a little differently - do you need to first break it into sentences?\n",
        "* describe your dataset and what you did to prepare it\n",
        "\n",
        "Perform a neural language modeling experiment\n",
        "* experiment with something to try to find a well-performing model\n",
        "    * sliding window size\n",
        "    * number of hidden nodes in the network\n",
        "    * learning rate\n",
        "* describe what you did and write up an interpretation of your results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "x-THJF0ErW3l",
        "outputId": "088e4565-bfd7-4384-bffb-0fd6551772c9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
            "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
          ]
        }
      ],
      "source": [
        "from keras import optimizers\n",
        "\n",
        "#an example on changing the learning rate\n",
        "optimizer = optimizers.Adam(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy',optimizer=optimizer, metrics=[\"accuracy\"])"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
