{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/blob/main/F3_3_HMMPOS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C192SOmJS6lw"
      },
      "source": [
        "# CS 195: Natural Language Processing\n",
        "## Hidden Markov Models and Part-of-Speech Tagging\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ericmanley/f23-CS195NLP/blob/main/F3_3_HMMPOS.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_X8YVMlOgu6"
      },
      "source": [
        "## References\n",
        "\n",
        "Wisdom ML's *Hidden Markov Model (HMM) in NLP: Complete Implementation in Python*: https://wisdomml.in/hidden-markov-model-hmm-in-nlp-python/\n",
        "\n",
        "Great Learning's *Part of Speech (POS) tagging with Hidden Markov Model*: https://www.mygreatlearning.com/blog/pos-tagging/\n",
        "\n",
        "Hidden Markov Model on Wikipedia: https://en.wikipedia.org/wiki/Hidden_Markov_model\n",
        "\n",
        "Viterbi Algorithm on Wikipedia: https://en.wikipedia.org/wiki/Viterbi_algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RI12--QYOgu6",
        "outputId": "0a598526-8952-4b90-f530-ea7ffddf8dca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.8.1)\n",
            "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (8.1.6)\n",
            "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (2023.8.8)\n",
            "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (4.66.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fqcePeodOgu7"
      },
      "outputs": [],
      "source": [
        "#you shouldn't need to do this in Colab, but I had to do it on my own machine\n",
        "#in order to connect to the nltk service\n",
        "import nltk\n",
        "import ssl\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shGO-U-9Ogu7"
      },
      "source": [
        "## Review: Markov Chain\n",
        "\n",
        "A **Markov Chain** describes a sequence of possible events and the probabilities of transitioning from one event state to another.\n",
        "\n",
        "It can be applied to text - what is the probability that one word follows another?\n",
        "\n",
        "Here is a diagram which corresponds to this text:\n",
        "\n",
        "`\"I code when I am happy. I am happy therefore I code.\"`\n",
        "\n",
        "<div>\n",
        "<img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/happy_markov_model.png?raw=1\"/>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLV9MnWEOgu8"
      },
      "source": [
        "## Hidden Markov Model\n",
        "\n",
        "A **Hidden Markov Model** is a Markov model in which the states of the model are not directly observable - instead, they emit some other observable output that is based on which state it is in.\n",
        "\n",
        "**Example:** Your weather in your friend's city could be *sunny* or *rainy*, and the transition between them is described by a Markov model. When you talk to your friend on the phone each day, they tell you what they did that day but not what the weather was like.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/weather_hmm.png?raw=1\"/ width=400>\n",
        "</div>\n",
        "\n",
        "image source: https://en.wikipedia.org/wiki/Hidden_Markov_model#/media/File:HiddenMarkovModel.svg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dyqcvr9bOgu8"
      },
      "source": [
        "## What is this useful for?\n",
        "\n",
        "* lots of non-NLP problems like *protein folding* and *transportation forecasting*\n",
        "* NLP problems like *speech recognition*, *speech synthesis*, *handwriting recognition*, and *part-of-speech tagging*\n",
        "\n",
        "**Speech Recognition:** actual words are hidden states (the $x$s), observe audio waves (the $y$s)\n",
        "\n",
        "**Part-of-Speech Tagging:** part of speech (noun, verb, adjective, etc.) are hidden (the $x$s), observe words (the $y$s)\n",
        "\n",
        "<div>\n",
        "<img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/HiddenMarkovModel.png?raw=1\"/ width=400>\n",
        "</div>\n",
        "\n",
        "$a$s: probabilities of transitioning between Markov model states\n",
        "\n",
        "$b$s: probabilities if emitting observation from a given state\n",
        "\n",
        "image source: https://en.wikipedia.org/wiki/Hidden_Markov_model#/media/File:HMMGraph.svg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY9UEE2qOgu8"
      },
      "source": [
        "## Example HMM Training\n",
        "\n",
        "\n",
        "Words:\n",
        "\n",
        "Mark Page will close the book. Page left a will. Will may mark the page. Will will page Mark.\n",
        "\n",
        "Words with tags:\n",
        "\n",
        "Mark *(noun)* Page *(noun)* will *(modal)* close *(verb)* the *(article)* book *(noun)*. Page *(noun)* left *(verb)* a *(article)* will *(noun)*. Will *(noun)* may *(modal)* mark *(verb)* the *(article)* page *(noun)*. Will *(noun)* will *(modal)* page *(verb)* Mark *(noun)*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUd8npodOgu8"
      },
      "source": [
        "### Training the POS Markov Model\n",
        "\n",
        "We can make the Markov model just from the tags - we can do this with the basic Markov model we used before:\n",
        "\n",
        "* (start) noun noun modal verb article noun (end)\n",
        "* (start) noun verb article noun (end)\n",
        "* (start) noun modal verb article noun (end)\n",
        "* (start) noun modal verb noun (end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ia4pnmtfOgu9",
        "outputId": "a3afca45-c41d-4d87-984a-e78f5a3c0227"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'drake_nlp'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part 3 - Syntax, Parsing, and Linguistic Structures /F3_3_HMMPOS.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part%203%20-%20Syntax%2C%20Parsing%2C%20and%20Linguistic%20Structures%20/F3_3_HMMPOS.ipynb#X13sdnNjb2RlLXZmcw%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdrake_nlp\u001b[39;00m \u001b[39mimport\u001b[39;00m MarkovModel\n\u001b[1;32m      <a href='vscode-notebook-cell://github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part%203%20-%20Syntax%2C%20Parsing%2C%20and%20Linguistic%20Structures%20/F3_3_HMMPOS.ipynb#X13sdnNjb2RlLXZmcw%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnetworkx\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnx\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/Part%203%20-%20Syntax%2C%20Parsing%2C%20and%20Linguistic%20Structures%20/F3_3_HMMPOS.ipynb#X13sdnNjb2RlLXZmcw%3D%3D?line=3'>4</a>\u001b[0m pos1 \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m(start) noun noun modal verb article noun (end)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39msplit()\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'drake_nlp'"
          ]
        }
      ],
      "source": [
        "from drake_nlp import MarkovModel\n",
        "import networkx as nx\n",
        "\n",
        "pos1 = \"(start) noun noun modal verb article noun (end)\".split()\n",
        "pos2 = \"(start) noun verb article noun (end)\".split()\n",
        "pos3 = \"(start) noun modal verb article noun (end)\".split()\n",
        "pos4 = \"(start) noun modal verb noun (end)\".split()\n",
        "\n",
        "\n",
        "pos_model = MarkovModel()\n",
        "pos_model.train(pos1)\n",
        "pos_model.train(pos2)\n",
        "pos_model.train(pos3)\n",
        "pos_model.train(pos4)\n",
        "\n",
        "pos_model.visualize(probabilities=True,layout=nx.circular_layout) #this doesn't show the noun -> noun probability, but it is 0.11\n",
        "print(\"probability of states coming after noun:\",pos_model.next_state_probabilities(\"noun\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mZg4CVTOgu9"
      },
      "source": [
        "### Calculating the Emission Probabilities\n",
        "\n",
        "Page *(noun)* will *(modal)* close *(verb)* the *(article)* book *(noun)*. Page *(noun)* left *(verb)* a *(article)* will *(noun)*. Will *(noun)* may *(modal)* mark *(verb)* the *(article)* page *(noun)*. Will *(noun)* can *(modal)* page *(verb)* Mark *(noun)*.\n",
        "\n",
        "First let's make a table where we count how often we see each word as a given part-of-speech\n",
        "\n",
        "\n",
        "| Words  | noun | modal | verb | article |\n",
        "|--------|------|-------|------|---------|\n",
        "| page   | 3    | 0     | 1    | 0       |\n",
        "| will   | 3    | 2     | 0    | 0       |\n",
        "| close  | 0    | 0     | 1    | 0       |\n",
        "| the    | 0    | 0     | 0    | 2       |\n",
        "| book   | 1    | 0     | 0    | 0       |\n",
        "| left   | 0    | 0     | 1    | 0       |\n",
        "| a      | 0    | 0     | 0    | 1       |\n",
        "| may    | 0    | 1     | 0    | 0       |\n",
        "| mark   | 2    | 0     | 1    | 0       |\n",
        "\n",
        "\n",
        "We can see that there are 9 total nouns, and page is 3 of them, thus the probability of emitting \"page\" from the *noun* state is $3/9 \\approx 0.33$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQCHiFEoOgu9"
      },
      "source": [
        "Let's do this for the whole table:\n",
        "\n",
        "| Words  | noun | modal | verb | article |\n",
        "|--------|------|-------|------|---------|\n",
        "| page   | 0.33    | 0     | 0.25    | 0       |\n",
        "| will   | 0.33    | 0.67     | 0    | 0       |\n",
        "| close  | 0    | 0     | 0.25    | 0       |\n",
        "| the    | 0    | 0     | 0    | 0.67       |\n",
        "| book   | 0.11    | 0     | 0    | 0       |\n",
        "| left   | 0    | 0     | 0.25    | 0       |\n",
        "| a      | 0    | 0     | 0    | 0.33       |\n",
        "| may    | 0    | 0.33     | 0    | 0       |\n",
        "| mark   | 0.22    | 0     | 0.25    | 0       |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlGq_dL4Ogu9"
      },
      "source": [
        "## Using a model to make predictions\n",
        "\n",
        "<div>\n",
        "<img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/pos_markov_chain.png?raw=1\"/>\n",
        "</div>\n",
        "\n",
        "| Words  | noun | modal | verb | article |\n",
        "|--------|------|-------|------|---------|\n",
        "| page   | 0.33    | 0     | 0.25    | 0       |\n",
        "| will   | 0.33    | 0.67     | 0    | 0       |\n",
        "| close  | 0    | 0     | 0.25    | 0       |\n",
        "| the    | 0    | 0     | 0    | 0.67       |\n",
        "| book   | 0.11    | 0     | 0    | 0       |\n",
        "| left   | 0    | 0     | 0.25    | 0       |\n",
        "| a      | 0    | 0     | 0    | 0.33       |\n",
        "| may    | 0    | 0.33     | 0    | 0       |\n",
        "| mark   | 0.22    | 0     | 0.25    | 0       |\n",
        "\n",
        "Sentence with unknown parts-of-speech: \"Will will page Mark\"\n",
        "\n",
        "What is the probability that this will be tagged as (start) noun modal verb noun (end)?\n",
        "\n",
        "* probability of transitioning from *(start)* to *noun*: 1\n",
        "* probability of emitting *will* from *noun*: 0.33\n",
        "* probability of transitioning from *noun* to *modal*: 0.33\n",
        "* probability of emitting *will* from *modal*: 0.67\n",
        "* probability of transitioning from *modal* to *verb*: 1\n",
        "* probability of emitting *page* from *verb*: 0.25\n",
        "* probability of transitioning from *verb* to *noun*: 0.25\n",
        "* probability of emitting *mark* from *noun*: 0.22\n",
        "* probability of transitioning from *noun* to *(end)*: 0.5\n",
        "\n",
        "Final probability = $1*0.33*0.33*0.67*1*0.25*0.25*0.22*0.5 \\approx 0.0005$\n",
        "\n",
        "This seems low, but it is the highest probability of any possible tagging!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EanlUl8Ogu9"
      },
      "source": [
        "probability of (start) noun noun verb noun (end): $1*0.33*0.11*0.33*1*0.25*0.25*0.22*0.5 \\approx 0.000085$\n",
        "\n",
        "probability of (start) noun noun noun noun (end): $1*0.33*0.11*0.33*0.11*0.33*0.11*0.22*0.5 \\approx 0.000005$\n",
        "\n",
        "Many probabilites are 0 - for instance, you can't transition from *article* to *verb* or emit \"page\" from *article*, so the probability of (start) noun verb article verb (end) is 0.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKJIO0pxOgu9"
      },
      "source": [
        "## Calculating Emission Probabilities Automatically\n",
        "\n",
        "We need a data structure to store the emission probabilties.\n",
        "\n",
        "Could be a 2D array/matrix\n",
        "\n",
        "Here's an example of using a dictionary of dictionaries of floats\n",
        "* default dict so that we can easily deal with words or pos states we haven't yet seen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYwks-QqOgu9",
        "outputId": "6c566173-0e45-467f-9311-6e76444eb2f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing entry: 0.0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "defaultdict(<function __main__.<lambda>()>,\n",
              "            {'noun': defaultdict(float,\n",
              "                         {'page': 0.33,\n",
              "                          'will': 0.33,\n",
              "                          'book': 0.11,\n",
              "                          'mark': 0.22}),\n",
              "             'modal': defaultdict(float,\n",
              "                         {'will': 0.67, 'may': 0.33, 'page': 0.0}),\n",
              "             'verb': defaultdict(float,\n",
              "                         {'page': 0.25,\n",
              "                          'close': 0.25,\n",
              "                          'left': 0.25,\n",
              "                          'mark': 0.25}),\n",
              "             'article': defaultdict(float, {'the': 0.67, 'a': 0.33})})"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "#default dictionary of dictionary of float\n",
        "#entries  will be 0.0 by default\n",
        "emission_probabilities = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "emission_probabilities[\"noun\"][\"page\"] = 0.33\n",
        "emission_probabilities[\"noun\"][\"will\"] = 0.33\n",
        "emission_probabilities[\"noun\"][\"book\"] = 0.11\n",
        "emission_probabilities[\"noun\"][\"mark\"] = 0.22\n",
        "\n",
        "emission_probabilities[\"modal\"][\"will\"] = 0.67\n",
        "emission_probabilities[\"modal\"][\"may\"] = 0.33\n",
        "\n",
        "emission_probabilities[\"verb\"][\"page\"] = 0.25\n",
        "emission_probabilities[\"verb\"][\"close\"] = 0.25\n",
        "emission_probabilities[\"verb\"][\"left\"] = 0.25\n",
        "emission_probabilities[\"verb\"][\"mark\"] = 0.25\n",
        "\n",
        "emission_probabilities[\"article\"][\"the\"] = 0.67\n",
        "emission_probabilities[\"article\"][\"a\"] = 0.33\n",
        "\n",
        "\n",
        "print(\"Missing entry:\",emission_probabilities[\"modal\"][\"page\"])\n",
        "\n",
        "display(emission_probabilities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KhfbQl6Ogu-"
      },
      "source": [
        "## Group Exercise\n",
        "\n",
        "Suppose your data is in a data structure that looks like this. Create a function that computes the emission probabilities automatically.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4ig41O9Ogu-"
      },
      "outputs": [],
      "source": [
        "def calculate_emission_probabilities(list_of_sentences):\n",
        "\n",
        "    #step 1: initialize emission counts\n",
        "    emission_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    #step 2: count how many times you see each (word,pos) pair\n",
        "    #loop through all sentences\n",
        "        #loop through all (word,pos) pairs in the sentence\n",
        "            #add 1 to emission_counts[pos][word]\n",
        "\n",
        "    #step 3: for each pos state calculate how many total occurrences there were\n",
        "    #e.g., 9 nouns, 4 verbs, etc.\n",
        "\n",
        "    #step 4: create emission_probabilities by dividing emission_counts\n",
        "    # by the total occurrences for that pos state\n",
        "\n",
        "    #step 5: return emission_probabilities\n",
        "    return None # change this\n",
        "\n",
        "\n",
        "sentences = [[(\"mark\", \"noun\"), (\"page\", \"noun\"), (\"will\", \"modal\"), (\"close\", \"verb\"), (\"the\",\"article\"), (\"book\", \"noun\")],\n",
        "    [(\"page\", \"noun\"), (\"left\", \"verb\"), (\"a\", \"article\"), (\"will\", \"noun\")],\n",
        "    [(\"will\", \"noun\"), (\"may\", \"modal\"), (\"mark\", \"verb\"), (\"the\", \"article\"), (\"page\", \"noun\")],\n",
        "    [(\"will\", \"noun\"), (\"will\", \"modal\"), (\"page\", \"verb\"), (\"mark\", \"noun\")]]\n",
        "\n",
        "ep = calculate_emission_probabilities(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A08uOCOQOgu-"
      },
      "source": [
        "## Viterbi Algorithm\n",
        "\n",
        "*Dynamic programming algorithm* for computing the highest possible sequence of states through the HMM.\n",
        "\n",
        "Basic idea: simultaneously calculate probability of all possible paths\n",
        "* only need to base calculations for next word from the previous one and the current transition and emission probabilities\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQjf96RDOgu-"
      },
      "source": [
        "**First observation:** probability of each POS state is probability of transitioning from *(start)* to that state multiplied by the emission probability of that that word from that state.\n",
        "\n",
        "This model can only transition from *(start)* to *noun* - probability 1 (0 all others)\n",
        "\n",
        "0.33 probability to emit \"will\" from *noun*.\n",
        "\n",
        "|     | will | will | page | mark |\n",
        "| --- | ---- | ---- | ---- | ---- |\n",
        "| noun     |  0.33    |      |      |      |\n",
        "| modal    |  0    |      |      |      |\n",
        "| verb     |  0    |      |      |      |\n",
        "| article  |  0    |      |      |      |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JzSLePUOgu-"
      },
      "source": [
        "**Second observation:** multiply highest probability observation from observation 1 (0.33) by transition and emission probabilities for the second\n",
        "\n",
        "*noun* $\\rightarrow$ *noun* (0.11), \"will\" from *noun* (0.33): $0.33 *  (0.11 * 0.33) \\approx 0.012$\n",
        "\n",
        "*noun* $\\rightarrow$ *modal* (0.33), \"will\" from *modal* (0.67): $0.33 * (0.33 * 0.67) \\approx 0.073$\n",
        "\n",
        "*noun* $\\rightarrow$ *verb* (0.11), \"will\" from *verb* (0.0): $0.33 * (0.11 * 0.0) \\approx 0.0$\n",
        "\n",
        "*noun* $\\rightarrow$ *verb* (0.0), \"will\" from *verb* (0.0): $0.33 * (0.0 * 0.0) \\approx 0.0$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "|     | will | will | page | mark |\n",
        "| --- | ---- | ---- | ---- | ---- |\n",
        "| noun     |  0.33    | 0.012     |      |      |\n",
        "| modal    |  0    |  0.073    |      |      |\n",
        "| verb     |  0    |   0   |      |      |\n",
        "| article  |  0    |   0   |      |      |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgwYH8xLOgu-"
      },
      "source": [
        "**Third observation:**\n",
        "\n",
        "Consider \"page\" as a noun - which is higher?\n",
        "* 0.012 * (probability of going *noun* $\\rightarrow$ *noun*) * (probability of emitting \"page\" from noun) $\\approx 0.00044$\n",
        "* 0.073 * (probability of going *modal* $\\rightarrow$ *noun*) * (probability of emitting \"page\" from noun) $= 0$\n",
        "* technically calculate passing through verb and article too\n",
        "\n",
        "Consider \"page\" as a verb - which is higher?\n",
        "* 0.012 * (probability of going *noun* $\\rightarrow$ *verb*) * (probability of emitting \"page\" from verb) $\\approx 0.00033$\n",
        "* 0.073 * (probability of going *modal* $\\rightarrow$ *verb*) * (probability of emitting \"page\" from verb) $\\approx 0.018$\n",
        "* technically calculate passing through verb and article too\n",
        "\n",
        "Technically consider the ones where emission probability is 0 too\n",
        "\n",
        "\n",
        "|     | will | will | page | mark |\n",
        "| --- | ---- | ---- | ---- | ---- |\n",
        "| noun     |  0.33    | 0.012     |  0.00044    |      |\n",
        "| modal    |  0    |  0.073    |   0   |      |\n",
        "| verb     |  0    |   0   |   0.018   |      |\n",
        "| article  |  0    |   0   |  0    |      |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14EX6gF0Ogu-"
      },
      "source": [
        "## Group Exercise\n",
        "\n",
        "What's next? Calculate the probabilities to determine the probability of \"mark\" being a *noun* or *verb*.\n",
        "\n",
        "Remember:\n",
        "\n",
        "probability of going *noun* $\\rightarrow$ *noun* is 1\n",
        "\n",
        "probability of going *verb* $\\rightarrow$ *noun* is 0.25\n",
        "\n",
        "probability of going *verb* $\\rightarrow$ *verb* is 0\n",
        "\n",
        "probability of going *noun* $\\rightarrow$ *verb* is 0.11\n",
        "\n",
        "emission probability of \"mark\" from \"noun\" is 0.22\n",
        "\n",
        "emission probability of \"mark\" from \"verb\" is 0.25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3J-kZ8QOgu-"
      },
      "source": [
        "## Final step\n",
        "\n",
        "You have to also keep track of which transitions were used in each calculation.\n",
        "\n",
        "Start at the end and follow the path back to the beginning that led to that highest probability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-st60vSOgu-"
      },
      "source": [
        "## Some Viterbi code\n",
        "\n",
        "First, repeating the model-building code from above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30F208EgOgu-"
      },
      "outputs": [],
      "source": [
        "from drake_nlp import MarkovModel\n",
        "import networkx as nx\n",
        "\n",
        "pos1 = \"(start) noun noun modal verb article noun (end)\".split()\n",
        "pos2 = \"(start) noun verb article noun (end)\".split()\n",
        "pos3 = \"(start) noun modal verb article noun (end)\".split()\n",
        "pos4 = \"(start) noun modal verb noun (end)\".split()\n",
        "\n",
        "\n",
        "pos_model = MarkovModel()\n",
        "pos_model.train(pos1)\n",
        "pos_model.train(pos2)\n",
        "pos_model.train(pos3)\n",
        "pos_model.train(pos4)\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "#default dictionary of dictionary of float\n",
        "#entries  will be 0.0 by default\n",
        "emission_probabilities = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "emission_probabilities[\"noun\"][\"page\"] = 0.33\n",
        "emission_probabilities[\"noun\"][\"will\"] = 0.33\n",
        "emission_probabilities[\"noun\"][\"book\"] = 0.11\n",
        "emission_probabilities[\"noun\"][\"mark\"] = 0.22\n",
        "\n",
        "emission_probabilities[\"modal\"][\"will\"] = 0.67\n",
        "emission_probabilities[\"modal\"][\"may\"] = 0.33\n",
        "\n",
        "emission_probabilities[\"verb\"][\"page\"] = 0.25\n",
        "emission_probabilities[\"verb\"][\"close\"] = 0.25\n",
        "emission_probabilities[\"verb\"][\"left\"] = 0.25\n",
        "emission_probabilities[\"verb\"][\"mark\"] = 0.25\n",
        "\n",
        "emission_probabilities[\"article\"][\"the\"] = 0.67\n",
        "emission_probabilities[\"article\"][\"a\"] = 0.33\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H5EgTVrOgu-"
      },
      "source": [
        "### Setting up our data structures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNYl6eT_Ogu-",
        "outputId": "b70e4edc-0975-45f6-df55-66cedd88ecbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['(start)', 'noun', 'modal', 'verb', 'article']\n"
          ]
        }
      ],
      "source": [
        "observation = [\"will\",\"will\",\"page\",\"mark\"] #the sentence we're going to tag\n",
        "states = pos_model.get_state_list() #we added this to our MarkovModel code from before\n",
        "print(states)\n",
        "\n",
        "#state-observation table from above\n",
        "state_observation_table = defaultdict(lambda: defaultdict(float))\n",
        "#keep track of the state that led to this place in the state-observation table\n",
        "prior_state = defaultdict(lambda: defaultdict(str))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98x1PWXKOgu-"
      },
      "source": [
        "### initialize the first column\n",
        "\n",
        "just multiply the probability of transition *(start)* $\\rightarrow$ each state by emission probability of the observed word from that state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-K-sAyfOgu_",
        "outputId": "2a8dd20e-8c4a-477c-aaf9-fcf5bfade21b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "defaultdict(<function __main__.<lambda>()>,\n",
              "            {'(start)': defaultdict(float, {0: 0.0}),\n",
              "             'noun': defaultdict(float, {0: 0.33}),\n",
              "             'modal': defaultdict(float, {0: 0.0}),\n",
              "             'verb': defaultdict(float, {0: 0.0}),\n",
              "             'article': defaultdict(float, {0: 0.0})})"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "for state in states:\n",
        "    state_observation_table[state][0] = pos_model.next_state_probabilities(\"(start)\")[state]*emission_probabilities[state][observation[0]]\n",
        "\n",
        "display(state_observation_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s0TwGxtOgu_"
      },
      "source": [
        "### continue for the rest of the observations\n",
        "\n",
        "for each *to_state* - the state we're considering transitioning into for this word\n",
        "* find the best *from_state* to transition from - multiply the previous column by transition probability and emission probability\n",
        "* record the from state in the `prior_state`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trpO98W6Ogu_",
        "outputId": "8c9db112-4555-4cfe-b879-28ca1e8a9713"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "defaultdict(<function __main__.<lambda>()>,\n",
              "            {'(start)': defaultdict(float, {0: 0.0, 1: 0.0}),\n",
              "             'noun': defaultdict(float, {0: 0.33, 1: 0.012100000000000001}),\n",
              "             'modal': defaultdict(float, {0: 0.0, 1: 0.0737}),\n",
              "             'verb': defaultdict(float, {0: 0.0, 1: 0.0}),\n",
              "             'article': defaultdict(float, {0: 0.0, 1: 0.0}),\n",
              "             None: defaultdict(float, {0: 0.0})})"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "defaultdict(<function __main__.<lambda>()>,\n",
              "            {'(start)': defaultdict(float, {0: 0.0, 1: 0.0, 2: 0.0}),\n",
              "             'noun': defaultdict(float,\n",
              "                         {0: 0.33,\n",
              "                          1: 0.012100000000000001,\n",
              "                          2: 0.0004436666666666667}),\n",
              "             'modal': defaultdict(float, {0: 0.0, 1: 0.0737, 2: 0.0}),\n",
              "             'verb': defaultdict(float, {0: 0.0, 1: 0.0, 2: 0.018425}),\n",
              "             'article': defaultdict(float, {0: 0.0, 1: 0.0, 2: 0.0}),\n",
              "             None: defaultdict(float, {0: 0.0, 1: 0.0})})"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "defaultdict(<function __main__.<lambda>()>,\n",
              "            {'(start)': defaultdict(float, {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}),\n",
              "             'noun': defaultdict(float,\n",
              "                         {0: 0.33,\n",
              "                          1: 0.012100000000000001,\n",
              "                          2: 0.0004436666666666667,\n",
              "                          3: 0.001013375}),\n",
              "             'modal': defaultdict(float, {0: 0.0, 1: 0.0737, 2: 0.0, 3: 0.0}),\n",
              "             'verb': defaultdict(float,\n",
              "                         {0: 0.0,\n",
              "                          1: 0.0,\n",
              "                          2: 0.018425,\n",
              "                          3: 1.2324074074074075e-05}),\n",
              "             'article': defaultdict(float, {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}),\n",
              "             None: defaultdict(float, {0: 0.0, 1: 0.0, 2: 0.0})})"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "for observation_idx in range(1,len(observation)):\n",
        "    for to_state in states:\n",
        "        best_from_state = None\n",
        "        best_from_state_probability = 0.0\n",
        "        for from_state in states:\n",
        "            curr_prob = state_observation_table[from_state][observation_idx-1]*pos_model.next_state_probabilities(from_state)[to_state]*emission_probabilities[to_state][observation[observation_idx]]\n",
        "            if curr_prob > best_from_state_probability:\n",
        "                best_from_state_probability = curr_prob\n",
        "                best_from_state = from_state\n",
        "        state_observation_table[to_state][observation_idx] = state_observation_table[best_from_state][observation_idx-1]*pos_model.next_state_probabilities(best_from_state)[to_state]*emission_probabilities[to_state][observation[observation_idx]]\n",
        "        prior_state[to_state][observation_idx] = best_from_state\n",
        "\n",
        "    display(state_observation_table) #display for each observation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkoM-KLtOgu_"
      },
      "source": [
        "### Find the best final state\n",
        "\n",
        "Note that in these examples, we're ignore the (\"end\") state - we could include that in the calculation if we wanted to - in this simple example, we always end on a noun, so it would make the choice trivial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wo8GELgOgu_",
        "outputId": "ccd9df6c-93dd-4f79-e8bd-bf824649e4ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "noun 0.001013375\n"
          ]
        }
      ],
      "source": [
        "best_final_state = None\n",
        "best_final_state_prob = 0\n",
        "for state in states:\n",
        "    if state_observation_table[state][len(observation)-1] > best_final_state_prob:\n",
        "        best_final_state_prob = state_observation_table[state][len(observation)-1]\n",
        "        best_final_state = state\n",
        "\n",
        "print(best_final_state, best_final_state_prob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBiw7JaDOgu_"
      },
      "source": [
        "### Make the list of states - backing up from the end\n",
        "\n",
        "The result is our prediction for each word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D04aBh-5Ogu_",
        "outputId": "67223ee9-fb86-4a48-b1a7-351bb68c1e64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['noun', 'modal', 'verb', 'noun']\n"
          ]
        }
      ],
      "source": [
        "best_states = [best_final_state]\n",
        "\n",
        "for observation_idx in range(len(observation)-1,0,-1):\n",
        "    prev_best_state = prior_state[best_states[0]][observation_idx]\n",
        "    best_states.insert(0,prev_best_state)\n",
        "\n",
        "print(best_states)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50ytWVKCOgu_"
      },
      "source": [
        "## A Corpus with tagged sentences to work from\n",
        "\n",
        "The Penn Treebank corpus in `nltk` has sentences tagged with parts-of-speech\n",
        "\n",
        "Parts-of-speech list here: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "io_ItuUkOgu_",
        "outputId": "e45814a3-ba19-41eb-c386-311b07e9ae4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[('Pierre', 'NNP'),\n",
            "  ('Vinken', 'NNP'),\n",
            "  (',', ','),\n",
            "  ('61', 'CD'),\n",
            "  ('years', 'NNS'),\n",
            "  ('old', 'JJ'),\n",
            "  (',', ','),\n",
            "  ('will', 'MD'),\n",
            "  ('join', 'VB'),\n",
            "  ('the', 'DT'),\n",
            "  ('board', 'NN'),\n",
            "  ('as', 'IN'),\n",
            "  ('a', 'DT'),\n",
            "  ('nonexecutive', 'JJ'),\n",
            "  ('director', 'NN'),\n",
            "  ('Nov.', 'NNP'),\n",
            "  ('29', 'CD'),\n",
            "  ('.', '.')],\n",
            " [('Mr.', 'NNP'),\n",
            "  ('Vinken', 'NNP'),\n",
            "  ('is', 'VBZ'),\n",
            "  ('chairman', 'NN'),\n",
            "  ('of', 'IN'),\n",
            "  ('Elsevier', 'NNP'),\n",
            "  ('N.V.', 'NNP'),\n",
            "  (',', ','),\n",
            "  ('the', 'DT'),\n",
            "  ('Dutch', 'NNP'),\n",
            "  ('publishing', 'VBG'),\n",
            "  ('group', 'NN'),\n",
            "  ('.', '.')],\n",
            " [('Rudolph', 'NNP'),\n",
            "  ('Agnew', 'NNP'),\n",
            "  (',', ','),\n",
            "  ('55', 'CD'),\n",
            "  ('years', 'NNS'),\n",
            "  ('old', 'JJ'),\n",
            "  ('and', 'CC'),\n",
            "  ('former', 'JJ'),\n",
            "  ('chairman', 'NN'),\n",
            "  ('of', 'IN'),\n",
            "  ('Consolidated', 'NNP'),\n",
            "  ('Gold', 'NNP'),\n",
            "  ('Fields', 'NNP'),\n",
            "  ('PLC', 'NNP'),\n",
            "  (',', ','),\n",
            "  ('was', 'VBD'),\n",
            "  ('named', 'VBN'),\n",
            "  ('*-1', '-NONE-'),\n",
            "  ('a', 'DT'),\n",
            "  ('nonexecutive', 'JJ'),\n",
            "  ('director', 'NN'),\n",
            "  ('of', 'IN'),\n",
            "  ('this', 'DT'),\n",
            "  ('British', 'JJ'),\n",
            "  ('industrial', 'JJ'),\n",
            "  ('conglomerate', 'NN'),\n",
            "  ('.', '.')],\n",
            " [('A', 'DT'),\n",
            "  ('form', 'NN'),\n",
            "  ('of', 'IN'),\n",
            "  ('asbestos', 'NN'),\n",
            "  ('once', 'RB'),\n",
            "  ('used', 'VBN'),\n",
            "  ('*', '-NONE-'),\n",
            "  ('*', '-NONE-'),\n",
            "  ('to', 'TO'),\n",
            "  ('make', 'VB'),\n",
            "  ('Kent', 'NNP'),\n",
            "  ('cigarette', 'NN'),\n",
            "  ('filters', 'NNS'),\n",
            "  ('has', 'VBZ'),\n",
            "  ('caused', 'VBN'),\n",
            "  ('a', 'DT'),\n",
            "  ('high', 'JJ'),\n",
            "  ('percentage', 'NN'),\n",
            "  ('of', 'IN'),\n",
            "  ('cancer', 'NN'),\n",
            "  ('deaths', 'NNS'),\n",
            "  ('among', 'IN'),\n",
            "  ('a', 'DT'),\n",
            "  ('group', 'NN'),\n",
            "  ('of', 'IN'),\n",
            "  ('workers', 'NNS'),\n",
            "  ('exposed', 'VBN'),\n",
            "  ('*', '-NONE-'),\n",
            "  ('to', 'TO'),\n",
            "  ('it', 'PRP'),\n",
            "  ('more', 'RBR'),\n",
            "  ('than', 'IN'),\n",
            "  ('30', 'CD'),\n",
            "  ('years', 'NNS'),\n",
            "  ('ago', 'IN'),\n",
            "  (',', ','),\n",
            "  ('researchers', 'NNS'),\n",
            "  ('reported', 'VBD'),\n",
            "  ('0', '-NONE-'),\n",
            "  ('*T*-1', '-NONE-'),\n",
            "  ('.', '.')],\n",
            " [('The', 'DT'),\n",
            "  ('asbestos', 'NN'),\n",
            "  ('fiber', 'NN'),\n",
            "  (',', ','),\n",
            "  ('crocidolite', 'NN'),\n",
            "  (',', ','),\n",
            "  ('is', 'VBZ'),\n",
            "  ('unusually', 'RB'),\n",
            "  ('resilient', 'JJ'),\n",
            "  ('once', 'IN'),\n",
            "  ('it', 'PRP'),\n",
            "  ('enters', 'VBZ'),\n",
            "  ('the', 'DT'),\n",
            "  ('lungs', 'NNS'),\n",
            "  (',', ','),\n",
            "  ('with', 'IN'),\n",
            "  ('even', 'RB'),\n",
            "  ('brief', 'JJ'),\n",
            "  ('exposures', 'NNS'),\n",
            "  ('to', 'TO'),\n",
            "  ('it', 'PRP'),\n",
            "  ('causing', 'VBG'),\n",
            "  ('symptoms', 'NNS'),\n",
            "  ('that', 'WDT'),\n",
            "  ('*T*-1', '-NONE-'),\n",
            "  ('show', 'VBP'),\n",
            "  ('up', 'RP'),\n",
            "  ('decades', 'NNS'),\n",
            "  ('later', 'JJ'),\n",
            "  (',', ','),\n",
            "  ('researchers', 'NNS'),\n",
            "  ('said', 'VBD'),\n",
            "  ('0', '-NONE-'),\n",
            "  ('*T*-2', '-NONE-'),\n",
            "  ('.', '.')]]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from pprint import pprint\n",
        "\n",
        "#nltk.download('treebank') #only need to do it once\n",
        "sentences = nltk.corpus.treebank.tagged_sents()\n",
        "\n",
        "#let's look at the first 5\n",
        "pprint(sentences[0:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXZ8mWXfOgu_"
      },
      "source": [
        "## Applied Exploration\n",
        "\n",
        "Run the model-building code and Viterbi algorithm with some of the Penn Treebank data\n",
        "* practice by training with a small number of sentences\n",
        "* test on some sentences (you may need to use one of the same you trained on to make sure you've seen all those words and POS states)\n",
        "* when you have it working, random sample train and test sets\n",
        "* calculate how well the model does on the test set (accuracy - how many words did you get right over the total number of words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-9Ro2kBOgvD"
      },
      "source": [
        "## Extended Implementation Idea\n",
        "\n",
        "Generalize the code\n",
        "* Make a HiddenMarkovModel class\n",
        "    * method for training\n",
        "    * method for predicting on new sentences\n",
        "    * methods for testing\n",
        "* Perform a large experiment using your code with Penn Treebank data or other data sets"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
