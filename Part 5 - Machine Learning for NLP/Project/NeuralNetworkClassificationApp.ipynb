{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network for Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the dataset srtucture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 120000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 7600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the first observation of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", 'label': 2}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "# Access the 'train' split\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "# Print the first observation\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Budiling the initial neural network (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3750/3750 [==============================] - 3s 700us/step - loss: 0.3583 - accuracy: 0.8835\n",
      "Epoch 2/10\n",
      "3750/3750 [==============================] - 3s 672us/step - loss: 0.2324 - accuracy: 0.9203\n",
      "Epoch 3/10\n",
      "3750/3750 [==============================] - 3s 672us/step - loss: 0.2104 - accuracy: 0.9270\n",
      "Epoch 4/10\n",
      "3750/3750 [==============================] - 3s 671us/step - loss: 0.1948 - accuracy: 0.9320\n",
      "Epoch 5/10\n",
      "3750/3750 [==============================] - 3s 672us/step - loss: 0.1814 - accuracy: 0.9362\n",
      "Epoch 6/10\n",
      "3750/3750 [==============================] - 3s 699us/step - loss: 0.1694 - accuracy: 0.9401\n",
      "Epoch 7/10\n",
      "3750/3750 [==============================] - 3s 699us/step - loss: 0.1588 - accuracy: 0.9437\n",
      "Epoch 8/10\n",
      "3750/3750 [==============================] - 3s 677us/step - loss: 0.1484 - accuracy: 0.9480\n",
      "Epoch 9/10\n",
      "3750/3750 [==============================] - 2s 662us/step - loss: 0.1388 - accuracy: 0.9512\n",
      "Epoch 10/10\n",
      "3750/3750 [==============================] - 2s 656us/step - loss: 0.1294 - accuracy: 0.9551\n",
      "238/238 [==============================] - 0s 456us/step - loss: 0.3684 - accuracy: 0.8982\n",
      "Test accuracy: 89.82%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Load your dataset\n",
    "data = load_dataset(\"ag_news\")\n",
    "\n",
    "train_texts  = data[\"train\"][\"text\"]\n",
    "train_labels = data[\"train\"][\"label\"]\n",
    "test_texts   = data[\"test\"][\"text\"]\n",
    "test_labels  = data[\"test\"][\"label\"]\n",
    "\n",
    "# Consider top 5000 frequent words\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "vectorizer.fit(train_texts)\n",
    "\n",
    "train_vectors = vectorizer.transform(train_texts)\n",
    "test_vectors  = vectorizer.transform(test_texts)\n",
    "\n",
    "# Convert the sklearn vectors to numpy arrays\n",
    "train_vectors_arrays = train_vectors.toarray()\n",
    "test_vectors_arrays  = test_vectors.toarray()\n",
    "\n",
    "# Model Architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=5000, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(4,  activation='softmax'))  # Output layer with 4 classes\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Convert labels to one-hot encoded vectors for multi-class classification\n",
    "train_labels_onehot = to_categorical(train_labels, num_classes=4)\n",
    "test_labels_onehot  = to_categorical(test_labels,   num_classes=4)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_vectors_arrays, train_labels_onehot, epochs=10, verbose=1)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(test_vectors_arrays, test_labels_onehot)\n",
    "print(f\"Test accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final and tunned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3000/3000 [==============================] - 6s 2ms/step - loss: 0.3416 - accuracy: 0.8825 - val_loss: 0.2611 - val_accuracy: 0.9127\n",
      "Epoch 2/100\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.2311 - accuracy: 0.9206 - val_loss: 0.2597 - val_accuracy: 0.9122\n",
      "Epoch 3/100\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.1928 - accuracy: 0.9344 - val_loss: 0.2642 - val_accuracy: 0.9110\n",
      "Epoch 4/100\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.1621 - accuracy: 0.9454 - val_loss: 0.2732 - val_accuracy: 0.9121\n",
      "Epoch 5/100\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: 0.1375 - accuracy: 0.9535 - val_loss: 0.2887 - val_accuracy: 0.9101\n",
      "238/238 [==============================] - 0s 604us/step - loss: 0.2686 - accuracy: 0.9103\n",
      "Test accuracy: 91.03%\n",
      "238/238 [==============================] - 0s 544us/step\n",
      "Test accuracy (after argmax): 91.03%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load your dataset\n",
    "data = load_dataset(\"ag_news\")\n",
    "\n",
    "train_texts  = data[\"train\"][\"text\"]\n",
    "train_labels = data[\"train\"][\"label\"]\n",
    "test_texts   = data[\"test\"][\"text\"]\n",
    "test_labels  = data[\"test\"][\"label\"]\n",
    "\n",
    "# Tokenization and TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "vectorizer.fit(train_texts)\n",
    "\n",
    "train_vectors = vectorizer.transform(train_texts)\n",
    "test_vectors  = vectorizer.transform(test_texts)\n",
    "\n",
    "# Convert the sklearn vectors to numpy arrays\n",
    "train_vectors_arrays = train_vectors.toarray()\n",
    "test_vectors_arrays  = test_vectors.toarray()\n",
    "\n",
    "# Convert labels to one-hot encoded vectors for multi-class classification\n",
    "train_labels_onehot = to_categorical(train_labels, num_classes=4)\n",
    "test_labels_onehot  = to_categorical(test_labels, num_classes=4)\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_vectors_arrays, train_labels_onehot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model Architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=5000, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))  # Output layer with 4 classes\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(patience=3, monitor='val_loss', restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(test_vectors_arrays, test_labels_onehot)\n",
    "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(test_vectors_arrays)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "test_accuracy = accuracy_score(test_labels, y_pred_labels)\n",
    "print(f\"Test accuracy (after argmax): {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Neural Network on user prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3395 - accuracy: 0.8850 - val_loss: 0.2615 - val_accuracy: 0.9108\n",
      "Epoch 2/100\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.2311 - accuracy: 0.9213 - val_loss: 0.2537 - val_accuracy: 0.9132\n",
      "Epoch 3/100\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.1902 - accuracy: 0.9351 - val_loss: 0.2624 - val_accuracy: 0.9110\n",
      "Epoch 4/100\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: 0.1605 - accuracy: 0.9451 - val_loss: 0.2732 - val_accuracy: 0.9124\n",
      "Epoch 5/100\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: 0.1362 - accuracy: 0.9543 - val_loss: 0.2850 - val_accuracy: 0.9116\n",
      "238/238 [==============================] - 0s 623us/step - loss: 0.2654 - accuracy: 0.9138\n",
      "Test accuracy: 91.38%\n",
      "238/238 [==============================] - 0s 571us/step\n",
      "Test accuracy (after argmax): 91.38%\n",
      "Predicted category: Sci/Tech\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load your dataset\n",
    "data = load_dataset(\"ag_news\")\n",
    "\n",
    "train_texts  = data[\"train\"][\"text\"]\n",
    "train_labels = data[\"train\"][\"label\"]\n",
    "test_texts   = data[\"test\"][\"text\"]\n",
    "test_labels  = data[\"test\"][\"label\"]\n",
    "\n",
    "# Tokenization and TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "vectorizer.fit(train_texts)\n",
    "\n",
    "train_vectors = vectorizer.transform(train_texts)\n",
    "test_vectors  = vectorizer.transform(test_texts)\n",
    "\n",
    "# Convert the sklearn vectors to numpy arrays\n",
    "train_vectors_arrays = train_vectors.toarray()\n",
    "test_vectors_arrays  = test_vectors.toarray()\n",
    "\n",
    "# Convert labels to one-hot encoded vectors for multi-class classification\n",
    "train_labels_onehot = to_categorical(train_labels, num_classes=4)\n",
    "test_labels_onehot  = to_categorical(test_labels, num_classes=4)\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_vectors_arrays, train_labels_onehot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model Architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=5000, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))  # Output layer with 4 classes\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(patience=3, monitor='val_loss', restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(test_vectors_arrays, test_labels_onehot)\n",
    "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(test_vectors_arrays)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "test_accuracy = accuracy_score(test_labels, y_pred_labels)\n",
    "print(f\"Test accuracy (after argmax): {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "category_mapping = {0: 'World', 1: 'Sports', 2: 'Business', 3: 'Sci/Tech'}\n",
    "\n",
    "# Function to predict text\n",
    "def predict_text(input_text):\n",
    "    new_text = [input_text]\n",
    "    new_text_vectors = vectorizer.transform(new_text)\n",
    "    new_text_vectors_array = new_text_vectors.toarray()\n",
    "    \n",
    "    predictions = model.predict(new_text_vectors_array, verbose=0)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Map numerical labels to category names\n",
    "    predicted_category = category_mapping.get(predicted_labels[0], 'Unknown')\n",
    "    \n",
    "    return predicted_category\n",
    "\n",
    "# Prompt for user input and make predictions\n",
    "input_text = input(\"Enter your text: \")\n",
    "predicted_category = predict_text(input_text)\n",
    "print(f\"Predicted category: {predicted_category}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model results and architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/Users/gonzalovaldenebro/Library/CloudStorage/OneDrive-DrakeUniversity/CS 195/Fortnight 5/NeuralNetwork.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/gonzalovaldenebro/Library/CloudStorage/OneDrive-DrakeUniversity/CS 195/Fortnight 5/vectorizer.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the vectorizer\n",
    "joblib.dump(vectorizer, '/Users/gonzalovaldenebro/Library/CloudStorage/OneDrive-DrakeUniversity/CS 195/Fortnight 5/vectorizer.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
