{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gonzalovaldenebro/NaturalLanguageProcessing-Portfolio/blob/main/F7_1_TransferLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C192SOmJS6lw"
      },
      "source": [
        "# CS 195: Natural Language Processing\n",
        "## Handling Long-Term Information in Recurrent Neural Networks\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ericmanley/f23-CS195NLP/blob/main/F7_1_TransferLearning.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdF8jAluU3ST"
      },
      "source": [
        "## Reference\n",
        "\n",
        "Hugging Face NLP Course Chapter 1: Transformer Models https://huggingface.co/learn/nlp-course/chapter1/1\n",
        "\n",
        "Hugging Face NLP Course Chapter 3: Fine-tuning a model with the Trainer API or Keras https://huggingface.co/learn/nlp-course/chapter3/1\n",
        "\n",
        "Hugging Face NLP Course Chapter 7, Section 5: Summarization https://huggingface.co/learn/nlp-course/chapter7/5?fw=tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGPT9A3UU3SU",
        "outputId": "de47b23f-da79-4876-f1b6-f4e562c03483"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: datasets in /Users/000794593/Library/Python/3.10/lib/python/site-packages (2.14.4)\n",
            "Requirement already satisfied: keras in /Users/000794593/Library/Python/3.10/lib/python/site-packages (2.14.0)\n",
            "Requirement already satisfied: tensorflow in /Users/000794593/Library/Python/3.10/lib/python/site-packages (2.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (1.24.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (1.4.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (2.28.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: tensorflow-macos==2.14.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (3.10.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.14.0->tensorflow) (63.4.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (4.3.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.59.0)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.14.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from requests>=2.19.0->datasets) (1.26.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas->datasets) (2022.2.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.14.0->tensorflow) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.23.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (3.5)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.2.2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (5.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.1.1)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (3.2.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Frameworks/Python.framework/Versions/3.10/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install --no-cache-dir datasets keras tensorflow sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8m8cTXgU3SU"
      },
      "source": [
        "## Transfer Learning\n",
        "\n",
        "**Transfer Learning** is the process of taking a model that was trained (**pre-trained**) on one task and then **fine tuned** for another task.\n",
        "\n",
        "Today we're going to practice fine-tuning a pre-trained **transformer** model - we'll cover transformers in more detail next week, but they work a lot like the other neural network models we've looked at so far."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Finq7gVLU3SV"
      },
      "source": [
        "<div>\n",
        "    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/pretraining.svg?raw=1\" width=700>\n",
        "    <br />\n",
        "    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/finetuning.svg?raw=1\" width=700>\n",
        "</div>\n",
        "\n",
        "image source: https://huggingface.co/learn/nlp-course/chapter1/4?fw=tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTBu97yfU3SV"
      },
      "source": [
        "## Common pre-trained models\n",
        "\n",
        "There are a variety of pre-trained models out there\n",
        "* usually *very large*\n",
        "* pretrained on *massive amounts of data*\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/model_parameters.png?raw=1\" width=800>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy590YUYU3SV"
      },
      "source": [
        "**Encoders:** BERT, ALBERT, DistilBERT, ELECTRA, RoBERTa\n",
        "* Usually trained on masked input - model tries to predict the missing word in a sequence\n",
        "\n",
        "\n",
        "**Decoders:** CTRL, GPT, GPT-2, Transformer XL\n",
        "* Neural language models - usually trying to predict the next word in a sequence\n",
        "\n",
        "**Encoder-Decoder Models:** BART, mBART, Marian, T5\n",
        "* full sequence-to-sequence models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDEA1B-KU3SV"
      },
      "source": [
        "## Working Example\n",
        "\n",
        "We're going to work through our text-to-emoji example, fine-tuning a variant of T5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS6uNyR5U3SV"
      },
      "source": [
        "### Load and filter our dataset just like before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jnzz2tFMU3SV",
        "outputId": "ce1dd9d5-4264-46af-b28a-a42d987af18a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'emoji', 'topic'],\n",
              "    num_rows: 503682\n",
              "})"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "# Define a function to check if 'text' is not None\n",
        "def is_not_none(example):\n",
        "    return example['text'] is not None\n",
        "\n",
        "dataset = load_dataset(\"KomeijiForce/Text2Emoji\",split=\"train\")\n",
        "\n",
        "# Filter the dataset\n",
        "dataset = dataset.filter(is_not_none)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOK4tobeU3SW"
      },
      "source": [
        "### choosing a sample to work with\n",
        "\n",
        "Even the smaller transformer models will take too long to train on in class\n",
        "\n",
        "Let's choose a small sample to work on in class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-TBJ3gGU3SW"
      },
      "outputs": [],
      "source": [
        "# Shuffle the dataset\n",
        "shuffled_dataset = dataset.shuffle(seed=42)\n",
        "\n",
        "# Select a small sample\n",
        "sample_size = 5000  # Define your sample size\n",
        "sample_dataset = shuffled_dataset.select(range(sample_size))\n",
        "\n",
        "#if you want to use the entire dataset just uncomment the following\n",
        "#sample_dataset = shuffled_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyP7hLmeU3SW"
      },
      "source": [
        "### Train/test split\n",
        "\n",
        "Hugging Face datasets actually include a `train_test_split` function for splitting into training and testing sets if you don't already have them split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W19dXtjDU3SW",
        "outputId": "ce833376-a61a-41ee-a4df-870de418dfe0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'emoji', 'topic'],\n",
              "        num_rows: 4000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'emoji', 'topic'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_split = sample_dataset.train_test_split(test_size=0.2)\n",
        "dataset_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3-J7WZSU3SW"
      },
      "source": [
        "### Reminder of what the data looks like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKC-w5pcU3SW",
        "outputId": "eed9d603-2752-47ca-90b7-df0fea498d42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Riding a ferry across the bay offers incredible views of the skyline.\n",
            "‚õ¥üåâüåäüëÄ\n"
          ]
        }
      ],
      "source": [
        "print(dataset_split[\"train\"][\"text\"][46])\n",
        "print(dataset_split[\"train\"][\"emoji\"][46])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueFs7ldcU3SX"
      },
      "source": [
        "### The Tokenizer\n",
        "\n",
        "Since we will be using an existing model to start, we need to make sure we prepare our data in the same way that model was trained on.\n",
        "\n",
        "**T5:** an encoder-decoder Transformer architecture suitable for sequence-to-sequences tasks\n",
        "\n",
        "**mT5:** A multilingual version of T5, pretrained on the multilingual Common Crawl corpus (mC4), covering 101 languages\n",
        "\n",
        "**mt5-small:** A small version of mT5, suitable for getting things working before attempting to train on a large model\n",
        "\n",
        "`mt5-small` uses the SentencePiece tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9buCsGSU3SX",
        "outputId": "2a7d53c3-0a76-4c1b-fafc-265a3a3e4c84"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "/Users/000794593/Library/Python/3.10/lib/python/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "#uses the sentencepiece tokenizer\n",
        "model_checkpoint = \"google/mt5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLVMwiLTU3SX"
      },
      "source": [
        "### Looking at an example of the tokenization\n",
        "\n",
        "You'll see that the token ids get returned as `input_ids`\n",
        "\n",
        "It also includes an `attention_mask` which allows the algorithm to focus on specific important words using its attention mechanism - it's initialized to all 1s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoKiJNr4U3SX",
        "outputId": "823324bf-08b9-4a8e-8057-404587fd0583"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [47368, 347, 259, 262, 100174, 276, 259, 15259, 287, 7662, 259, 5760, 259, 87448, 6179, 304, 287, 20495, 1397, 260, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = tokenizer(dataset_split[\"train\"][\"text\"][46])\n",
        "inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivaCiDZJU3SX"
      },
      "source": [
        "### Converting ids back to tokens\n",
        "\n",
        "Here's what the tokens look like.\n",
        "\n",
        "The `‚ñÅ` and `</s>` are hallmarks of the SentencePiece tokenizer algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAqiJAbUU3SX",
        "outputId": "c5969794-c2a0-47fb-8667-1cfed529f018"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['‚ñÅRid',\n",
              " 'ing',\n",
              " '‚ñÅ',\n",
              " 'a',\n",
              " '‚ñÅferr',\n",
              " 'y',\n",
              " '‚ñÅ',\n",
              " 'across',\n",
              " '‚ñÅthe',\n",
              " '‚ñÅbay',\n",
              " '‚ñÅ',\n",
              " 'offers',\n",
              " '‚ñÅ',\n",
              " 'incredible',\n",
              " '‚ñÅviews',\n",
              " '‚ñÅof',\n",
              " '‚ñÅthe',\n",
              " '‚ñÅsky',\n",
              " 'line',\n",
              " '.',\n",
              " '</s>']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.convert_ids_to_tokens(inputs.input_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUqkC4_dU3SX"
      },
      "source": [
        "### How does it work on the emojis?\n",
        "\n",
        "Fortunately, this seems to work pretty well for the emoji output too\n",
        "\n",
        "some may come back as `<unk>` for unknown tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuIYDgU1U3SX",
        "outputId": "b4c257f2-33af-43b0-c8eb-5c306228d44b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [259, 2, 241593, 239651, 1], 'attention_mask': [1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target = tokenizer(dataset_split[\"train\"][\"emoji\"][46])\n",
        "target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzY5CW50U3SX",
        "outputId": "866a4430-5f6d-4143-82dc-ade358242ddb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['‚ñÅ', '<unk>', 'üåä', 'üëÄ', '</s>']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.convert_ids_to_tokens(target.input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtSvb4fgU3SX",
        "outputId": "4a47a580-8620-4866-b4b4-9593a2765ad3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<unk>üåäüëÄ</s>'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(target.input_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMR8g9fgU3SY"
      },
      "source": [
        "### Let's define a preprocessing function\n",
        "\n",
        "This will allow us to tokenize both the text and labels while allow use to add the token ids from the emojis as the `\"labels\"` key in the overall data structure where it will be convenient to have them for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3TX5psMU3SY"
      },
      "outputs": [],
      "source": [
        "max_input_length = 100\n",
        "max_target_length = 20\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    model_inputs = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        max_length=max_input_length,\n",
        "        truncation=True,\n",
        "    )\n",
        "    labels = tokenizer(\n",
        "        examples[\"emoji\"], max_length=max_target_length, truncation=True\n",
        "    )\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXxgOd1aU3SY"
      },
      "source": [
        "Hugging Face datasets have a `map` method that allows you to apply a preprocessing function like this to every example in the data set.\n",
        "\n",
        "Notice that we get everything we had before (text, emoji, topic), but now we also have the input_ids (the tokens), the attention mask, and the labels (also token ids)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHEqxfrbU3SY",
        "outputId": "53f2742a-21b7-4d13-bd2f-778d556dce23",
        "colab": {
          "referenced_widgets": [
            "6232d2ec04424a2c86c3010a35fcee6f",
            "993da05b58464d5285aebff90cb5ee75"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6232d2ec04424a2c86c3010a35fcee6f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "993da05b58464d5285aebff90cb5ee75",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'emoji', 'topic', 'input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 4000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'emoji', 'topic', 'input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#turn the tokenized data back into a dataset\n",
        "tokenized_datasets = dataset_split.map(preprocess_function, batched=True)\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnUG9CeOU3SY"
      },
      "source": [
        "### Grabbing the pre-trained model\n",
        "\n",
        "as a reminder, `model_checkpoint` was defined earlier - it is `\"google/mt5-small\"`\n",
        "\n",
        "Note that this is an encoder-decoder transformer model the was pretrained on a 750 GB dataset which included tasks for summarization, translation, question answering, and classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95RPu0nbU3SY",
        "outputId": "7d54fabc-20a5-4433-8e51-9a8bf1bdf2c8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/000794593/Library/Python/3.10/lib/python/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
            "  warnings.warn(\n",
            "All model checkpoint layers were used when initializing TFMT5ForConditionalGeneration.\n",
            "\n",
            "All the layers of TFMT5ForConditionalGeneration were initialized from the model checkpoint at google/mt5-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMT5ForConditionalGeneration for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFAutoModelForSeq2SeqLM\n",
        "\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvS1tZUnU3SY"
      },
      "source": [
        "### Using a data collator\n",
        "\n",
        "Hugging Face provides a Data Collator class which is used to collect the training data into batches and dynamically pad them so that each batch is appropriately padded but without an overall fixed length.\n",
        "\n",
        "With `return_tensors=\"tf\"` we're saying we want the data back in an appropriate data structure suitable for using with Keras/Tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OMFNIHXU3SY"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RMJplBHU3SY"
      },
      "source": [
        "Let's make a version of the dataset where the original text fields are removed so we can use it with the collator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnaQ_fPpU3SY",
        "outputId": "b411b7cc-b38b-48d8-e819-d519ac0932fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 4000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_datasets_no_text = tokenized_datasets.remove_columns([\"text\",\"emoji\",\"topic\"])\n",
        "tokenized_datasets_no_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLW10hu5U3SY",
        "outputId": "3a0d3b66-59d7-4070-f31e-bdefcac19ef8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        }
      ],
      "source": [
        "tf_train_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_datasets_no_text[\"train\"],\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=True,\n",
        "    batch_size=32,\n",
        ")\n",
        "tf_eval_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_datasets_no_text[\"test\"],\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=False,\n",
        "    batch_size=32,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBw8PAMYU3SY"
      },
      "source": [
        "### Setting up the optimizer\n",
        "\n",
        "When fine-tuning a pre-trained algorithm, you usually want to use a smaller learning rate.\n",
        "\n",
        "Note that we do not specify a loss function - it will use whatever was used in the base model.\n",
        "\n",
        "*NB:* I'm using values that were in the example on the website (https://huggingface.co/learn/nlp-course/chapter7/5?fw=tf ) for a different dataset - I don't know if these are the best for this problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tfhmz73pU3SY"
      },
      "outputs": [],
      "source": [
        "from transformers import create_optimizer\n",
        "import tensorflow as tf\n",
        "\n",
        "num_train_epochs = 8\n",
        "num_train_steps = len(tf_train_dataset) * num_train_epochs\n",
        "\n",
        "optimizer, schedule = create_optimizer(\n",
        "    init_lr=5.6e-5,\n",
        "    num_warmup_steps=0,\n",
        "    num_train_steps=num_train_steps,\n",
        "    weight_decay_rate=0.01,\n",
        ")\n",
        "\n",
        "model.compile(optimizer=optimizer)\n",
        "\n",
        "# Train in mixed-precision float16 - can be helpful if running on a GPU\n",
        "#tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqKlYuyBU3Sc",
        "outputId": "75d0ab0c-be8d-4d40-e136-7d1da817534d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "125/125 [==============================] - 343s 3s/step - loss: 15.4882 - val_loss: 7.6471\n",
            "Epoch 2/8\n",
            "125/125 [==============================] - 350s 3s/step - loss: 9.1760 - val_loss: 6.4821\n",
            "Epoch 3/8\n",
            "125/125 [==============================] - 377s 3s/step - loss: 7.5378 - val_loss: 5.6253\n",
            "Epoch 4/8\n",
            "125/125 [==============================] - 352s 3s/step - loss: 6.7815 - val_loss: 4.9688\n",
            "Epoch 5/8\n",
            "125/125 [==============================] - 380s 3s/step - loss: 6.2377 - val_loss: 4.5019\n",
            "Epoch 6/8\n",
            "125/125 [==============================] - 351s 3s/step - loss: 5.9614 - val_loss: 4.3511\n",
            "Epoch 7/8\n",
            "125/125 [==============================] - 394s 3s/step - loss: 5.7288 - val_loss: 4.3082\n",
            "Epoch 8/8\n",
            "125/125 [==============================] - 363s 3s/step - loss: 5.6847 - val_loss: 4.2973\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x2c4214e20>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFAjwMmAU3Sc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5fQ-UpXU3Sc"
      },
      "source": [
        "### Saving a copy of the model's weights\n",
        "\n",
        "This will allow us to load the model later and work with it without completely retraining."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaZ91OFZU3Sc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"models/emoji-model-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k6vziYjU3Sc"
      },
      "source": [
        "### Reload a saved model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFU3PRywU3Sc"
      },
      "outputs": [],
      "source": [
        "#model = TFAutoModelForSeq2SeqLM.from_pretrained(\"models/emoji-model-v1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WM4Wsz4U3Sc"
      },
      "source": [
        "### Inference\n",
        "\n",
        "Let's suppose we have an example to get a prediction for. For now, let's grab one from the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uL4rDcDoU3Sc",
        "outputId": "af5f3784-25c3-4b43-a6b6-42578db3a13f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Marvel at the towering cathedral steeples and intricate stained glass windows. This stunning architectural wonder radiates a sense of divine presence and spirituality.\n",
            "üèôÔ∏èüííüßö‚õ™üöÑüí´üïäÔ∏èüå∏‚ú®\n",
            "[46577, 344, 287, 288, 176572, 317, 216387, 113489, 104793, 305, 281, 92804, 346, 259, 263, 29967, 27416, 20727, 260, 1494, 259, 263, 59976, 259, 262, 115957, 29100, 79398, 1837, 259, 262, 13336, 304, 64236, 265, 65901, 265, 305, 43498, 2302, 260, 1]\n"
          ]
        }
      ],
      "source": [
        "print( tokenized_datasets[\"test\"][\"text\"][15] )\n",
        "print( tokenized_datasets[\"test\"][\"emoji\"][15] )\n",
        "print( tokenized_datasets[\"test\"][\"input_ids\"][15] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U27sMAi8U3Sc"
      },
      "source": [
        "Use the `generate` method to get a prediction sequence from the intput IDs.\n",
        "\n",
        "If you don't already have the tokens, make sure to use your tokenizer first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwG9L7QkU3Sc",
        "outputId": "7de19c0d-d734-4610-ff5e-e6df27091bbb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<pad>', '‚ñÅ', '‚ú®', '‚ú®', '</s>']"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prediction = model.generate([tokenized_datasets[\"test\"][\"input_ids\"][15]], max_length=max_target_length)\n",
        "tokenizer.convert_ids_to_tokens(prediction[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lX4ZLcb6U3Sc",
        "outputId": "95f595c3-9ecd-4252-d314-ccd5a9e2424a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'‚ú®‚ú®'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decoded_output = tokenizer.decode(prediction[0], skip_special_tokens=True)\n",
        "decoded_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Sw3k8q-U3Sd"
      },
      "source": [
        "## Applied Exploration\n",
        "\n",
        "The applied exploration for this fortnight will be a little different. I want everyone to get some experience fine-tuning an existing model, so this will be the task for the entire fortnight.\n",
        "\n",
        "Fine-tune an existing model with the following requirements\n",
        "* Choose a different starting model - you can use any Hugging Face model, but consider starting with a general one like BART or Llama2.\n",
        "* Choose a different data set - think about something that would be good to include in an application that interests you\n",
        "* Evaluate how well it performed. For sequence-to-sequence model, try going back and using Rouge from Fortnight 1."
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}